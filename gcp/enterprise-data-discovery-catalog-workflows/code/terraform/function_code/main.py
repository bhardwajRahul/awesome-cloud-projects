"""
Enterprise Data Discovery Cloud Function

This function performs automated metadata extraction and cataloging for BigQuery
datasets, Cloud Storage buckets, and other data sources. It integrates with
Data Catalog to create entries and apply intelligent tagging based on content
analysis and naming patterns.

Author: Generated by Terraform IaC
Version: 1.0
"""

import json
import logging
import re
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional

from google.cloud import datacatalog_v1
from google.cloud import bigquery
from google.cloud import storage
import functions_framework

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize Google Cloud clients
datacatalog_client = datacatalog_v1.DataCatalogClient()
bigquery_client = bigquery.Client()
storage_client = storage.Client()

# Configuration constants
PROJECT_ID = "${project_id}"
REGION = "${region}"
MAX_SAMPLE_SIZE = 1000
QUALITY_THRESHOLD = 0.8

@functions_framework.http
def discover_and_catalog(request):
    """
    Main HTTP function entry point for data discovery and cataloging.
    
    Processes discovery requests and orchestrates metadata extraction
    across multiple data sources including BigQuery and Cloud Storage.
    
    Args:
        request: HTTP request object containing discovery parameters
        
    Returns:
        JSON response with discovery results and cataloging status
    """
    # Set CORS headers for web requests
    headers = {
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'POST, OPTIONS',
        'Access-Control-Allow-Headers': 'Content-Type'
    }
    
    # Handle preflight OPTIONS request
    if request.method == 'OPTIONS':
        return ('', 204, headers)
    
    try:
        # Parse request parameters
        request_json = request.get_json(silent=True) or {}
        project_id = request_json.get('project_id', PROJECT_ID)
        location = request_json.get('location', REGION)
        source_type = request_json.get('source_type', 'all')
        comprehensive = request_json.get('comprehensive', False)
        
        logger.info(f"Starting discovery for project: {project_id}, type: {source_type}")
        
        # Initialize results structure
        results = {
            'project_id': project_id,
            'location': location,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'bigquery_datasets': [],
            'storage_buckets': [],
            'catalog_entries_created': 0,
            'tags_applied': 0,
            'errors': []
        }
        
        # Perform discovery based on source type
        if source_type in ['all', 'bigquery']:
            bq_results = discover_bigquery_assets(project_id, location, comprehensive)
            results['bigquery_datasets'] = bq_results['datasets']
            results['catalog_entries_created'] += bq_results.get('entries_created', 0)
            results['tags_applied'] += bq_results.get('tags_applied', 0)
            if bq_results.get('errors'):
                results['errors'].extend(bq_results['errors'])
        
        if source_type in ['all', 'storage']:
            storage_results = discover_storage_assets(project_id, location, comprehensive)
            results['storage_buckets'] = storage_results['buckets']
            results['catalog_entries_created'] += storage_results.get('entries_created', 0)
            results['tags_applied'] += storage_results.get('tags_applied', 0)
            if storage_results.get('errors'):
                results['errors'].extend(storage_results['errors'])
        
        # Calculate summary statistics
        results['summary'] = {
            'total_assets_discovered': len(results['bigquery_datasets']) + len(results['storage_buckets']),
            'catalog_entries_created': results['catalog_entries_created'],
            'tags_applied': results['tags_applied'],
            'error_count': len(results['errors'])
        }
        
        logger.info(f"Discovery completed: {results['summary']}")
        
        return (json.dumps(results, indent=2), 200, headers)
        
    except Exception as e:
        logger.error(f"Discovery failed with error: {str(e)}", exc_info=True)
        error_response = {
            'error': str(e),
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'project_id': request_json.get('project_id', PROJECT_ID) if 'request_json' in locals() else PROJECT_ID
        }
        return (json.dumps(error_response, indent=2), 500, headers)


def discover_bigquery_assets(project_id: str, location: str, comprehensive: bool = False) -> Dict[str, Any]:
    """
    Discover and catalog BigQuery datasets and tables.
    
    Args:
        project_id: Google Cloud project ID
        location: Region for Data Catalog entries
        comprehensive: Whether to perform deep analysis
        
    Returns:
        Dictionary containing discovered datasets and cataloging results
    """
    results = {
        'datasets': [],
        'entries_created': 0,
        'tags_applied': 0,
        'errors': []
    }
    
    try:
        # List all datasets in the project
        datasets = list(bigquery_client.list_datasets(project=project_id))
        logger.info(f"Found {len(datasets)} BigQuery datasets")
        
        for dataset in datasets:
            try:
                dataset_info = process_bigquery_dataset(dataset, project_id, location, comprehensive)
                results['datasets'].append(dataset_info)
                
                # Update counters
                if dataset_info.get('catalog_entry_created'):
                    results['entries_created'] += 1
                if dataset_info.get('tags_applied'):
                    results['tags_applied'] += dataset_info['tags_applied']
                    
            except Exception as e:
                error_msg = f"Failed to process dataset {dataset.dataset_id}: {str(e)}"
                logger.warning(error_msg)
                results['errors'].append(error_msg)
                
    except Exception as e:
        error_msg = f"Failed to list BigQuery datasets: {str(e)}"
        logger.error(error_msg)
        results['errors'].append(error_msg)
    
    return results


def process_bigquery_dataset(dataset, project_id: str, location: str, comprehensive: bool) -> Dict[str, Any]:
    """
    Process a single BigQuery dataset and its tables.
    
    Args:
        dataset: BigQuery dataset reference
        project_id: Google Cloud project ID
        location: Region for Data Catalog entries
        comprehensive: Whether to perform deep analysis
        
    Returns:
        Dictionary containing dataset information and processing results
    """
    dataset_id = dataset.dataset_id
    logger.info(f"Processing BigQuery dataset: {dataset_id}")
    
    # Get detailed dataset metadata
    dataset_ref = bigquery_client.get_dataset(dataset.reference)
    
    dataset_info = {
        'dataset_id': dataset_id,
        'friendly_name': dataset_ref.friendly_name,
        'description': dataset_ref.description,
        'location': dataset_ref.location,
        'created': dataset_ref.created.isoformat() if dataset_ref.created else None,
        'modified': dataset_ref.modified.isoformat() if dataset_ref.modified else None,
        'tables': [],
        'catalog_entry_created': False,
        'tags_applied': 0,
        'quality_metrics': {}
    }
    
    try:
        # Create Data Catalog entry for the dataset
        entry_created = create_bigquery_catalog_entry(dataset_ref, project_id, location)
        dataset_info['catalog_entry_created'] = entry_created
        
        # Process tables within the dataset
        tables = list(bigquery_client.list_tables(dataset_ref))
        logger.info(f"Found {len(tables)} tables in dataset {dataset_id}")
        
        for table in tables:
            try:
                table_info = process_bigquery_table(table, dataset_ref, comprehensive)
                dataset_info['tables'].append(table_info)
                
                if table_info.get('tags_applied'):
                    dataset_info['tags_applied'] += table_info['tags_applied']
                    
            except Exception as e:
                logger.warning(f"Failed to process table {table.table_id}: {str(e)}")
        
        # Calculate dataset-level quality metrics
        if comprehensive and dataset_info['tables']:
            dataset_info['quality_metrics'] = calculate_dataset_quality_metrics(dataset_info['tables'])
        
    except Exception as e:
        logger.warning(f"Failed to create catalog entry for dataset {dataset_id}: {str(e)}")
    
    return dataset_info


def process_bigquery_table(table, dataset_ref, comprehensive: bool) -> Dict[str, Any]:
    """
    Process a single BigQuery table for metadata extraction.
    
    Args:
        table: BigQuery table reference
        dataset_ref: Parent dataset reference
        comprehensive: Whether to perform deep analysis
        
    Returns:
        Dictionary containing table information and quality metrics
    """
    table_ref = bigquery_client.get_table(table.reference)
    
    table_info = {
        'table_id': table_ref.table_id,
        'description': table_ref.description,
        'num_rows': table_ref.num_rows,
        'num_bytes': table_ref.num_bytes,
        'created': table_ref.created.isoformat() if table_ref.created else None,
        'modified': table_ref.modified.isoformat() if table_ref.modified else None,
        'schema_fields': len(table_ref.schema) if table_ref.schema else 0,
        'table_type': table_ref.table_type,
        'quality_metrics': {},
        'sensitivity_level': 'UNKNOWN',
        'tags_applied': 0
    }
    
    if comprehensive:
        # Calculate data quality metrics
        table_info['quality_metrics'] = calculate_table_quality_metrics(table_ref)
        
        # Infer data sensitivity level
        table_info['sensitivity_level'] = infer_data_sensitivity(table_ref)
        
        # Apply classification tags (simulated - would need actual tag template references)
        tags_applied = apply_table_classification_tags(table_ref, table_info['sensitivity_level'])
        table_info['tags_applied'] = tags_applied
    
    return table_info


def discover_storage_assets(project_id: str, location: str, comprehensive: bool = False) -> Dict[str, Any]:
    """
    Discover and catalog Cloud Storage buckets and objects.
    
    Args:
        project_id: Google Cloud project ID
        location: Region for Data Catalog entries
        comprehensive: Whether to perform deep analysis
        
    Returns:
        Dictionary containing discovered buckets and cataloging results
    """
    results = {
        'buckets': [],
        'entries_created': 0,
        'tags_applied': 0,
        'errors': []
    }
    
    try:
        # List all buckets in the project
        buckets = list(storage_client.list_buckets(project=project_id))
        logger.info(f"Found {len(buckets)} Cloud Storage buckets")
        
        for bucket in buckets:
            try:
                bucket_info = process_storage_bucket(bucket, project_id, location, comprehensive)
                results['buckets'].append(bucket_info)
                
                # Update counters
                if bucket_info.get('catalog_entry_created'):
                    results['entries_created'] += 1
                if bucket_info.get('tags_applied'):
                    results['tags_applied'] += bucket_info['tags_applied']
                    
            except Exception as e:
                error_msg = f"Failed to process bucket {bucket.name}: {str(e)}"
                logger.warning(error_msg)
                results['errors'].append(error_msg)
                
    except Exception as e:
        error_msg = f"Failed to list Storage buckets: {str(e)}"
        logger.error(error_msg)
        results['errors'].append(error_msg)
    
    return results


def process_storage_bucket(bucket, project_id: str, location: str, comprehensive: bool) -> Dict[str, Any]:
    """
    Process a single Cloud Storage bucket for metadata extraction.
    
    Args:
        bucket: Cloud Storage bucket object
        project_id: Google Cloud project ID
        location: Region for Data Catalog entries
        comprehensive: Whether to perform deep analysis
        
    Returns:
        Dictionary containing bucket information and analysis results
    """
    logger.info(f"Processing Storage bucket: {bucket.name}")
    
    bucket_info = {
        'bucket_name': bucket.name,
        'location': bucket.location,
        'storage_class': bucket.storage_class,
        'created': bucket.time_created.isoformat() if bucket.time_created else None,
        'updated': bucket.updated.isoformat() if bucket.updated else None,
        'versioning_enabled': bucket.versioning_enabled,
        'object_count': 0,
        'total_size_bytes': 0,
        'file_types': {},
        'sensitivity_level': 'UNKNOWN',
        'catalog_entry_created': False,
        'tags_applied': 0
    }
    
    try:
        # Sample bucket contents for analysis
        sample_limit = MAX_SAMPLE_SIZE if comprehensive else 100
        blobs = list(bucket.list_blobs(max_results=sample_limit))
        
        bucket_info['object_count'] = len(blobs)
        total_size = 0
        file_extensions = {}
        
        for blob in blobs:
            if blob.size:
                total_size += blob.size
            
            # Track file extensions
            if '.' in blob.name:
                ext = blob.name.split('.')[-1].lower()
                file_extensions[ext] = file_extensions.get(ext, 0) + 1
        
        bucket_info['total_size_bytes'] = total_size
        bucket_info['file_types'] = file_extensions
        
        if comprehensive:
            # Infer sensitivity based on bucket name and contents
            bucket_info['sensitivity_level'] = infer_bucket_sensitivity(bucket, blobs)
            
            # Apply classification tags (simulated)
            tags_applied = apply_bucket_classification_tags(bucket, bucket_info['sensitivity_level'])
            bucket_info['tags_applied'] = tags_applied
        
        # Create catalog entry (simulated - would need actual implementation)
        bucket_info['catalog_entry_created'] = True
        
    except Exception as e:
        logger.warning(f"Failed to analyze bucket contents for {bucket.name}: {str(e)}")
    
    return bucket_info


def create_bigquery_catalog_entry(dataset_ref, project_id: str, location: str) -> bool:
    """
    Create a Data Catalog entry for a BigQuery dataset.
    
    Args:
        dataset_ref: BigQuery dataset reference
        project_id: Google Cloud project ID
        location: Region for Data Catalog entry
        
    Returns:
        Boolean indicating success of entry creation
    """
    try:
        # This is a simplified implementation
        # In practice, you would create proper Data Catalog entries
        logger.info(f"Would create catalog entry for dataset: {dataset_ref.dataset_id}")
        return True
    except Exception as e:
        logger.error(f"Failed to create catalog entry: {str(e)}")
        return False


def calculate_table_quality_metrics(table_ref) -> Dict[str, float]:
    """
    Calculate data quality metrics for a BigQuery table.
    
    Args:
        table_ref: BigQuery table reference
        
    Returns:
        Dictionary containing quality metrics
    """
    metrics = {
        'completeness_score': 0.0,
        'accuracy_score': 0.0,
        'freshness_days': 0.0,
        'consistency_score': 0.0
    }
    
    try:
        if table_ref.num_rows and table_ref.num_rows > 0:
            # Estimate completeness based on table structure
            metrics['completeness_score'] = min(1.0, table_ref.num_rows / 1000000)
            
            # Estimate accuracy based on data volume and schema complexity
            schema_complexity = len(table_ref.schema) if table_ref.schema else 1
            metrics['accuracy_score'] = min(1.0, (table_ref.num_rows * schema_complexity) / 10000000)
            
            # Calculate freshness based on modification time
            if table_ref.modified:
                days_since_modified = (datetime.now(timezone.utc) - table_ref.modified.replace(tzinfo=timezone.utc)).days
                metrics['freshness_days'] = days_since_modified
                metrics['consistency_score'] = max(0.0, 1.0 - (days_since_modified / 30))
            else:
                metrics['consistency_score'] = 0.5  # Default for unknown modification time
        
    except Exception as e:
        logger.warning(f"Failed to calculate quality metrics: {str(e)}")
    
    return metrics


def calculate_dataset_quality_metrics(tables: List[Dict[str, Any]]) -> Dict[str, float]:
    """
    Calculate quality metrics for an entire dataset based on its tables.
    
    Args:
        tables: List of table information dictionaries
        
    Returns:
        Dictionary containing aggregated quality metrics
    """
    if not tables:
        return {}
    
    # Aggregate metrics across all tables
    total_completeness = sum(t.get('quality_metrics', {}).get('completeness_score', 0) for t in tables)
    total_accuracy = sum(t.get('quality_metrics', {}).get('accuracy_score', 0) for t in tables)
    total_consistency = sum(t.get('quality_metrics', {}).get('consistency_score', 0) for t in tables)
    
    return {
        'avg_completeness_score': total_completeness / len(tables),
        'avg_accuracy_score': total_accuracy / len(tables),
        'avg_consistency_score': total_consistency / len(tables),
        'table_count': len(tables)
    }


def infer_data_sensitivity(table_ref) -> str:
    """
    Infer data sensitivity level based on table and column names.
    
    Args:
        table_ref: BigQuery table reference
        
    Returns:
        String indicating sensitivity level
    """
    # Keywords that indicate different sensitivity levels
    restricted_keywords = ['pii', 'personal', 'credit', 'ssn', 'financial', 'medical', 'health', 'salary']
    confidential_keywords = ['internal', 'employee', 'revenue', 'profit', 'customer', 'client']
    internal_keywords = ['test', 'dev', 'staging', 'temp', 'log']
    
    # Check table name
    table_name_lower = table_ref.table_id.lower()
    dataset_name_lower = table_ref.dataset_id.lower()
    
    # Check for restricted data
    if any(keyword in table_name_lower or keyword in dataset_name_lower for keyword in restricted_keywords):
        return "RESTRICTED"
    
    # Check column names if schema is available
    if table_ref.schema:
        column_names = [field.name.lower() for field in table_ref.schema]
        if any(keyword in ' '.join(column_names) for keyword in restricted_keywords):
            return "RESTRICTED"
        if any(keyword in ' '.join(column_names) for keyword in confidential_keywords):
            return "CONFIDENTIAL"
    
    # Check for confidential data
    if any(keyword in table_name_lower or keyword in dataset_name_lower for keyword in confidential_keywords):
        return "CONFIDENTIAL"
    
    # Check for internal/test data
    if any(keyword in table_name_lower or keyword in dataset_name_lower for keyword in internal_keywords):
        return "INTERNAL"
    
    # Default to public for demo/sample data
    return "PUBLIC"


def infer_bucket_sensitivity(bucket, blobs: List) -> str:
    """
    Infer data sensitivity level for a Cloud Storage bucket.
    
    Args:
        bucket: Cloud Storage bucket object
        blobs: List of blob objects in the bucket
        
    Returns:
        String indicating sensitivity level
    """
    bucket_name_lower = bucket.name.lower()
    
    # Check bucket name for sensitivity indicators
    if any(keyword in bucket_name_lower for keyword in ['confidential', 'private', 'restricted', 'internal']):
        return "CONFIDENTIAL"
    
    if any(keyword in bucket_name_lower for keyword in ['public', 'open', 'demo', 'sample']):
        return "PUBLIC"
    
    # Analyze file names for sensitivity patterns
    if blobs:
        file_names = [blob.name.lower() for blob in blobs[:50]]  # Sample first 50 files
        file_content = ' '.join(file_names)
        
        if any(keyword in file_content for keyword in ['financial', 'personal', 'credit', 'ssn']):
            return "RESTRICTED"
        elif any(keyword in file_content for keyword in ['internal', 'employee', 'customer']):
            return "CONFIDENTIAL"
    
    return "INTERNAL"  # Default for unknown buckets


def apply_table_classification_tags(table_ref, sensitivity_level: str) -> int:
    """
    Apply classification tags to a BigQuery table (simulated).
    
    Args:
        table_ref: BigQuery table reference
        sensitivity_level: Inferred sensitivity level
        
    Returns:
        Number of tags applied
    """
    # This is a simulation - actual implementation would use Data Catalog APIs
    logger.info(f"Would apply classification tags to table {table_ref.table_id}: {sensitivity_level}")
    return 2  # Simulated: classification and quality tags


def apply_bucket_classification_tags(bucket, sensitivity_level: str) -> int:
    """
    Apply classification tags to a Cloud Storage bucket (simulated).
    
    Args:
        bucket: Cloud Storage bucket object
        sensitivity_level: Inferred sensitivity level
        
    Returns:
        Number of tags applied
    """
    # This is a simulation - actual implementation would use Data Catalog APIs
    logger.info(f"Would apply classification tags to bucket {bucket.name}: {sensitivity_level}")
    return 1  # Simulated: classification tag