# =============================================================================
# Large-Scale ML Training Pipeline - Terraform Variables Example
# =============================================================================
# Copy this file to terraform.tfvars and customize for your environment

# -----------------------------------------------------------------------------
# Required Variables
# -----------------------------------------------------------------------------

# Your Google Cloud Project ID
project_id = "your-gcp-project-id"

# Primary region for resources (TPU v6e availability)
# Recommended regions: us-central2, us-east4, europe-west4
region = "us-central2"

# Zone within the region for zonal resources
zone = "us-central2-b"

# -----------------------------------------------------------------------------
# Storage Configuration
# -----------------------------------------------------------------------------

# Prefix for Cloud Storage bucket (will be made unique)
bucket_name_prefix = "ml-training-pipeline"

# Storage class for the bucket
# Options: STANDARD, NEARLINE, COLDLINE, ARCHIVE
storage_class = "STANDARD"

# Enable object versioning for data protection
bucket_versioning_enabled = true

# Enable uniform bucket-level access for security
bucket_uniform_access = true

# Automatically delete bucket when destroying infrastructure
# WARNING: Set to false for production to prevent data loss
auto_delete_bucket = false

# -----------------------------------------------------------------------------
# TPU Configuration
# -----------------------------------------------------------------------------

# TPU instance name prefix
tpu_name_prefix = "training-tpu"

# TPU accelerator type - choose based on your model size and requirements
# Options: v6e-8, v6e-16, v6e-32, v6e-64, v6e-128, v6e-256
# v6e-8:   8 cores,  32GB HBM (~$2.40/hour)
# v6e-16:  16 cores, 64GB HBM (~$4.80/hour)
# v6e-32:  32 cores, 128GB HBM (~$9.60/hour)
tpu_accelerator_type = "v6e-8"

# TPU runtime version
tpu_runtime_version = "tpu-vm-v4-base"

# Network configuration for TPU
tpu_network = "default"
tpu_subnetwork = "default"

# Enable preemptible TPU for cost savings (up to 70% discount)
# NOTE: Preemptible instances can be terminated at any time
# Recommended: false for production, true for development/testing
enable_preemptible_tpu = false

# -----------------------------------------------------------------------------
# Dataproc Serverless Configuration
# -----------------------------------------------------------------------------

# Dataproc batch job ID prefix
dataproc_batch_id_prefix = "preprocessing"

# Spark executor memory allocation
dataproc_executor_memory = "4g"

# Spark driver memory allocation
dataproc_driver_memory = "2g"

# Maximum number of executors for auto-scaling
dataproc_max_executors = 10

# -----------------------------------------------------------------------------
# Vertex AI Configuration
# -----------------------------------------------------------------------------

# Display name for Vertex AI training jobs
vertex_ai_training_display_name = "tpu-v6e-training-pipeline"

# Container image for training
# Default uses Google's optimized TensorFlow 2.11 TPU image
vertex_ai_container_image = "gcr.io/deeplearning-platform-release/tf2-tpu.2-11"

# -----------------------------------------------------------------------------
# Monitoring and Alerting
# -----------------------------------------------------------------------------

# Enable Cloud Monitoring dashboards and metrics
enable_monitoring = true

# Name for the monitoring dashboard
monitoring_dashboard_name = "TPU v6e Training Dashboard"

# Notification channels for alerts (optional)
# Get channel IDs with: gcloud alpha monitoring channels list
alert_notification_channels = [
  # "projects/YOUR_PROJECT/notificationChannels/CHANNEL_ID_1",
  # "projects/YOUR_PROJECT/notificationChannels/CHANNEL_ID_2"
]

# -----------------------------------------------------------------------------
# Security Configuration
# -----------------------------------------------------------------------------

# Custom service account email (optional)
# Leave empty to use auto-created service account with minimal permissions
custom_service_account_email = ""

# Source IP ranges for firewall rules
# Restrict TPU access to specific networks for security
firewall_source_ranges = [
  "10.0.0.0/8",     # Private networks
  "172.16.0.0/12",  # Private networks
  "192.168.0.0/16"  # Private networks
]

# -----------------------------------------------------------------------------
# Resource Labels
# -----------------------------------------------------------------------------

# Labels applied to all resources for organization and cost tracking
resource_labels = {
  environment  = "development"        # or "staging", "production"
  team         = "ml-engineering"
  project      = "transformer-training"
  cost-center  = "research"
  managed-by   = "terraform"
  owner        = "data-science-team"
}

# =============================================================================
# Advanced Configuration Examples
# =============================================================================

# For production workloads:
# tpu_accelerator_type = "v6e-32"
# enable_preemptible_tpu = false
# dataproc_max_executors = 50
# storage_class = "STANDARD"

# For development/testing:
# tpu_accelerator_type = "v6e-8"
# enable_preemptible_tpu = true
# dataproc_max_executors = 5
# auto_delete_bucket = true

# For cost-optimized training:
# tpu_accelerator_type = "v6e-8"
# enable_preemptible_tpu = true
# storage_class = "NEARLINE"
# dataproc_executor_memory = "2g"

# For large-scale training:
# tpu_accelerator_type = "v6e-64"
# dataproc_max_executors = 100
# dataproc_executor_memory = "8g"
# dataproc_driver_memory = "8g"