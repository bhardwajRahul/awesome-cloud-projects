# ================================================================
# Google Cloud Platform Cross-Cloud Data Migration Infrastructure
# ================================================================
#
# This Terraform configuration deploys a complete cross-cloud data migration
# solution using Google Cloud Storage Transfer Service and Cloud Composer.
#
# Architecture Components:
# - Cloud Storage buckets for staging and target data
# - Cloud Composer environment for workflow orchestration
# - Storage Transfer Service jobs for cross-cloud migration
# - Service accounts with appropriate IAM permissions
# - Monitoring and logging infrastructure
# - Pub/Sub topics for notifications
#
# Author: Generated by Terraform Recipe Generator
# Version: 1.0
# ================================================================

# Configure random suffix for unique resource naming
resource "random_string" "suffix" {
  length  = 6
  special = false
  upper   = false
}

# ================================================================
# DATA SOURCES
# ================================================================

# Get current project information
data "google_project" "current" {}

# Get current client configuration
data "google_client_config" "current" {}

# Get Storage Transfer Service service account
data "google_storage_transfer_project_service_account" "default" {
  project = var.project_id
}

# ================================================================
# CLOUD STORAGE INFRASTRUCTURE
# ================================================================

# Staging bucket for temporary data processing during migration
resource "google_storage_bucket" "staging" {
  name          = "${var.staging_bucket_name}-${random_string.suffix.result}"
  location      = var.region
  storage_class = "STANDARD"
  project       = var.project_id

  # Enable versioning for data protection
  versioning {
    enabled = true
  }

  # Lifecycle management to control costs
  lifecycle_rule {
    condition {
      age = 30
    }
    action {
      type = "Delete"
    }
  }

  # Enable uniform bucket-level access for consistent security
  uniform_bucket_level_access = true

  # Prevent accidental deletion in production
  lifecycle {
    prevent_destroy = false
  }

  labels = {
    environment = var.environment
    purpose     = "data-migration-staging"
    managed_by  = "terraform"
  }
}

# Target bucket for final migrated data
resource "google_storage_bucket" "target" {
  name          = "${var.target_bucket_name}-${random_string.suffix.result}"
  location      = var.region
  storage_class = "STANDARD"
  project       = var.project_id

  # Enable versioning for data protection
  versioning {
    enabled = true
  }

  # Enable uniform bucket-level access for consistent security
  uniform_bucket_level_access = true

  # Prevent accidental deletion in production
  lifecycle {
    prevent_destroy = false
  }

  labels = {
    environment = var.environment
    purpose     = "data-migration-target"
    managed_by  = "terraform"
  }
}

# Logs bucket for storing migration logs
resource "google_storage_bucket" "logs" {
  name          = "${var.logs_bucket_name}-${random_string.suffix.result}"
  location      = var.region
  storage_class = "COLDLINE"
  project       = var.project_id

  # Lifecycle management for log retention
  lifecycle_rule {
    condition {
      age = 365
    }
    action {
      type = "Delete"
    }
  }

  # Enable uniform bucket-level access for consistent security
  uniform_bucket_level_access = true

  labels = {
    environment = var.environment
    purpose     = "migration-logs"
    managed_by  = "terraform"
  }
}

# ================================================================
# IAM CONFIGURATION
# ================================================================

# Service account for Storage Transfer Service operations
resource "google_service_account" "transfer_service" {
  account_id   = "storage-transfer-sa-${random_string.suffix.result}"
  display_name = "Storage Transfer Service Account"
  description  = "Service account for cross-cloud data migration operations"
  project      = var.project_id
}

# Grant Storage Transfer Service Admin role
resource "google_project_iam_member" "transfer_service_admin" {
  project = var.project_id
  role    = "roles/storagetransfer.admin"
  member  = "serviceAccount:${google_service_account.transfer_service.email}"
}

# Grant Storage Admin role for bucket operations
resource "google_project_iam_member" "transfer_service_storage" {
  project = var.project_id
  role    = "roles/storage.admin"
  member  = "serviceAccount:${google_service_account.transfer_service.email}"
}

# Grant IAM permissions for staging bucket
resource "google_storage_bucket_iam_member" "staging_bucket_admin" {
  bucket = google_storage_bucket.staging.name
  role   = "roles/storage.admin"
  member = "serviceAccount:${data.google_storage_transfer_project_service_account.default.email}"
}

# Grant IAM permissions for target bucket
resource "google_storage_bucket_iam_member" "target_bucket_admin" {
  bucket = google_storage_bucket.target.name
  role   = "roles/storage.admin"
  member = "serviceAccount:${data.google_storage_transfer_project_service_account.default.email}"
}

# Grant IAM permissions for logs bucket
resource "google_storage_bucket_iam_member" "logs_bucket_admin" {
  bucket = google_storage_bucket.logs.name
  role   = "roles/storage.admin"
  member = "serviceAccount:${data.google_storage_transfer_project_service_account.default.email}"
}

# ================================================================
# PUB/SUB NOTIFICATION INFRASTRUCTURE
# ================================================================

# Pub/Sub topic for transfer job notifications
resource "google_pubsub_topic" "transfer_notifications" {
  name    = "storage-transfer-notifications-${random_string.suffix.result}"
  project = var.project_id

  labels = {
    environment = var.environment
    purpose     = "transfer-notifications"
    managed_by  = "terraform"
  }
}

# Grant Pub/Sub publisher role to Storage Transfer Service
resource "google_pubsub_topic_iam_member" "transfer_notifications_publisher" {
  topic  = google_pubsub_topic.transfer_notifications.id
  role   = "roles/pubsub.publisher"
  member = "serviceAccount:${data.google_storage_transfer_project_service_account.default.email}"
}

# Pub/Sub subscription for monitoring transfer events
resource "google_pubsub_subscription" "transfer_events" {
  name    = "transfer-events-${random_string.suffix.result}"
  topic   = google_pubsub_topic.transfer_notifications.name
  project = var.project_id

  # Configure message retention
  message_retention_duration = "1209600s" # 14 days
  retain_acked_messages      = true

  # Configure acknowledgment deadline
  ack_deadline_seconds = 300

  labels = {
    environment = var.environment
    purpose     = "transfer-monitoring"
    managed_by  = "terraform"
  }
}

# ================================================================
# CLOUD COMPOSER ENVIRONMENT
# ================================================================

# Cloud Composer environment for workflow orchestration
resource "google_composer_environment" "migration_orchestrator" {
  name    = "${var.composer_environment_name}-${random_string.suffix.result}"
  region  = var.region
  project = var.project_id

  config {
    # Node configuration for the Composer environment
    node_config {
      zone         = var.zone
      machine_type = var.composer_machine_type
      disk_size_gb = var.composer_disk_size

      # Enable IP aliases for better networking
      ip_allocation_policy {
        use_ip_aliases = true
      }

      # Service account for Composer nodes
      service_account = google_service_account.transfer_service.email

      # OAuth scopes for Composer nodes
      oauth_scopes = [
        "https://www.googleapis.com/auth/cloud-platform",
        "https://www.googleapis.com/auth/pubsub",
        "https://www.googleapis.com/auth/storage-rw",
      ]

      # Network tags for security
      tags = ["composer", "migration"]
    }

    # Airflow configuration
    software_config {
      airflow_config_overrides = {
        "core-dags_are_paused_at_creation" = "False"
        "core-max_active_runs_per_dag"      = "1"
        "core-parallelism"                  = "32"
        "core-dag_concurrency"              = "16"
        "scheduler-catchup_by_default"      = "False"
      }

      # Environment variables for DAGs
      env_variables = {
        STAGING_BUCKET         = google_storage_bucket.staging.name
        TARGET_BUCKET          = google_storage_bucket.target.name
        LOGS_BUCKET            = google_storage_bucket.logs.name
        PUBSUB_TOPIC           = google_pubsub_topic.transfer_notifications.name
        TRANSFER_SERVICE_ACCOUNT = google_service_account.transfer_service.email
        PROJECT_ID             = var.project_id
        REGION                 = var.region
      }

      # Python version
      python_version = "3"

      # Airflow version
      airflow_version = "2.8.1"
    }

    # Node count configuration
    node_count = var.composer_node_count

    # Private IP configuration for enhanced security
    private_environment_config {
      enable_private_endpoint = var.enable_private_endpoint
    }

    # Database configuration
    database_config {
      machine_type = var.composer_db_machine_type
    }

    # Web server configuration
    web_server_config {
      machine_type = var.composer_web_server_machine_type
    }
  }

  # Prevent accidental deletion in production
  lifecycle {
    prevent_destroy = false
  }

  labels = {
    environment = var.environment
    purpose     = "migration-orchestration"
    managed_by  = "terraform"
  }

  depends_on = [
    google_service_account.transfer_service,
    google_storage_bucket.staging,
    google_storage_bucket.target,
    google_pubsub_topic.transfer_notifications,
  ]
}

# ================================================================
# STORAGE TRANSFER SERVICE CONFIGURATION
# ================================================================

# Storage Transfer Job for cross-cloud migration
resource "google_storage_transfer_job" "cross_cloud_migration" {
  count = var.create_sample_transfer_job ? 1 : 0

  description = "Cross-cloud data migration from ${var.source_provider} to Google Cloud Storage"
  project     = var.project_id

  # Transfer specification
  transfer_spec {
    # Configure source based on provider
    dynamic "aws_s3_data_source" {
      for_each = var.source_provider == "aws" ? [1] : []
      content {
        bucket_name = var.source_bucket_name
        aws_access_key {
          access_key_id     = var.aws_access_key_id
          secret_access_key = var.aws_secret_access_key
        }
        path = var.source_path
      }
    }

    dynamic "azure_blob_storage_data_source" {
      for_each = var.source_provider == "azure" ? [1] : []
      content {
        storage_account = var.azure_storage_account
        container       = var.azure_container
        path           = var.source_path
        azure_credentials {
          sas_token = var.azure_sas_token
        }
      }
    }

    dynamic "gcs_data_source" {
      for_each = var.source_provider == "gcp" ? [1] : []
      content {
        bucket_name = var.source_bucket_name
        path       = var.source_path
      }
    }

    # Target configuration
    gcs_data_sink {
      bucket_name = google_storage_bucket.staging.name
      path       = var.target_path
    }

    # Object conditions for selective transfer
    object_conditions {
      max_time_elapsed_since_last_modification = var.max_time_elapsed_since_last_modification
      exclude_prefixes                         = var.exclude_prefixes
      include_prefixes                         = var.include_prefixes
    }

    # Transfer options
    transfer_options {
      overwrite_objects_already_existing_in_sink = var.overwrite_objects_already_existing_in_sink
      delete_objects_unique_in_sink             = var.delete_objects_unique_in_sink
      delete_objects_from_source_after_transfer = var.delete_objects_from_source_after_transfer
    }
  }

  # Schedule configuration
  schedule {
    schedule_start_date {
      year  = var.schedule_start_year
      month = var.schedule_start_month
      day   = var.schedule_start_day
    }

    dynamic "schedule_end_date" {
      for_each = var.schedule_end_year != null ? [1] : []
      content {
        year  = var.schedule_end_year
        month = var.schedule_end_month
        day   = var.schedule_end_day
      }
    }

    dynamic "start_time_of_day" {
      for_each = var.schedule_start_hour != null ? [1] : []
      content {
        hours   = var.schedule_start_hour
        minutes = var.schedule_start_minute
        seconds = 0
        nanos   = 0
      }
    }

    repeat_interval = var.schedule_repeat_interval
  }

  # Notification configuration
  notification_config {
    pubsub_topic = google_pubsub_topic.transfer_notifications.id
    event_types = [
      "TRANSFER_OPERATION_SUCCESS",
      "TRANSFER_OPERATION_FAILED",
      "TRANSFER_OPERATION_ABORTED"
    ]
    payload_format = "JSON"
  }

  # Logging configuration
  logging_config {
    log_actions = [
      "FIND",
      "DELETE",
      "COPY"
    ]
    log_action_states = [
      "SUCCEEDED",
      "FAILED"
    ]
  }

  # Status configuration
  status = var.transfer_job_status

  depends_on = [
    google_storage_bucket.staging,
    google_pubsub_topic.transfer_notifications,
    google_pubsub_topic_iam_member.transfer_notifications_publisher,
  ]
}

# ================================================================
# MONITORING AND LOGGING
# ================================================================

# Log sink for Storage Transfer Service
resource "google_logging_project_sink" "storage_transfer_sink" {
  name        = "storage-transfer-sink-${random_string.suffix.result}"
  destination = "storage.googleapis.com/${google_storage_bucket.logs.name}/transfer-logs"

  filter = "resource.type=\"storage_transfer_job\""

  # Use a unique writer identity
  unique_writer_identity = true

  depends_on = [google_storage_bucket.logs]
}

# Grant sink writer permissions
resource "google_storage_bucket_iam_member" "transfer_sink_writer" {
  bucket = google_storage_bucket.logs.name
  role   = "roles/storage.objectCreator"
  member = google_logging_project_sink.storage_transfer_sink.writer_identity
}

# Log sink for Cloud Composer
resource "google_logging_project_sink" "composer_sink" {
  name        = "composer-migration-sink-${random_string.suffix.result}"
  destination = "storage.googleapis.com/${google_storage_bucket.logs.name}/composer-logs"

  filter = "resource.type=\"gce_instance\" AND resource.labels.instance_name:\"${google_composer_environment.migration_orchestrator.name}\""

  # Use a unique writer identity
  unique_writer_identity = true

  depends_on = [
    google_storage_bucket.logs,
    google_composer_environment.migration_orchestrator,
  ]
}

# Grant sink writer permissions for Composer logs
resource "google_storage_bucket_iam_member" "composer_sink_writer" {
  bucket = google_storage_bucket.logs.name
  role   = "roles/storage.objectCreator"
  member = google_logging_project_sink.composer_sink.writer_identity
}

# ================================================================
# MONITORING ALERT POLICIES
# ================================================================

# Notification channel for alerts (email)
resource "google_monitoring_notification_channel" "email_alerts" {
  count = var.alert_email != "" ? 1 : 0

  display_name = "Email Alerts for Data Migration"
  type         = "email"

  labels = {
    email_address = var.alert_email
  }

  depends_on = [google_composer_environment.migration_orchestrator]
}

# Alert policy for transfer job failures
resource "google_monitoring_alert_policy" "transfer_job_failures" {
  count = var.enable_monitoring_alerts ? 1 : 0

  display_name = "Storage Transfer Job Failures"
  combiner     = "OR"

  conditions {
    display_name = "Transfer job failure rate"

    condition_threshold {
      filter         = "resource.type=\"storage_transfer_job\""
      comparison     = "COMPARISON_GREATER_THAN"
      threshold_value = 0
      duration       = "300s"

      aggregations {
        alignment_period   = "300s"
        per_series_aligner = "ALIGN_RATE"
      }
    }
  }

  alert_strategy {
    auto_close = "86400s"
  }

  # Include notification channels if email is provided
  dynamic "notification_channels" {
    for_each = var.alert_email != "" ? [1] : []
    content {
      notification_channels = [google_monitoring_notification_channel.email_alerts[0].name]
    }
  }

  depends_on = [google_composer_environment.migration_orchestrator]
}

# Alert policy for Composer environment health
resource "google_monitoring_alert_policy" "composer_health" {
  count = var.enable_monitoring_alerts ? 1 : 0

  display_name = "Cloud Composer Environment Health"
  combiner     = "OR"

  conditions {
    display_name = "Composer environment down"

    condition_threshold {
      filter         = "resource.type=\"composer_environment\" AND resource.labels.environment_name=\"${google_composer_environment.migration_orchestrator.name}\""
      comparison     = "COMPARISON_EQUAL"
      threshold_value = 0
      duration       = "600s"

      aggregations {
        alignment_period   = "300s"
        per_series_aligner = "ALIGN_MEAN"
      }
    }
  }

  alert_strategy {
    auto_close = "86400s"
  }

  # Include notification channels if email is provided
  dynamic "notification_channels" {
    for_each = var.alert_email != "" ? [1] : []
    content {
      notification_channels = [google_monitoring_notification_channel.email_alerts[0].name]
    }
  }

  depends_on = [google_composer_environment.migration_orchestrator]
}