# ============================================================================
# Terraform Variables Example File
# ============================================================================
#
# Copy this file to terraform.tfvars and customize the values for your
# specific Google Cloud environment and requirements.
#
# cp terraform.tfvars.example terraform.tfvars
# vim terraform.tfvars
#
# ============================================================================

# ============================================================================
# REQUIRED CONFIGURATION
# ============================================================================

# Google Cloud Project ID where resources will be created
project_id = "your-gcp-project-id"

# Google Cloud region for resource deployment
region = "us-central1"

# Environment name (dev, test, staging, prod)
environment = "dev"

# List of email addresses to receive pipeline recovery notifications
notification_recipients = [
  "admin@company.com",
  "ops-team@company.com",
  "data-engineering@company.com"
]

# ============================================================================
# RESOURCE NAMING (Optional - defaults provided)
# ============================================================================

# Base names for resources (random suffix will be auto-generated)
# dataform_repository_name = "pipeline-recovery-repo"
# bigquery_dataset_name = "pipeline_monitoring"
# task_queue_name = "recovery-queue"
# controller_function_name = "pipeline-controller"
# worker_function_name = "recovery-worker"
# notification_function_name = "notification-handler"
# pubsub_topic_name = "pipeline-notifications"

# ============================================================================
# BIGQUERY CONFIGURATION (Optional)
# ============================================================================

# Number of days after which BigQuery tables will expire (0 = never expire)
# bigquery_table_expiration_days = 90

# ============================================================================
# CLOUD TASKS CONFIGURATION (Optional)
# ============================================================================

# Task queue rate limiting and retry configuration
# task_queue_max_dispatches_per_second = 10
# task_queue_max_concurrent_dispatches = 5
# task_queue_max_attempts = 5
# task_queue_max_retry_duration_hours = 1
# task_queue_min_backoff_seconds = 30
# task_queue_max_backoff_seconds = 300
# task_queue_max_doublings = 3
# task_queue_logging_sampling_ratio = 0.1

# ============================================================================
# CLOUD FUNCTIONS CONFIGURATION (Optional)
# ============================================================================

# Cloud Functions scaling configuration
# function_max_instances = 10
# function_min_instances = 0

# ============================================================================
# MONITORING AND ALERTING (Optional)
# ============================================================================

# Enable or disable Cloud Monitoring alerts
# enable_monitoring_alerts = true

# ============================================================================
# SECURITY CONFIGURATION (Optional)
# ============================================================================

# Enable Private Google Access for Cloud Functions
# enable_private_google_access = true

# Enable VPC Connector (requires existing VPC network)
# enable_vpc_connector = false
# vpc_connector_name = ""

# ============================================================================
# COST OPTIMIZATION (Optional)
# ============================================================================

# Function source bucket lifecycle management
# enable_function_source_bucket_lifecycle = true
# function_source_retention_days = 30

# ============================================================================
# ADVANCED CONFIGURATION (Optional)
# ============================================================================

# Enable enhanced logging for debugging
# enable_enhanced_logging = false

# Custom domain for webhook endpoints
# custom_domain = ""

# Backup region for disaster recovery
# backup_region = ""

# ============================================================================
# RESOURCE LABELING (Optional)
# ============================================================================

# Additional labels to apply to all resources
labels = {
  team            = "data-engineering"
  cost_center     = "analytics"
  application     = "pipeline-recovery"
  owner           = "data-team"
  business_unit   = "engineering"
}

# ============================================================================
# EXAMPLE CONFIGURATIONS FOR DIFFERENT ENVIRONMENTS
# ============================================================================

# Development Environment Example:
# project_id = "my-project-dev"
# environment = "dev"
# region = "us-central1"
# function_max_instances = 5
# task_queue_max_concurrent_dispatches = 3
# enable_monitoring_alerts = false
# bigquery_table_expiration_days = 30

# Staging Environment Example:
# project_id = "my-project-staging"
# environment = "staging"
# region = "us-central1"
# function_max_instances = 8
# task_queue_max_concurrent_dispatches = 5
# enable_monitoring_alerts = true
# bigquery_table_expiration_days = 60

# Production Environment Example:
# project_id = "my-project-prod"
# environment = "prod"
# region = "us-central1"
# function_max_instances = 20
# function_min_instances = 2
# task_queue_max_concurrent_dispatches = 10
# enable_monitoring_alerts = true
# enable_private_google_access = true
# bigquery_table_expiration_days = 365
# backup_region = "us-east1"