#!/bin/bash

# AWS EKS Ingress Controllers with AWS Load Balancer Controller - Cleanup Script
# This script safely removes all resources created by the deployment script
# Author: Generated by Claude Code for AWS Recipe
# Version: 1.0

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Error handler
error_handler() {
    local line_no=$1
    log_error "Script failed at line $line_no"
    log_error "Cleanup incomplete. Some resources may need manual removal."
    exit 1
}

trap 'error_handler $LINENO' ERR

# Default configuration
CLUSTER_NAME=""
NAMESPACE="ingress-demo"
IAM_POLICY_NAME="AWSLoadBalancerControllerIAMPolicy"
SERVICE_ACCOUNT_NAME="aws-load-balancer-controller"
HELM_RELEASE_NAME="aws-load-balancer-controller"
AWS_REGION=""

# Help function
show_help() {
    cat << EOF
AWS EKS Ingress Controllers Cleanup Script

USAGE:
    $0 [OPTIONS]

OPTIONS:
    -c, --cluster-name NAME     Specify EKS cluster name (required if no config file)
    -r, --region REGION         AWS region (default: from AWS CLI config)
    -n, --namespace NAME        Kubernetes namespace (default: $NAMESPACE)
    --keep-cluster              Keep EKS cluster (only remove ingress resources)
    --force                     Skip confirmation prompts
    --dry-run                   Show what would be deleted without making changes
    -h, --help                  Show this help message

EXAMPLES:
    $0                                          # Use saved configuration
    $0 -c my-cluster -r us-west-2             # Specify cluster name and region
    $0 --keep-cluster                          # Keep cluster, remove only ingress resources
    $0 --force                                 # Skip confirmations
    $0 --dry-run                               # Preview cleanup

The script will automatically load configuration from deployment-config.env if available.

EOF
}

# Parse command line arguments
KEEP_CLUSTER=false
FORCE=false
DRY_RUN=false
CUSTOM_CLUSTER_NAME=""
CUSTOM_REGION=""
CUSTOM_NAMESPACE=""

while [[ $# -gt 0 ]]; do
    case $1 in
        -c|--cluster-name)
            CUSTOM_CLUSTER_NAME="$2"
            shift 2
            ;;
        -r|--region)
            CUSTOM_REGION="$2"
            shift 2
            ;;
        -n|--namespace)
            CUSTOM_NAMESPACE="$2"
            shift 2
            ;;
        --keep-cluster)
            KEEP_CLUSTER=true
            shift
            ;;
        --force)
            FORCE=true
            shift
            ;;
        --dry-run)
            DRY_RUN=true
            shift
            ;;
        -h|--help)
            show_help
            exit 0
            ;;
        *)
            log_error "Unknown option: $1"
            show_help
            exit 1
            ;;
    esac
done

# Load configuration from file if available
load_config() {
    local config_file="deployment-config.env"
    
    if [ -f "$config_file" ]; then
        log_info "Loading configuration from $config_file"
        # shellcheck source=/dev/null
        source "$config_file"
        log_success "Configuration loaded"
    else
        log_warning "No configuration file found ($config_file)"
        log_info "You may need to specify cluster name and region manually"
    fi
}

# Set final configuration
set_config() {
    AWS_REGION="${CUSTOM_REGION:-${AWS_REGION:-$(aws configure get region)}}"
    CLUSTER_NAME="${CUSTOM_CLUSTER_NAME:-${CLUSTER_NAME}}"
    NAMESPACE="${CUSTOM_NAMESPACE:-${NAMESPACE}}"
    
    if [ -z "$CLUSTER_NAME" ]; then
        log_error "Cluster name not specified. Use -c option or ensure deployment-config.env exists"
        exit 1
    fi
    
    if [ -z "$AWS_REGION" ]; then
        log_error "AWS region not configured. Set region with: aws configure set region <region>"
        exit 1
    fi
    
    log_info "Using configuration:"
    log_info "  Cluster: $CLUSTER_NAME"
    log_info "  Region: $AWS_REGION"
    log_info "  Namespace: $NAMESPACE"
    log_info "  Keep cluster: $KEEP_CLUSTER"
}

# Prerequisites check
check_prerequisites() {
    log_info "Checking prerequisites..."
    
    local missing_tools=()
    
    # Check required tools
    for tool in aws kubectl helm; do
        if ! command -v "$tool" &> /dev/null; then
            missing_tools+=("$tool")
        fi
    done
    
    if [ "$KEEP_CLUSTER" = false ]; then
        if ! command -v eksctl &> /dev/null; then
            missing_tools+=("eksctl")
        fi
    fi
    
    if [ ${#missing_tools[@]} -ne 0 ]; then
        log_error "Missing required tools: ${missing_tools[*]}"
        log_error "Please install missing tools and try again"
        exit 1
    fi
    
    # Check AWS CLI configuration
    if ! aws sts get-caller-identity &> /dev/null; then
        log_error "AWS CLI not configured or invalid credentials"
        exit 1
    fi
    
    # Get AWS account ID
    AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
    
    log_success "Prerequisites check passed"
    log_info "AWS Account ID: $AWS_ACCOUNT_ID"
}

# Dry run summary
show_dry_run() {
    cat << EOF

${BLUE}=== DRY RUN MODE ===${NC}
The following resources would be deleted:

${YELLOW}Kubernetes Resources:${NC}
  - Ingress resources in namespace: $NAMESPACE
  - Services (including NLB) in namespace: $NAMESPACE
  - Deployments in namespace: $NAMESPACE
  - Namespace: $NAMESPACE
  - AWS Load Balancer Controller (Helm)

${YELLOW}IAM Resources:${NC}
  - Service Account: $SERVICE_ACCOUNT_NAME
  - IAM Role: AmazonEKSLoadBalancerControllerRole-$CLUSTER_NAME
  - IAM Policy: $IAM_POLICY_NAME (if not used elsewhere)

${YELLOW}AWS Resources (created by controller):${NC}
  - Application Load Balancers
  - Network Load Balancer
  - Target Groups
  - Security Groups
  - S3 Bucket for access logs

EOF

    if [ "$KEEP_CLUSTER" = false ]; then
        cat << EOF
${YELLOW}EKS Cluster:${NC}
  - Cluster: $CLUSTER_NAME
  - Node Groups
  - Associated IAM roles
  - VPC and networking (if created by eksctl)

EOF
    else
        echo -e "${YELLOW}EKS Cluster:${NC} Will be preserved"
        echo
    fi

    echo "To proceed with actual cleanup, run without --dry-run flag."
    echo
}

# Connect to cluster
connect_cluster() {
    log_info "Connecting to cluster: $CLUSTER_NAME"
    
    # Verify cluster exists
    if ! aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" &> /dev/null; then
        log_error "Cluster $CLUSTER_NAME not found in region $AWS_REGION"
        exit 1
    fi
    
    # Update kubeconfig
    aws eks update-kubeconfig --region "$AWS_REGION" --name "$CLUSTER_NAME"
    
    # Verify connectivity
    if ! kubectl get nodes &> /dev/null; then
        log_error "Cannot connect to cluster $CLUSTER_NAME"
        exit 1
    fi
    
    log_success "Connected to cluster: $CLUSTER_NAME"
}

# Delete ingress resources
delete_ingress() {
    log_info "Deleting ingress resources..."
    
    # Check if namespace exists
    if ! kubectl get namespace "$NAMESPACE" &> /dev/null; then
        log_warning "Namespace $NAMESPACE does not exist, skipping ingress cleanup"
        return 0
    fi
    
    # Delete all ingresses (this will trigger ALB deletion)
    log_info "Deleting ingress resources..."
    if kubectl get ingress -n "$NAMESPACE" &> /dev/null; then
        kubectl delete ingress --all -n "$NAMESPACE" || log_warning "Some ingresses may have already been deleted"
        
        # Wait for ALBs to be deleted
        log_info "Waiting for Application Load Balancers to be deleted (this may take 2-3 minutes)..."
        sleep 60
        
        # Check if ALBs are being deleted
        local alb_count
        alb_count=$(aws elbv2 describe-load-balancers \
            --query "length(LoadBalancers[?contains(LoadBalancerName, 'k8s-') && State.Code != 'deleting'])" \
            --output text 2>/dev/null || echo "0")
        
        if [ "$alb_count" -gt 0 ]; then
            log_info "Waiting for remaining ALBs to finish deleting..."
            for i in {1..10}; do
                sleep 30
                alb_count=$(aws elbv2 describe-load-balancers \
                    --query "length(LoadBalancers[?contains(LoadBalancerName, 'k8s-') && State.Code != 'deleting'])" \
                    --output text 2>/dev/null || echo "0")
                if [ "$alb_count" -eq 0 ]; then
                    break
                fi
                log_info "Still waiting for ALBs to delete... ($i/10)"
            done
        fi
    else
        log_info "No ingress resources found"
    fi
    
    log_success "Ingress resources deleted"
}

# Delete services (including NLB)
delete_services() {
    log_info "Deleting services..."
    
    if kubectl get namespace "$NAMESPACE" &> /dev/null; then
        # Delete LoadBalancer services (this will trigger NLB deletion)
        local lb_services
        lb_services=$(kubectl get service -n "$NAMESPACE" -o jsonpath='{.items[?(@.spec.type=="LoadBalancer")].metadata.name}' 2>/dev/null || echo "")
        
        if [ -n "$lb_services" ]; then
            log_info "Deleting LoadBalancer services: $lb_services"
            # shellcheck disable=SC2086
            kubectl delete service $lb_services -n "$NAMESPACE" || log_warning "Some services may have already been deleted"
            
            # Wait for NLBs to be deleted
            log_info "Waiting for Network Load Balancers to be deleted..."
            sleep 30
        fi
        
        # Delete remaining services
        kubectl delete service --all -n "$NAMESPACE" || log_warning "Some services may have already been deleted"
    else
        log_info "Namespace $NAMESPACE does not exist, skipping service cleanup"
    fi
    
    log_success "Services deleted"
}

# Delete applications and namespace
delete_applications() {
    log_info "Deleting applications and namespace..."
    
    if kubectl get namespace "$NAMESPACE" &> /dev/null; then
        # Delete deployments
        kubectl delete deployment --all -n "$NAMESPACE" || log_warning "Some deployments may have already been deleted"
        
        # Delete namespace
        kubectl delete namespace "$NAMESPACE" || log_warning "Namespace may have already been deleted"
        
        # Wait for namespace to be fully deleted
        log_info "Waiting for namespace to be fully deleted..."
        kubectl wait --for=delete namespace/"$NAMESPACE" --timeout=300s || log_warning "Namespace deletion timeout"
    else
        log_info "Namespace $NAMESPACE does not exist"
    fi
    
    log_success "Applications and namespace deleted"
}

# Uninstall AWS Load Balancer Controller
uninstall_controller() {
    log_info "Uninstalling AWS Load Balancer Controller..."
    
    # Check if Helm release exists
    if helm list -n kube-system | grep -q "$HELM_RELEASE_NAME"; then
        helm uninstall "$HELM_RELEASE_NAME" -n kube-system || log_warning "Controller may have already been uninstalled"
        
        # Wait for controller pods to be deleted
        log_info "Waiting for controller pods to be deleted..."
        kubectl wait --for=delete pod -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system --timeout=120s || log_warning "Controller pod deletion timeout"
    else
        log_info "AWS Load Balancer Controller Helm release not found"
    fi
    
    log_success "AWS Load Balancer Controller uninstalled"
}

# Delete IAM resources
delete_iam() {
    log_info "Deleting IAM resources..."
    
    # Delete service account (this will also delete the IAM role)
    if kubectl get serviceaccount "$SERVICE_ACCOUNT_NAME" -n kube-system &> /dev/null; then
        log_info "Deleting service account and IAM role..."
        if command -v eksctl &> /dev/null; then
            eksctl delete iamserviceaccount \
                --cluster="$CLUSTER_NAME" \
                --namespace=kube-system \
                --name="$SERVICE_ACCOUNT_NAME" || log_warning "Service account may have already been deleted"
        else
            log_warning "eksctl not available, skipping service account deletion"
            log_warning "You may need to manually delete the IAM role: AmazonEKSLoadBalancerControllerRole-$CLUSTER_NAME"
        fi
    else
        log_info "Service account $SERVICE_ACCOUNT_NAME not found"
    fi
    
    # Check if IAM policy should be deleted
    local policy_arn
    policy_arn=$(aws iam list-policies --query "Policies[?PolicyName=='$IAM_POLICY_NAME'].Arn" --output text 2>/dev/null || echo "")
    
    if [ -n "$policy_arn" ]; then
        # Check if policy is attached to any roles (excluding the one we just deleted)
        local attached_roles
        attached_roles=$(aws iam list-entities-for-policy --policy-arn "$policy_arn" \
            --query "PolicyRoles[?RoleName!='AmazonEKSLoadBalancerControllerRole-$CLUSTER_NAME'].RoleName" \
            --output text 2>/dev/null || echo "")
        
        if [ -z "$attached_roles" ] || [ "$attached_roles" = "None" ]; then
            log_info "Deleting IAM policy (not used by other roles)..."
            aws iam delete-policy --policy-arn "$policy_arn" || log_warning "Failed to delete IAM policy"
        else
            log_warning "IAM policy $IAM_POLICY_NAME is used by other roles, keeping it"
        fi
    else
        log_info "IAM policy $IAM_POLICY_NAME not found"
    fi
    
    log_success "IAM resources cleanup completed"
}

# Delete S3 bucket for access logs
delete_s3_bucket() {
    log_info "Cleaning up S3 buckets..."
    
    # Find S3 buckets with our naming pattern
    local bucket_pattern="alb-access-logs-"
    local buckets
    buckets=$(aws s3 ls | grep "$bucket_pattern" | awk '{print $3}' || echo "")
    
    if [ -n "$buckets" ]; then
        for bucket in $buckets; do
            log_info "Found S3 bucket: $bucket"
            if [ "$FORCE" = true ]; then
                aws s3 rb "s3://$bucket" --force || log_warning "Failed to delete bucket $bucket"
            else
                read -p "Delete S3 bucket $bucket? (y/N): " -n 1 -r
                echo
                if [[ $REPLY =~ ^[Yy]$ ]]; then
                    aws s3 rb "s3://$bucket" --force || log_warning "Failed to delete bucket $bucket"
                else
                    log_info "Skipping bucket $bucket"
                fi
            fi
        done
    else
        log_info "No S3 buckets found with pattern $bucket_pattern"
    fi
    
    log_success "S3 cleanup completed"
}

# Delete EKS cluster
delete_cluster() {
    if [ "$KEEP_CLUSTER" = true ]; then
        log_info "Keeping EKS cluster as requested"
        return 0
    fi
    
    log_info "Deleting EKS cluster: $CLUSTER_NAME"
    
    # Verify cluster exists
    if ! aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" &> /dev/null; then
        log_warning "Cluster $CLUSTER_NAME not found"
        return 0
    fi
    
    # Final confirmation for cluster deletion
    if [ "$FORCE" = false ]; then
        echo
        log_warning "This will DELETE the entire EKS cluster: $CLUSTER_NAME"
        log_warning "This action cannot be undone!"
        echo
        read -p "Are you absolutely sure you want to delete the cluster? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_info "Cluster deletion cancelled"
            return 0
        fi
    fi
    
    # Delete cluster using eksctl
    if command -v eksctl &> /dev/null; then
        eksctl delete cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" || log_error "Failed to delete cluster"
    else
        log_error "eksctl not available, cannot delete cluster automatically"
        log_error "You may need to delete the cluster manually via AWS console or AWS CLI"
        return 1
    fi
    
    log_success "EKS cluster deleted"
}

# Verify cleanup
verify_cleanup() {
    log_info "Verifying cleanup..."
    
    # Check for remaining AWS load balancers
    local remaining_lbs
    remaining_lbs=$(aws elbv2 describe-load-balancers \
        --query "LoadBalancers[?contains(LoadBalancerName, 'k8s-')].LoadBalancerName" \
        --output text 2>/dev/null || echo "")
    
    if [ -n "$remaining_lbs" ] && [ "$remaining_lbs" != "None" ]; then
        log_warning "Some load balancers may still exist: $remaining_lbs"
        log_warning "They may still be in the process of being deleted"
    else
        log_success "No AWS load balancers found"
    fi
    
    # Check cluster status
    if [ "$KEEP_CLUSTER" = false ]; then
        if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" &> /dev/null; then
            log_warning "Cluster $CLUSTER_NAME still exists (may still be deleting)"
        else
            log_success "Cluster $CLUSTER_NAME has been deleted"
        fi
    fi
    
    log_success "Cleanup verification completed"
}

# Clean up temporary files
cleanup_files() {
    log_info "Cleaning up temporary files..."
    
    # Remove IAM policy file
    if [ -f "iam-policy.json" ]; then
        rm -f "iam-policy.json"
        log_success "Removed iam-policy.json"
    fi
    
    # Optionally remove config file
    if [ -f "deployment-config.env" ] && [ "$FORCE" = true ]; then
        rm -f "deployment-config.env"
        log_success "Removed deployment-config.env"
    elif [ -f "deployment-config.env" ]; then
        log_info "Keeping deployment-config.env (use --force to remove)"
    fi
}

# Main cleanup function
main() {
    log_info "Starting AWS EKS Ingress Controllers cleanup..."
    log_info "Timestamp: $(date)"
    
    # Load configuration
    load_config
    
    # Set final configuration
    set_config
    
    # Check prerequisites
    check_prerequisites
    
    # Show dry run if requested
    if [ "$DRY_RUN" = true ]; then
        show_dry_run
        exit 0
    fi
    
    # Confirm cleanup
    if [ "$FORCE" = false ]; then
        echo
        log_warning "About to clean up AWS EKS Ingress Controllers with the following configuration:"
        echo "  Cluster: $CLUSTER_NAME"
        echo "  Region: $AWS_REGION"
        echo "  Namespace: $NAMESPACE"
        echo "  Keep cluster: $KEEP_CLUSTER"
        echo
        if [ "$KEEP_CLUSTER" = false ]; then
            log_warning "WARNING: This will delete the entire EKS cluster!"
        fi
        echo
        read -p "Continue with cleanup? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_info "Cleanup cancelled by user"
            exit 0
        fi
    fi
    
    # Connect to cluster (if it exists)
    if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" &> /dev/null; then
        connect_cluster
        
        # Execute cleanup steps in order
        delete_ingress
        delete_services
        delete_applications
        uninstall_controller
        delete_iam
    else
        log_warning "Cluster $CLUSTER_NAME not found, skipping Kubernetes resource cleanup"
    fi
    
    # Clean up AWS resources
    delete_s3_bucket
    
    # Delete cluster if requested
    delete_cluster
    
    # Verify cleanup
    verify_cleanup
    
    # Clean up local files
    cleanup_files
    
    log_success "Cleanup completed successfully!"
    echo
    log_info "Summary:"
    log_info "- Ingress resources and load balancers deleted"
    log_info "- Sample applications removed"
    log_info "- AWS Load Balancer Controller uninstalled"
    log_info "- IAM resources cleaned up"
    if [ "$KEEP_CLUSTER" = false ]; then
        log_info "- EKS cluster deleted"
    else
        log_info "- EKS cluster preserved"
    fi
    echo
    log_info "If you encounter any issues, check the AWS console for any remaining resources"
}

# Run main function
main "$@"