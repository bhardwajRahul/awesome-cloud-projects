#!/usr/bin/env python3
"""
AWS CDK Python Application for Database Disaster Recovery with Read Replicas

This CDK application deploys a comprehensive disaster recovery architecture using Amazon RDS
cross-region read replicas with automated failover mechanisms. The solution includes:

- Primary RDS MySQL instance in us-east-1
- Cross-region read replica in us-west-2
- CloudWatch alarms for monitoring database health
- SNS topics for notifications
- Lambda functions for automated failover coordination
- EventBridge rules for event-driven automation
- Route 53 health checks for DNS-level failover
- S3 bucket for configuration storage
- Comprehensive monitoring and alerting

Author: Generated by CDK Python for Recipe database-disaster-recovery-cross-region-read-replicas
Version: 1.0
"""

import os
from typing import Dict, Any

import aws_cdk as cdk
from aws_cdk import (
    Stack,
    Duration,
    RemovalPolicy,
    aws_rds as rds,
    aws_ec2 as ec2,
    aws_sns as sns,
    aws_lambda as lambda_,
    aws_events as events,
    aws_events_targets as targets,
    aws_cloudwatch as cloudwatch,
    aws_cloudwatch_actions as cw_actions,
    aws_iam as iam,
    aws_s3 as s3,
    aws_route53 as route53,
    aws_ssm as ssm,
    aws_logs as logs,
    CfnOutput,
    Environment,
)
from constructs import Construct


class DatabaseDisasterRecoveryStack(Stack):
    """
    CDK Stack for Database Disaster Recovery with Read Replicas
    
    This stack creates a comprehensive disaster recovery solution for RDS databases
    using cross-region read replicas and automated failover mechanisms.
    """

    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        primary_region: str = "us-east-1",
        dr_region: str = "us-west-2",
        **kwargs
    ) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # Configuration parameters
        self.primary_region = primary_region
        self.dr_region = dr_region
        self.db_instance_class = ec2.InstanceType.of(
            ec2.InstanceClass.T3, ec2.InstanceSize.MICRO
        )
        
        # Generate unique suffix for resource names
        unique_suffix = self.node.try_get_context("unique_suffix") or "dr"
        
        # Resource naming
        self.db_instance_id = f"primary-db-{unique_suffix}"
        self.db_replica_id = f"dr-replica-{unique_suffix}"
        self.sns_topic_primary = f"dr-primary-alerts-{unique_suffix}"
        self.sns_topic_dr = f"dr-failover-alerts-{unique_suffix}"
        self.lambda_function_primary = f"dr-coordinator-{unique_suffix}"
        self.lambda_function_dr = f"replica-promoter-{unique_suffix}"

        # Create VPC for database deployment
        self._create_vpc()
        
        # Create S3 bucket for configuration storage
        self._create_s3_bucket()
        
        # Create SNS topics for alerting
        self._create_sns_topics()
        
        # Create IAM roles for Lambda functions
        self._create_iam_roles()
        
        # Create RDS parameter group for optimized replication
        self._create_parameter_group()
        
        # Create primary RDS instance
        self._create_primary_database()
        
        # Create Lambda functions for disaster recovery automation
        self._create_lambda_functions()
        
        # Create CloudWatch alarms for monitoring
        self._create_cloudwatch_alarms()
        
        # Create EventBridge rules for automation
        self._create_eventbridge_rules()
        
        # Create Route 53 health checks
        self._create_route53_health_checks()
        
        # Create disaster recovery runbook in Parameter Store
        self._create_dr_runbook()
        
        # Create outputs for important resource identifiers
        self._create_outputs()

    def _create_vpc(self) -> None:
        """Create VPC with private subnets for RDS deployment"""
        self.vpc = ec2.Vpc(
            self,
            "DatabaseVPC",
            ip_addresses=ec2.IpAddresses.cidr("10.0.0.0/16"),
            max_azs=2,
            subnet_configuration=[
                ec2.SubnetConfiguration(
                    name="Private",
                    subnet_type=ec2.SubnetType.PRIVATE_ISOLATED,
                    cidr_mask=24,
                ),
                ec2.SubnetConfiguration(
                    name="Public",
                    subnet_type=ec2.SubnetType.PUBLIC,
                    cidr_mask=24,
                ),
            ],
            enable_dns_hostnames=True,
            enable_dns_support=True,
        )
        
        # Create VPC endpoint for Lambda functions to access AWS services
        self.vpc.add_interface_endpoint(
            "LambdaVPCEndpoint",
            service=ec2.InterfaceVpcEndpointAwsService.LAMBDA,
        )

    def _create_s3_bucket(self) -> None:
        """Create S3 bucket for disaster recovery configuration and logs"""
        self.config_bucket = s3.Bucket(
            self,
            "DRConfigBucket",
            bucket_name=f"dr-config-bucket-{self.account}-{self.region}",
            versioned=True,
            encryption=s3.BucketEncryption.S3_MANAGED,
            removal_policy=RemovalPolicy.DESTROY,
            auto_delete_objects=True,
            lifecycle_rules=[
                s3.LifecycleRule(
                    id="DeleteOldVersions",
                    enabled=True,
                    noncurrent_version_expiration=Duration.days(30),
                ),
            ],
        )

    def _create_sns_topics(self) -> None:
        """Create SNS topics for disaster recovery alerts"""
        # Primary region SNS topic
        self.primary_sns_topic = sns.Topic(
            self,
            "PrimaryAlertsTopic",
            topic_name=self.sns_topic_primary,
            display_name="Disaster Recovery Primary Alerts",
        )
        
        # Add email subscription (can be customized)
        admin_email = self.node.try_get_context("admin_email")
        if admin_email:
            self.primary_sns_topic.add_subscription(
                sns.EmailSubscription(admin_email)
            )

    def _create_iam_roles(self) -> None:
        """Create IAM roles for Lambda functions with necessary permissions"""
        # Role for DR coordinator Lambda function
        self.dr_coordinator_role = iam.Role(
            self,
            "DRCoordinatorRole",
            assumed_by=iam.ServicePrincipal("lambda.amazonaws.com"),
            managed_policies=[
                iam.ManagedPolicy.from_aws_managed_policy_name(
                    "service-role/AWSLambdaBasicExecutionRole"
                ),
                iam.ManagedPolicy.from_aws_managed_policy_name(
                    "service-role/AWSLambdaVPCAccessExecutionRole"
                ),
            ],
            inline_policies={
                "DRCoordinatorPolicy": iam.PolicyDocument(
                    statements=[
                        iam.PolicyStatement(
                            effect=iam.Effect.ALLOW,
                            actions=[
                                "rds:DescribeDBInstances",
                                "rds:PromoteReadReplica",
                                "sns:Publish",
                                "ssm:PutParameter",
                                "ssm:GetParameter",
                                "route53:GetHealthCheck",
                                "route53:ChangeResourceRecordSets",
                                "logs:CreateLogGroup",
                                "logs:CreateLogStream",
                                "logs:PutLogEvents",
                            ],
                            resources=["*"],
                        ),
                    ]
                )
            },
        )
        
        # Role for replica promoter Lambda function
        self.replica_promoter_role = iam.Role(
            self,
            "ReplicaPromoterRole",
            assumed_by=iam.ServicePrincipal("lambda.amazonaws.com"),
            managed_policies=[
                iam.ManagedPolicy.from_aws_managed_policy_name(
                    "service-role/AWSLambdaBasicExecutionRole"
                ),
            ],
            inline_policies={
                "ReplicaPromoterPolicy": iam.PolicyDocument(
                    statements=[
                        iam.PolicyStatement(
                            effect=iam.Effect.ALLOW,
                            actions=[
                                "rds:DescribeDBInstances",
                                "sns:Publish",
                                "ssm:PutParameter",
                                "ssm:GetParameter",
                                "route53:ChangeResourceRecordSets",
                                "logs:CreateLogGroup",
                                "logs:CreateLogStream",
                                "logs:PutLogEvents",
                            ],
                            resources=["*"],
                        ),
                    ]
                )
            },
        )
        
        # Role for RDS enhanced monitoring
        self.rds_monitoring_role = iam.Role(
            self,
            "RDSMonitoringRole",
            assumed_by=iam.ServicePrincipal("monitoring.rds.amazonaws.com"),
            managed_policies=[
                iam.ManagedPolicy.from_aws_managed_policy_name(
                    "service-role/AmazonRDSEnhancedMonitoringRole"
                ),
            ],
        )

    def _create_parameter_group(self) -> None:
        """Create RDS parameter group optimized for replication"""
        self.parameter_group = rds.ParameterGroup(
            self,
            "ReplicationOptimizedParameterGroup",
            engine=rds.DatabaseInstanceEngine.mysql(
                version=rds.MysqlEngineVersion.VER_8_0_35
            ),
            description="Optimized parameters for cross-region replication",
            parameters={
                "innodb_flush_log_at_trx_commit": "2",
                "sync_binlog": "0",
                "binlog_format": "ROW",
            },
        )

    def _create_primary_database(self) -> None:
        """Create primary RDS MySQL instance with disaster recovery configuration"""
        # Create subnet group for RDS
        subnet_group = rds.SubnetGroup(
            self,
            "DatabaseSubnetGroup",
            description="Subnet group for disaster recovery database",
            vpc=self.vpc,
            vpc_subnets=ec2.SubnetSelection(
                subnet_type=ec2.SubnetType.PRIVATE_ISOLATED
            ),
        )
        
        # Create security group for RDS
        db_security_group = ec2.SecurityGroup(
            self,
            "DatabaseSecurityGroup",
            vpc=self.vpc,
            description="Security group for disaster recovery database",
            allow_all_outbound=False,
        )
        
        # Allow MySQL access from within VPC
        db_security_group.add_ingress_rule(
            peer=ec2.Peer.ipv4(self.vpc.vpc_cidr_block),
            connection=ec2.Port.tcp(3306),
            description="MySQL access from VPC",
        )
        
        # Create primary database instance
        self.primary_database = rds.DatabaseInstance(
            self,
            "PrimaryDatabase",
            instance_identifier=self.db_instance_id,
            engine=rds.DatabaseInstanceEngine.mysql(
                version=rds.MysqlEngineVersion.VER_8_0_35
            ),
            instance_type=self.db_instance_class,
            vpc=self.vpc,
            subnet_group=subnet_group,
            security_groups=[db_security_group],
            parameter_group=self.parameter_group,
            allocated_storage=20,
            storage_type=rds.StorageType.GP2,
            storage_encrypted=True,
            multi_az=False,  # Single AZ for cost optimization
            backup_retention=Duration.days(7),
            deletion_protection=True,
            monitoring_interval=Duration.seconds(60),
            monitoring_role=self.rds_monitoring_role,
            enable_performance_insights=True,
            performance_insight_retention=rds.PerformanceInsightRetention.DEFAULT,
            cloudwatch_logs_exports=["error", "general", "slow-query"],
            removal_policy=RemovalPolicy.SNAPSHOT,
        )

    def _create_lambda_functions(self) -> None:
        """Create Lambda functions for disaster recovery automation"""
        # DR Coordinator Lambda function
        self.dr_coordinator_function = lambda_.Function(
            self,
            "DRCoordinatorFunction",
            function_name=self.lambda_function_primary,
            runtime=lambda_.Runtime.PYTHON_3_9,
            handler="index.lambda_handler",
            role=self.dr_coordinator_role,
            timeout=Duration.seconds(60),
            environment={
                "DR_REGION": self.dr_region,
                "DB_REPLICA_ID": self.db_replica_id,
                "PRIMARY_SNS_TOPIC_ARN": self.primary_sns_topic.topic_arn,
            },
            code=lambda_.Code.from_inline("""
import json
import boto3
import os
from datetime import datetime

def lambda_handler(event, context):
    \"\"\"
    Disaster Recovery Coordinator Lambda Function
    Handles primary database failure detection and initiates failover
    \"\"\"
    
    # Initialize AWS clients
    dr_region = os.environ['DR_REGION']
    rds = boto3.client('rds', region_name=dr_region)
    sns = boto3.client('sns')
    ssm = boto3.client('ssm')
    
    try:
        # Parse CloudWatch alarm from SNS message
        sns_message = json.loads(event['Records'][0]['Sns']['Message'])
        alarm_name = sns_message['AlarmName']
        
        print(f"Processing alarm: {alarm_name}")
        
        # Check if this is a database connection failure
        if 'database-connection-failure' in alarm_name:
            # Initiate disaster recovery procedure
            replica_id = os.environ['DB_REPLICA_ID']
            
            # Check replica status before promotion
            replica_status = rds.describe_db_instances(
                DBInstanceIdentifier=replica_id
            )['DBInstances'][0]['DBInstanceStatus']
            
            if replica_status == 'available':
                print(f"Promoting read replica {replica_id} to standalone instance")
                
                # Promote read replica
                rds.promote_read_replica(
                    DBInstanceIdentifier=replica_id
                )
                
                # Update parameter store with failover status
                ssm.put_parameter(
                    Name='/disaster-recovery/failover-status',
                    Value=json.dumps({
                        'status': 'in-progress',
                        'timestamp': datetime.utcnow().isoformat(),
                        'replica_id': replica_id
                    }),
                    Type='String',
                    Overwrite=True
                )
                
                # Send success notification
                sns.publish(
                    TopicArn=os.environ['PRIMARY_SNS_TOPIC_ARN'],
                    Subject='Disaster Recovery Initiated',
                    Message=f'Read replica {replica_id} promotion started. Monitor for completion.'
                )
                
                return {
                    'statusCode': 200,
                    'body': json.dumps(f'DR procedure initiated for {replica_id}')
                }
            else:
                print(f"Replica {replica_id} is not available for promotion: {replica_status}")
                return {
                    'statusCode': 400,
                    'body': json.dumps(f'Replica not ready for promotion: {replica_status}')
                }
        
    except Exception as e:
        print(f"Error in DR coordinator: {str(e)}")
        sns.publish(
            TopicArn=os.environ['PRIMARY_SNS_TOPIC_ARN'],
            Subject='Disaster Recovery Error',
            Message=f'Error in DR coordinator: {str(e)}'
        )
        return {
            'statusCode': 500,
            'body': json.dumps(f'Error: {str(e)}')
        }
            """),
        )
        
        # Grant SNS permission to invoke DR coordinator Lambda
        self.dr_coordinator_function.add_permission(
            "SNSInvokePermission",
            principal=iam.ServicePrincipal("sns.amazonaws.com"),
            source_arn=self.primary_sns_topic.topic_arn,
        )

    def _create_cloudwatch_alarms(self) -> None:
        """Create CloudWatch alarms for database monitoring"""
        # Database connection failure alarm
        self.db_connection_alarm = cloudwatch.Alarm(
            self,
            "DatabaseConnectionFailureAlarm",
            alarm_name=f"{self.db_instance_id}-database-connection-failure",
            alarm_description="Alarm when database connection fails",
            metric=cloudwatch.Metric(
                namespace="AWS/RDS",
                metric_name="DatabaseConnections",
                dimensions_map={
                    "DBInstanceIdentifier": self.primary_database.instance_identifier
                },
                statistic="Average",
                period=Duration.seconds(60),
            ),
            threshold=0,
            comparison_operator=cloudwatch.ComparisonOperator.LESS_THAN_THRESHOLD,
            evaluation_periods=3,
            treat_missing_data=cloudwatch.TreatMissingData.BREACHING,
        )
        
        # Add SNS action to alarm
        self.db_connection_alarm.add_alarm_action(
            cw_actions.SnsAction(self.primary_sns_topic)
        )
        
        # CPU utilization alarm for primary database
        self.db_cpu_alarm = cloudwatch.Alarm(
            self,
            "DatabaseCPUAlarm",
            alarm_name=f"{self.db_instance_id}-cpu-utilization-high",
            alarm_description="Alarm when CPU exceeds 80%",
            metric=cloudwatch.Metric(
                namespace="AWS/RDS",
                metric_name="CPUUtilization",
                dimensions_map={
                    "DBInstanceIdentifier": self.primary_database.instance_identifier
                },
                statistic="Average",
                period=Duration.seconds(300),
            ),
            threshold=80,
            comparison_operator=cloudwatch.ComparisonOperator.GREATER_THAN_THRESHOLD,
            evaluation_periods=2,
        )
        
        # Add SNS action to CPU alarm
        self.db_cpu_alarm.add_alarm_action(
            cw_actions.SnsAction(self.primary_sns_topic)
        )

    def _create_eventbridge_rules(self) -> None:
        """Create EventBridge rules for RDS event automation"""
        # EventBridge rule for RDS promotion events
        self.rds_promotion_rule = events.Rule(
            self,
            "RDSPromotionEventsRule",
            rule_name="rds-promotion-events",
            description="Capture RDS promotion events for DR automation",
            event_pattern=events.EventPattern(
                source=["aws.rds"],
                detail_type=["RDS DB Instance Event"],
                detail={
                    "eventName": ["promote-read-replica"]
                },
            ),
        )

    def _create_route53_health_checks(self) -> None:
        """Create Route 53 health checks for DNS-level failover"""
        # Health check for primary database alarm
        self.primary_health_check = route53.CfnHealthCheck(
            self,
            "PrimaryDatabaseHealthCheck",
            type="CLOUDWATCH_METRIC",
            cloudwatch_alarm_region=self.region,
            alarm_identifier=route53.CfnHealthCheck.AlarmIdentifierProperty(
                name=self.db_connection_alarm.alarm_name,
                region=self.region,
            ),
            insufficient_data_health_status="Failure",
        )

    def _create_dr_runbook(self) -> None:
        """Create disaster recovery runbook in Systems Manager Parameter Store"""
        dr_runbook = {
            "disaster_recovery_runbook": {
                "version": "1.0",
                "primary_region": self.primary_region,
                "dr_region": self.dr_region,
                "resources": {
                    "primary_db": self.db_instance_id,
                    "replica_db": self.db_replica_id,
                    "sns_primary": self.primary_sns_topic.topic_arn,
                    "lambda_coordinator": self.lambda_function_primary,
                },
                "manual_failover_steps": [
                    "1. Verify primary database is truly unavailable",
                    "2. Check replica lag is minimal (< 5 minutes)",
                    f"3. Promote replica using: aws rds promote-read-replica --db-instance-identifier {self.db_replica_id} --region {self.dr_region}",
                    "4. Wait for promotion to complete",
                    "5. Update application connection strings",
                    "6. Verify application functionality",
                    "7. Update DNS records if needed"
                ],
                "rollback_steps": [
                    "1. Create new read replica from promoted instance",
                    "2. Switch applications back to original region",
                    "3. Verify data consistency",
                    "4. Clean up temporary resources"
                ]
            }
        }
        
        self.dr_runbook_parameter = ssm.StringParameter(
            self,
            "DRRunbookParameter",
            parameter_name="/disaster-recovery/runbook",
            string_value=json.dumps(dr_runbook, indent=2),
            description="Disaster recovery runbook for database failover procedures",
            tier=ssm.ParameterTier.STANDARD,
        )

    def _create_outputs(self) -> None:
        """Create CloudFormation outputs for important resource identifiers"""
        CfnOutput(
            self,
            "PrimaryDatabaseEndpoint",
            value=self.primary_database.instance_endpoint.hostname,
            description="Primary database endpoint",
            export_name=f"{self.stack_name}-PrimaryDatabaseEndpoint",
        )
        
        CfnOutput(
            self,
            "PrimaryDatabaseInstanceId",
            value=self.primary_database.instance_identifier,
            description="Primary database instance identifier",
            export_name=f"{self.stack_name}-PrimaryDatabaseInstanceId",
        )
        
        CfnOutput(
            self,
            "SNSTopicArn",
            value=self.primary_sns_topic.topic_arn,
            description="SNS topic ARN for disaster recovery alerts",
            export_name=f"{self.stack_name}-SNSTopicArn",
        )
        
        CfnOutput(
            self,
            "DRCoordinatorFunctionArn",
            value=self.dr_coordinator_function.function_arn,
            description="DR coordinator Lambda function ARN",
            export_name=f"{self.stack_name}-DRCoordinatorFunctionArn",
        )
        
        CfnOutput(
            self,
            "ConfigBucketName",
            value=self.config_bucket.bucket_name,
            description="S3 bucket for disaster recovery configuration",
            export_name=f"{self.stack_name}-ConfigBucketName",
        )
        
        CfnOutput(
            self,
            "DRRunbookParameterName",
            value=self.dr_runbook_parameter.parameter_name,
            description="Systems Manager parameter containing DR runbook",
            export_name=f"{self.stack_name}-DRRunbookParameterName",
        )


class DatabaseDisasterRecoveryReplicaStack(Stack):
    """
    CDK Stack for the disaster recovery region components
    
    This stack should be deployed in the DR region and creates:
    - Cross-region read replica
    - DR region SNS topic
    - Replica promoter Lambda function
    - DR region monitoring
    """

    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        primary_stack_exports: Dict[str, str],
        **kwargs
    ) -> None:
        super().__init__(scope, construct_id, **kwargs)
        
        # Import values from primary stack
        self.primary_db_instance_id = primary_stack_exports["primary_db_instance_id"]
        self.primary_region = primary_stack_exports["primary_region"]
        self.account_id = primary_stack_exports["account_id"]
        
        # Configuration
        unique_suffix = self.node.try_get_context("unique_suffix") or "dr"
        self.db_replica_id = f"dr-replica-{unique_suffix}"
        self.sns_topic_dr = f"dr-failover-alerts-{unique_suffix}"
        self.lambda_function_dr = f"replica-promoter-{unique_suffix}"
        
        # Create DR region components
        self._create_dr_sns_topic()
        self._create_dr_iam_roles()
        self._create_read_replica()
        self._create_replica_promoter_lambda()
        self._create_dr_cloudwatch_alarms()
        self._create_dr_outputs()

    def _create_dr_sns_topic(self) -> None:
        """Create SNS topic for DR region alerts"""
        self.dr_sns_topic = sns.Topic(
            self,
            "DRAlertsTopic",
            topic_name=self.sns_topic_dr,
            display_name="Disaster Recovery DR Region Alerts",
        )
        
        # Add email subscription if provided
        admin_email = self.node.try_get_context("admin_email")
        if admin_email:
            self.dr_sns_topic.add_subscription(
                sns.EmailSubscription(admin_email)
            )

    def _create_dr_iam_roles(self) -> None:
        """Create IAM roles for DR region Lambda functions"""
        self.replica_promoter_role = iam.Role(
            self,
            "ReplicaPromoterRole",
            assumed_by=iam.ServicePrincipal("lambda.amazonaws.com"),
            managed_policies=[
                iam.ManagedPolicy.from_aws_managed_policy_name(
                    "service-role/AWSLambdaBasicExecutionRole"
                ),
            ],
            inline_policies={
                "ReplicaPromoterPolicy": iam.PolicyDocument(
                    statements=[
                        iam.PolicyStatement(
                            effect=iam.Effect.ALLOW,
                            actions=[
                                "rds:DescribeDBInstances",
                                "sns:Publish",
                                "ssm:PutParameter",
                                "ssm:GetParameter",
                                "route53:ChangeResourceRecordSets",
                                "logs:CreateLogGroup",
                                "logs:CreateLogStream",
                                "logs:PutLogEvents",
                            ],
                            resources=["*"],
                        ),
                    ]
                )
            },
        )

    def _create_read_replica(self) -> None:
        """Create cross-region read replica"""
        # Note: In CDK, cross-region read replicas need to be created programmatically
        # or through custom resources due to cross-region dependencies
        
        # For now, we'll create a placeholder that documents the replica creation
        # In practice, this would be handled by a custom resource or separate process
        self.replica_parameter = ssm.StringParameter(
            self,
            "ReplicaInstructions",
            parameter_name="/disaster-recovery/replica-creation-instructions",
            string_value=f"""
To create the cross-region read replica, run the following AWS CLI command:

aws rds create-db-instance-read-replica \\
    --region {self.region} \\
    --db-instance-identifier {self.db_replica_id} \\
    --source-db-instance-identifier arn:aws:rds:{self.primary_region}:{self.account_id}:db:{self.primary_db_instance_id} \\
    --db-instance-class db.t3.micro \\
    --publicly-accessible false \\
    --auto-minor-version-upgrade true \\
    --monitoring-interval 60 \\
    --enable-performance-insights \\
    --tags Key=Purpose,Value=DisasterRecovery Key=Environment,Value=Production Key=Role,Value=ReadReplica

This parameter will be updated once the replica is created.
            """,
            description="Instructions for creating cross-region read replica",
        )

    def _create_replica_promoter_lambda(self) -> None:
        """Create Lambda function for replica promotion handling"""
        self.replica_promoter_function = lambda_.Function(
            self,
            "ReplicaPromoterFunction",
            function_name=self.lambda_function_dr,
            runtime=lambda_.Runtime.PYTHON_3_9,
            handler="index.lambda_handler",
            role=self.replica_promoter_role,
            timeout=Duration.seconds(300),
            environment={
                "DR_REGION": self.region,
                "DR_SNS_TOPIC_ARN": self.dr_sns_topic.topic_arn,
            },
            code=lambda_.Code.from_inline("""
import json
import boto3
import os
from datetime import datetime

def lambda_handler(event, context):
    \"\"\"
    Replica Promoter Lambda Function
    Handles post-promotion tasks and DNS updates
    \"\"\"
    
    # Initialize AWS clients
    rds = boto3.client('rds', region_name=os.environ['DR_REGION'])
    route53 = boto3.client('route53')
    ssm = boto3.client('ssm', region_name=os.environ['DR_REGION'])
    sns = boto3.client('sns', region_name=os.environ['DR_REGION'])
    
    try:
        # Check if this is a promotion completion event
        if 'source' in event and event['source'] == 'aws.rds':
            detail = event['detail']
            
            if detail['eventName'] == 'promote-read-replica' and detail['responseElements']:
                db_instance_id = detail['responseElements']['dBInstanceIdentifier']
                
                print(f"Processing promotion completion for {db_instance_id}")
                
                # Wait for instance to be available
                waiter = rds.get_waiter('db_instance_available')
                waiter.wait(
                    DBInstanceIdentifier=db_instance_id,
                    WaiterConfig={'Delay': 30, 'MaxAttempts': 20}
                )
                
                # Get promoted instance details
                instance = rds.describe_db_instances(
                    DBInstanceIdentifier=db_instance_id
                )['DBInstances'][0]
                
                new_endpoint = instance['Endpoint']['Address']
                
                # Update parameter store
                ssm.put_parameter(
                    Name='/disaster-recovery/failover-status',
                    Value=json.dumps({
                        'status': 'completed',
                        'timestamp': datetime.utcnow().isoformat(),
                        'new_endpoint': new_endpoint,
                        'db_instance_id': db_instance_id
                    }),
                    Type='String',
                    Overwrite=True
                )
                
                # Send completion notification
                sns.publish(
                    TopicArn=os.environ['DR_SNS_TOPIC_ARN'],
                    Subject='Disaster Recovery Completed',
                    Message=f'''
Disaster recovery promotion completed successfully.

New Database Endpoint: {new_endpoint}
Instance ID: {db_instance_id}
Completion Time: {datetime.utcnow().isoformat()}

Please update application configurations to use the new endpoint.
                    '''
                )
                
                return {
                    'statusCode': 200,
                    'body': json.dumps('Promotion completion processed successfully')
                }
        
    except Exception as e:
        print(f"Error in replica promoter: {str(e)}")
        sns.publish(
            TopicArn=os.environ['DR_SNS_TOPIC_ARN'],
            Subject='Disaster Recovery Error',
            Message=f'Error in replica promoter: {str(e)}'
        )
        return {
            'statusCode': 500,
            'body': json.dumps(f'Error: {str(e)}')
        }
            """),
        )

    def _create_dr_cloudwatch_alarms(self) -> None:
        """Create CloudWatch alarms for DR region monitoring"""
        # Note: This would monitor the read replica once it's created
        # For now, we create a placeholder alarm structure
        
        # Create custom metric for DR readiness
        self.dr_readiness_metric = cloudwatch.Metric(
            namespace="Custom/DisasterRecovery",
            metric_name="DRReadiness",
            statistic="Average",
            period=Duration.seconds(300),
        )

    def _create_dr_outputs(self) -> None:
        """Create outputs for DR region resources"""
        CfnOutput(
            self,
            "DRSNSTopicArn",
            value=self.dr_sns_topic.topic_arn,
            description="DR region SNS topic ARN",
            export_name=f"{self.stack_name}-DRSNSTopicArn",
        )
        
        CfnOutput(
            self,
            "ReplicaPromoterFunctionArn",
            value=self.replica_promoter_function.function_arn,
            description="Replica promoter Lambda function ARN",
            export_name=f"{self.stack_name}-ReplicaPromoterFunctionArn",
        )
        
        CfnOutput(
            self,
            "ReplicaInstructionsParameter",
            value=self.replica_parameter.parameter_name,
            description="Parameter containing read replica creation instructions",
            export_name=f"{self.stack_name}-ReplicaInstructionsParameter",
        )


class DatabaseDisasterRecoveryApp(cdk.App):
    """
    CDK Application for Database Disaster Recovery
    
    This application creates a multi-region disaster recovery solution
    for RDS databases using cross-region read replicas.
    """

    def __init__(self) -> None:
        super().__init__()
        
        # Configuration
        primary_region = self.node.try_get_context("primary_region") or "us-east-1"
        dr_region = self.node.try_get_context("dr_region") or "us-west-2"
        account = self.node.try_get_context("account") or os.environ.get("CDK_DEFAULT_ACCOUNT")
        
        # Primary region stack
        primary_stack = DatabaseDisasterRecoveryStack(
            self,
            "DatabaseDisasterRecoveryPrimary",
            primary_region=primary_region,
            dr_region=dr_region,
            env=Environment(account=account, region=primary_region),
        )
        
        # DR region stack
        primary_exports = {
            "primary_db_instance_id": primary_stack.db_instance_id,
            "primary_region": primary_region,
            "account_id": account,
        }
        
        dr_stack = DatabaseDisasterRecoveryReplicaStack(
            self,
            "DatabaseDisasterRecoveryDR",
            primary_stack_exports=primary_exports,
            env=Environment(account=account, region=dr_region),
        )
        
        # Add dependency to ensure primary stack deploys first
        dr_stack.add_dependency(primary_stack)


# Create and run the CDK application
app = DatabaseDisasterRecoveryApp()
app.synth()