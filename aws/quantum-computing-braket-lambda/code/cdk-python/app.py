#!/usr/bin/env python3
"""
CDK Python application for building hybrid quantum-classical computing pipelines
with Amazon Braket and Lambda.

This application deploys a complete quantum computing infrastructure including:
- Lambda functions for quantum job orchestration
- S3 buckets for data and code storage
- IAM roles and policies for secure access
- CloudWatch monitoring and dashboards
- Quantum algorithm deployment pipeline

Author: Generated by AWS CDK
Version: 1.0.0
"""

import os
from typing import Dict, List, Any
import aws_cdk as cdk
from aws_cdk import (
    Stack,
    Duration,
    RemovalPolicy,
    aws_lambda as _lambda,
    aws_s3 as s3,
    aws_iam as iam,
    aws_logs as logs,
    aws_cloudwatch as cloudwatch,
    aws_events as events,
    aws_events_targets as targets,
    aws_s3_deployment as s3_deployment,
)
from constructs import Construct


class QuantumComputingPipelineStack(Stack):
    """
    CDK Stack for hybrid quantum-classical computing pipeline using Amazon Braket and Lambda.
    
    This stack creates:
    - Lambda functions for quantum job orchestration
    - S3 buckets for data, code, and results storage
    - IAM roles with appropriate permissions
    - CloudWatch monitoring infrastructure
    - EventBridge rules for job monitoring
    """

    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        project_name: str = "quantum-pipeline",
        **kwargs
    ) -> None:
        """
        Initialize the quantum computing pipeline stack.
        
        Args:
            scope: The scope in which to define this construct
            construct_id: The scoped construct ID
            project_name: Name prefix for all resources
            **kwargs: Additional keyword arguments
        """
        super().__init__(scope, construct_id, **kwargs)
        
        self.project_name = project_name
        self.account_id = Stack.of(self).account
        self.region = Stack.of(self).region
        
        # Create S3 buckets for quantum computing pipeline
        self.create_s3_buckets()
        
        # Create IAM roles and policies
        self.create_iam_resources()
        
        # Create Lambda functions
        self.create_lambda_functions()
        
        # Create CloudWatch resources
        self.create_cloudwatch_resources()
        
        # Create EventBridge rules
        self.create_eventbridge_rules()
        
        # Create outputs
        self.create_outputs()

    def create_s3_buckets(self) -> None:
        """Create S3 buckets for quantum computing pipeline data storage."""
        
        # Input bucket for quantum optimization problems
        self.input_bucket = s3.Bucket(
            self,
            "InputBucket",
            bucket_name=f"{self.project_name}-input-{self.account_id}",
            versioned=True,
            encryption=s3.BucketEncryption.S3_MANAGED,
            block_public_access=s3.BlockPublicAccess.BLOCK_ALL,
            removal_policy=RemovalPolicy.DESTROY,
            auto_delete_objects=True,
            lifecycle_rules=[
                s3.LifecycleRule(
                    id="delete-old-versions",
                    enabled=True,
                    noncurrent_version_expiration=Duration.days(30),
                    abort_incomplete_multipart_upload_after=Duration.days(7),
                )
            ],
        )
        
        # Output bucket for quantum results and analysis
        self.output_bucket = s3.Bucket(
            self,
            "OutputBucket",
            bucket_name=f"{self.project_name}-output-{self.account_id}",
            versioned=True,
            encryption=s3.BucketEncryption.S3_MANAGED,
            block_public_access=s3.BlockPublicAccess.BLOCK_ALL,
            removal_policy=RemovalPolicy.DESTROY,
            auto_delete_objects=True,
            lifecycle_rules=[
                s3.LifecycleRule(
                    id="transition-to-ia",
                    enabled=True,
                    transitions=[
                        s3.Transition(
                            storage_class=s3.StorageClass.INFREQUENT_ACCESS,
                            transition_after=Duration.days(30),
                        ),
                        s3.Transition(
                            storage_class=s3.StorageClass.GLACIER,
                            transition_after=Duration.days(90),
                        ),
                    ],
                )
            ],
        )
        
        # Code bucket for quantum algorithms and Lambda code
        self.code_bucket = s3.Bucket(
            self,
            "CodeBucket",
            bucket_name=f"{self.project_name}-code-{self.account_id}",
            versioned=True,
            encryption=s3.BucketEncryption.S3_MANAGED,
            block_public_access=s3.BlockPublicAccess.BLOCK_ALL,
            removal_policy=RemovalPolicy.DESTROY,
            auto_delete_objects=True,
        )
        
        # Add tags to all buckets
        for bucket in [self.input_bucket, self.output_bucket, self.code_bucket]:
            cdk.Tags.of(bucket).add("Project", self.project_name)
            cdk.Tags.of(bucket).add("Purpose", "QuantumComputing")
            cdk.Tags.of(bucket).add("ManagedBy", "CDK")

    def create_iam_resources(self) -> None:
        """Create IAM roles and policies for quantum computing pipeline."""
        
        # Lambda execution role with quantum computing permissions
        self.lambda_role = iam.Role(
            self,
            "LambdaExecutionRole",
            role_name=f"{self.project_name}-execution-role",
            assumed_by=iam.CompositePrincipal(
                iam.ServicePrincipal("lambda.amazonaws.com"),
                iam.ServicePrincipal("braket.amazonaws.com"),
            ),
            managed_policies=[
                iam.ManagedPolicy.from_aws_managed_policy_name(
                    "service-role/AWSLambdaBasicExecutionRole"
                ),
                iam.ManagedPolicy.from_aws_managed_policy_name(
                    "AmazonBraketFullAccess"
                ),
            ],
        )
        
        # Custom policy for S3 and CloudWatch access
        quantum_policy = iam.Policy(
            self,
            "QuantumPipelinePolicy",
            policy_name=f"{self.project_name}-policy",
            statements=[
                iam.PolicyStatement(
                    effect=iam.Effect.ALLOW,
                    actions=[
                        "s3:GetObject",
                        "s3:PutObject",
                        "s3:DeleteObject",
                        "s3:ListBucket",
                        "s3:GetBucketLocation",
                        "s3:GetBucketVersioning",
                    ],
                    resources=[
                        self.input_bucket.bucket_arn,
                        f"{self.input_bucket.bucket_arn}/*",
                        self.output_bucket.bucket_arn,
                        f"{self.output_bucket.bucket_arn}/*",
                        self.code_bucket.bucket_arn,
                        f"{self.code_bucket.bucket_arn}/*",
                    ],
                ),
                iam.PolicyStatement(
                    effect=iam.Effect.ALLOW,
                    actions=[
                        "cloudwatch:PutMetricData",
                        "cloudwatch:CreateLogGroup",
                        "cloudwatch:CreateLogStream",
                        "cloudwatch:PutLogEvents",
                        "cloudwatch:DescribeAlarms",
                        "cloudwatch:GetMetricStatistics",
                    ],
                    resources=["*"],
                ),
                iam.PolicyStatement(
                    effect=iam.Effect.ALLOW,
                    actions=[
                        "lambda:InvokeFunction",
                        "lambda:InvokeAsync",
                    ],
                    resources=[
                        f"arn:aws:lambda:{self.region}:{self.account_id}:function:{self.project_name}-*"
                    ],
                ),
                iam.PolicyStatement(
                    effect=iam.Effect.ALLOW,
                    actions=[
                        "braket:SearchJobs",
                        "braket:GetJob",
                        "braket:CreateJob",
                        "braket:CancelJob",
                        "braket:GetDevice",
                        "braket:SearchDevices",
                        "braket:CreateQuantumTask",
                        "braket:GetQuantumTask",
                    ],
                    resources=["*"],
                ),
            ],
        )
        
        quantum_policy.attach_to_role(self.lambda_role)

    def create_lambda_functions(self) -> None:
        """Create Lambda functions for quantum computing pipeline orchestration."""
        
        # Common Lambda configuration
        common_config = {
            "runtime": _lambda.Runtime.PYTHON_3_9,
            "role": self.lambda_role,
            "environment": {
                "PROJECT_NAME": self.project_name,
                "INPUT_BUCKET": self.input_bucket.bucket_name,
                "OUTPUT_BUCKET": self.output_bucket.bucket_name,
                "CODE_BUCKET": self.code_bucket.bucket_name,
                "EXECUTION_ROLE_ARN": self.lambda_role.role_arn,
            },
            "log_retention": logs.RetentionDays.ONE_WEEK,
            "tracing": _lambda.Tracing.ACTIVE,
        }
        
        # Data preparation Lambda function
        self.data_prep_function = _lambda.Function(
            self,
            "DataPreparationFunction",
            function_name=f"{self.project_name}-data-preparation",
            code=_lambda.Code.from_inline(self._get_data_preparation_code()),
            handler="index.lambda_handler",
            timeout=Duration.seconds(60),
            memory_size=256,
            description="Prepare and validate data for quantum optimization",
            **common_config,
        )
        
        # Job submission Lambda function
        self.job_submission_function = _lambda.Function(
            self,
            "JobSubmissionFunction",
            function_name=f"{self.project_name}-job-submission",
            code=_lambda.Code.from_inline(self._get_job_submission_code()),
            handler="index.lambda_handler",
            timeout=Duration.seconds(300),
            memory_size=512,
            description="Submit quantum optimization jobs to Amazon Braket",
            **common_config,
        )
        
        # Job monitoring Lambda function
        self.job_monitoring_function = _lambda.Function(
            self,
            "JobMonitoringFunction",
            function_name=f"{self.project_name}-job-monitoring",
            code=_lambda.Code.from_inline(self._get_job_monitoring_code()),
            handler="index.lambda_handler",
            timeout=Duration.seconds(300),
            memory_size=256,
            description="Monitor quantum job status and trigger post-processing",
            **common_config,
        )
        
        # Post-processing Lambda function
        self.post_processing_function = _lambda.Function(
            self,
            "PostProcessingFunction",
            function_name=f"{self.project_name}-post-processing",
            code=_lambda.Code.from_inline(self._get_post_processing_code()),
            handler="index.lambda_handler",
            timeout=Duration.seconds(900),
            memory_size=1024,
            description="Post-process quantum optimization results",
            layers=[
                _lambda.LayerVersion.from_layer_version_arn(
                    self,
                    "MatplotlibLayer",
                    # Note: This layer ARN may need to be updated based on region
                    layer_version_arn=f"arn:aws:lambda:{self.region}:336392948345:layer:AWSSDKPandas-Python39:2",
                )
            ],
            **common_config,
        )
        
        # Add tags to all Lambda functions
        for func in [
            self.data_prep_function,
            self.job_submission_function,
            self.job_monitoring_function,
            self.post_processing_function,
        ]:
            cdk.Tags.of(func).add("Project", self.project_name)
            cdk.Tags.of(func).add("Purpose", "QuantumComputing")
            cdk.Tags.of(func).add("ManagedBy", "CDK")

    def create_cloudwatch_resources(self) -> None:
        """Create CloudWatch dashboard and alarms for monitoring."""
        
        # Create CloudWatch dashboard
        self.dashboard = cloudwatch.Dashboard(
            self,
            "QuantumPipelineDashboard",
            dashboard_name=f"{self.project_name}-quantum-pipeline",
            widgets=[
                [
                    cloudwatch.GraphWidget(
                        title="Quantum Pipeline Activity",
                        width=12,
                        height=6,
                        left=[
                            cloudwatch.Metric(
                                namespace="QuantumPipeline",
                                metric_name="JobsSubmitted",
                                dimensions_map={"DeviceType": "simulator"},
                                statistic="Sum",
                                period=Duration.minutes(5),
                            ),
                            cloudwatch.Metric(
                                namespace="QuantumPipeline",
                                metric_name="JobsSubmitted",
                                dimensions_map={"DeviceType": "qpu"},
                                statistic="Sum",
                                period=Duration.minutes(5),
                            ),
                            cloudwatch.Metric(
                                namespace="QuantumPipeline",
                                metric_name="ProblemsProcessed",
                                dimensions_map={"ProblemType": "optimization"},
                                statistic="Sum",
                                period=Duration.minutes(5),
                            ),
                        ],
                    ),
                    cloudwatch.GraphWidget(
                        title="Optimization Performance",
                        width=12,
                        height=6,
                        left=[
                            cloudwatch.Metric(
                                namespace="QuantumPipeline",
                                metric_name="OptimizationEfficiency",
                                statistic="Average",
                                period=Duration.minutes(5),
                            ),
                            cloudwatch.Metric(
                                namespace="QuantumPipeline",
                                metric_name="FinalCost",
                                statistic="Average",
                                period=Duration.minutes(5),
                            ),
                            cloudwatch.Metric(
                                namespace="QuantumPipeline",
                                metric_name="ConvergenceRate",
                                statistic="Average",
                                period=Duration.minutes(5),
                            ),
                        ],
                    ),
                ],
                [
                    cloudwatch.LogQueryWidget(
                        title="Quantum Pipeline Logs",
                        width=24,
                        height=6,
                        log_groups=[
                            logs.LogGroup.from_log_group_name(
                                self,
                                "DataPrepLogGroup",
                                log_group_name=f"/aws/lambda/{self.project_name}-data-preparation",
                            ),
                            logs.LogGroup.from_log_group_name(
                                self,
                                "JobSubmissionLogGroup",
                                log_group_name=f"/aws/lambda/{self.project_name}-job-submission",
                            ),
                            logs.LogGroup.from_log_group_name(
                                self,
                                "JobMonitoringLogGroup",
                                log_group_name=f"/aws/lambda/{self.project_name}-job-monitoring",
                            ),
                            logs.LogGroup.from_log_group_name(
                                self,
                                "PostProcessingLogGroup",
                                log_group_name=f"/aws/lambda/{self.project_name}-post-processing",
                            ),
                        ],
                        query_lines=[
                            "fields @timestamp, @message",
                            "filter @message like /quantum/",
                            "sort @timestamp desc",
                            "limit 100",
                        ],
                    ),
                ],
            ],
        )
        
        # Create CloudWatch alarms
        self.job_failure_alarm = cloudwatch.Alarm(
            self,
            "JobFailureAlarm",
            alarm_name=f"{self.project_name}-job-failure-rate",
            alarm_description="Monitor quantum job failure rate",
            metric=cloudwatch.Metric(
                namespace="QuantumPipeline",
                metric_name="JobStatusUpdate",
                dimensions_map={"JobStatus": "FAILED"},
                statistic="Sum",
                period=Duration.minutes(15),
            ),
            threshold=3,
            evaluation_periods=2,
            comparison_operator=cloudwatch.ComparisonOperator.GREATER_THAN_THRESHOLD,
        )
        
        self.efficiency_alarm = cloudwatch.Alarm(
            self,
            "EfficiencyAlarm",
            alarm_name=f"{self.project_name}-low-efficiency",
            alarm_description="Monitor quantum optimization efficiency",
            metric=cloudwatch.Metric(
                namespace="QuantumPipeline",
                metric_name="OptimizationEfficiency",
                statistic="Average",
                period=Duration.minutes(10),
            ),
            threshold=0.3,
            evaluation_periods=2,
            comparison_operator=cloudwatch.ComparisonOperator.LESS_THAN_THRESHOLD,
        )

    def create_eventbridge_rules(self) -> None:
        """Create EventBridge rules for job monitoring automation."""
        
        # Rule to trigger job monitoring on a schedule
        self.monitoring_rule = events.Rule(
            self,
            "MonitoringRule",
            rule_name=f"{self.project_name}-monitoring-schedule",
            description="Schedule regular monitoring of quantum jobs",
            schedule=events.Schedule.rate(Duration.minutes(5)),
            enabled=True,
        )
        
        # Add Lambda target to monitoring rule
        self.monitoring_rule.add_target(
            targets.LambdaFunction(
                self.job_monitoring_function,
                event=events.RuleTargetInput.from_object({
                    "source": "eventbridge.schedule",
                    "detail-type": "Scheduled Event",
                    "detail": {
                        "monitoring_type": "scheduled",
                        "project_name": self.project_name,
                    },
                }),
            )
        )

    def create_outputs(self) -> None:
        """Create CloudFormation outputs for the quantum computing pipeline."""
        
        cdk.CfnOutput(
            self,
            "InputBucketName",
            value=self.input_bucket.bucket_name,
            description="S3 bucket for quantum optimization input data",
            export_name=f"{self.project_name}-input-bucket",
        )
        
        cdk.CfnOutput(
            self,
            "OutputBucketName",
            value=self.output_bucket.bucket_name,
            description="S3 bucket for quantum optimization results",
            export_name=f"{self.project_name}-output-bucket",
        )
        
        cdk.CfnOutput(
            self,
            "CodeBucketName",
            value=self.code_bucket.bucket_name,
            description="S3 bucket for quantum algorithm code",
            export_name=f"{self.project_name}-code-bucket",
        )
        
        cdk.CfnOutput(
            self,
            "DataPreparationFunctionName",
            value=self.data_prep_function.function_name,
            description="Lambda function for data preparation",
            export_name=f"{self.project_name}-data-prep-function",
        )
        
        cdk.CfnOutput(
            self,
            "JobSubmissionFunctionName",
            value=self.job_submission_function.function_name,
            description="Lambda function for quantum job submission",
            export_name=f"{self.project_name}-job-submission-function",
        )
        
        cdk.CfnOutput(
            self,
            "DashboardURL",
            value=f"https://{self.region}.console.aws.amazon.com/cloudwatch/home?region={self.region}#dashboards:name={self.project_name}-quantum-pipeline",
            description="CloudWatch dashboard URL for monitoring",
            export_name=f"{self.project_name}-dashboard-url",
        )
        
        cdk.CfnOutput(
            self,
            "ExecutionRoleArn",
            value=self.lambda_role.role_arn,
            description="IAM role ARN for Lambda and Braket execution",
            export_name=f"{self.project_name}-execution-role-arn",
        )

    def _get_data_preparation_code(self) -> str:
        """Get the Lambda function code for data preparation."""
        return '''
import json
import boto3
import numpy as np
from datetime import datetime

s3 = boto3.client('s3')
cloudwatch = boto3.client('cloudwatch')

def lambda_handler(event, context):
    """Prepare and validate data for quantum optimization."""
    try:
        # Extract input parameters
        input_bucket = event.get('input_bucket', os.environ.get('INPUT_BUCKET'))
        output_bucket = event.get('output_bucket', os.environ.get('OUTPUT_BUCKET'))
        problem_type = event.get('problem_type', 'optimization')
        problem_size = event.get('problem_size', 4)
        
        print(f"Preparing data for {problem_type} problem of size {problem_size}")
        
        # Generate or load optimization problem data
        if problem_type == 'optimization':
            # Create quadratic optimization problem coefficients
            np.random.seed(42)  # For reproducible results
            linear_coeffs = np.random.uniform(-2, 2, problem_size)
            quadratic_matrix = np.random.uniform(-1, 1, (problem_size, problem_size))
            # Ensure symmetric matrix for valid optimization
            quadratic_matrix = (quadratic_matrix + quadratic_matrix.T) / 2
            
            problem_data = {
                'linear_coefficients': linear_coeffs.tolist(),
                'quadratic_matrix': quadratic_matrix.tolist(),
                'problem_size': problem_size,
                'problem_type': problem_type,
                'created_at': datetime.utcnow().isoformat()
            }
        
        # Validate problem constraints
        validation_result = validate_quantum_problem(problem_data)
        if not validation_result['valid']:
            raise ValueError(f"Problem validation failed: {validation_result['error']}")
        
        # Store prepared data in S3
        data_key = f"prepared_data_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"
        s3.put_object(
            Bucket=input_bucket,
            Key=data_key,
            Body=json.dumps(problem_data, indent=2)
        )
        
        # Send metrics to CloudWatch
        cloudwatch.put_metric_data(
            Namespace='QuantumPipeline',
            MetricData=[
                {
                    'MetricName': 'ProblemsProcessed',
                    'Value': 1,
                    'Unit': 'Count',
                    'Dimensions': [
                        {'Name': 'ProblemType', 'Value': problem_type},
                        {'Name': 'ProblemSize', 'Value': str(problem_size)}
                    ]
                }
            ]
        )
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Data preparation completed successfully',
                'data_key': data_key,
                'problem_type': problem_type,
                'problem_size': problem_size,
                'validation': validation_result
            })
        }
        
    except Exception as e:
        print(f"Error in data preparation: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }

def validate_quantum_problem(problem_data):
    """Validate quantum problem constraints and requirements."""
    try:
        problem_size = problem_data['problem_size']
        problem_type = problem_data['problem_type']
        
        # Check problem size constraints
        if problem_size < 2 or problem_size > 20:
            return {'valid': False, 'error': 'Problem size must be between 2 and 20 qubits'}
        
        # Validate problem-specific constraints
        if problem_type == 'optimization':
            if len(problem_data['linear_coefficients']) != problem_size:
                return {'valid': False, 'error': 'Linear coefficients size mismatch'}
            
            quad_matrix = np.array(problem_data['quadratic_matrix'])
            if quad_matrix.shape != (problem_size, problem_size):
                return {'valid': False, 'error': 'Quadratic matrix dimension mismatch'}
        
        return {'valid': True, 'error': None}
        
    except Exception as e:
        return {'valid': False, 'error': f'Validation error: {str(e)}'}
'''

    def _get_job_submission_code(self) -> str:
        """Get the Lambda function code for job submission."""
        return '''
import json
import boto3
import os
from datetime import datetime

braket = boto3.client('braket')
s3 = boto3.client('s3')
cloudwatch = boto3.client('cloudwatch')

def lambda_handler(event, context):
    """Submit quantum optimization job to Amazon Braket."""
    try:
        # Extract job parameters
        input_bucket = event.get('input_bucket', os.environ.get('INPUT_BUCKET'))
        output_bucket = event.get('output_bucket', os.environ.get('OUTPUT_BUCKET'))
        code_bucket = event.get('code_bucket', os.environ.get('CODE_BUCKET'))
        data_key = event.get('data_key')
        job_name = f"quantum-opt-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}"
        
        print(f"Submitting quantum job: {job_name}")
        
        # Load problem data to determine optimal device and parameters
        problem_data = load_problem_data(input_bucket, data_key)
        device_config = select_optimal_device(problem_data)
        
        # Configure Braket Hybrid Job
        job_config = {
            'jobName': job_name,
            'algorithmSpecification': {
                'scriptModeConfig': {
                    'entryPoint': 'quantum_optimization.py',
                    's3Uri': f's3://{code_bucket}/quantum-code/'
                }
            },
            'instanceConfig': {
                'instanceType': device_config['instance_type'],
                'instanceCount': 1,
                'volumeSizeInGb': 30
            },
            'outputDataConfig': {
                's3Path': f's3://{output_bucket}/jobs/{job_name}/'
            },
            'roleArn': os.environ.get('EXECUTION_ROLE_ARN'),
            'inputDataConfig': [
                {
                    'channelName': 'input',
                    's3Uri': f's3://{input_bucket}/{data_key}'
                }
            ],
            'hyperParameters': {
                'learning_rate': '0.1',
                'iterations': '100',
                'target_cost': '0.3',
                'num_params': str(problem_data.get('problem_size', 4) * 2)
            },
            'environment': {
                'INPUT_BUCKET': input_bucket,
                'OUTPUT_BUCKET': output_bucket,
                'PROBLEM_TYPE': problem_data.get('problem_type', 'optimization')
            }
        }
        
        # Add device-specific configuration
        if device_config['use_qpu']:
            job_config['deviceConfig'] = {
                'device': device_config['device_arn']
            }
        
        # Submit job to Braket
        response = braket.create_job(**job_config)
        job_arn = response['jobArn']
        
        print(f"Quantum job submitted successfully: {job_arn}")
        
        # Store job metadata
        job_metadata = {
            'job_name': job_name,
            'job_arn': job_arn,
            'created_at': datetime.utcnow().isoformat(),
            'problem_data': problem_data,
            'device_config': device_config,
            'hyperparameters': job_config['hyperParameters']
        }
        
        s3.put_object(
            Bucket=output_bucket,
            Key=f'jobs/{job_name}/metadata.json',
            Body=json.dumps(job_metadata, indent=2)
        )
        
        # Send metrics to CloudWatch
        cloudwatch.put_metric_data(
            Namespace='QuantumPipeline',
            MetricData=[
                {
                    'MetricName': 'JobsSubmitted',
                    'Value': 1,
                    'Unit': 'Count',
                    'Dimensions': [
                        {'Name': 'DeviceType', 'Value': device_config['device_type']},
                        {'Name': 'ProblemType', 'Value': problem_data.get('problem_type', 'unknown')}
                    ]
                }
            ]
        )
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Quantum job submitted successfully',
                'job_name': job_name,
                'job_arn': job_arn,
                'device_type': device_config['device_type']
            })
        }
        
    except Exception as e:
        print(f"Error submitting quantum job: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }

def load_problem_data(bucket, key):
    """Load and parse problem data from S3."""
    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        return json.loads(response['Body'].read().decode('utf-8'))
    except Exception as e:
        print(f"Error loading problem data: {str(e)}")
        return {}

def select_optimal_device(problem_data):
    """Select optimal quantum device based on problem characteristics."""
    problem_size = problem_data.get('problem_size', 4)
    
    # Device selection logic
    if problem_size <= 8:
        # Use local simulator for small problems
        return {
            'device_type': 'simulator',
            'device_arn': 'local:braket/braket.devices.braket_sv_v2/BraketSvV2',
            'instance_type': 'ml.m5.large',
            'use_qpu': False
        }
    elif problem_size <= 16:
        # Use SV1 simulator for medium problems
        return {
            'device_type': 'simulator',
            'device_arn': 'arn:aws:braket:::device/quantum-simulator/amazon/sv1',
            'instance_type': 'ml.m5.xlarge',
            'use_qpu': False
        }
    else:
        # QPU required for large problems (requires approval)
        return {
            'device_type': 'qpu',
            'device_arn': 'arn:aws:braket:us-east-1::device/qpu/rigetti/Aspen-M-3',
            'instance_type': 'ml.m5.2xlarge',
            'use_qpu': True
        }
'''

    def _get_job_monitoring_code(self) -> str:
        """Get the Lambda function code for job monitoring."""
        return '''
import json
import boto3
import os
from datetime import datetime

braket = boto3.client('braket')
s3 = boto3.client('s3')
cloudwatch = boto3.client('cloudwatch')
lambda_client = boto3.client('lambda')

def lambda_handler(event, context):
    """Monitor quantum job status and trigger post-processing."""
    try:
        # Extract job information
        job_arn = event.get('job_arn')
        job_name = event.get('job_name')
        output_bucket = event.get('output_bucket', os.environ.get('OUTPUT_BUCKET'))
        
        if not job_arn:
            # Handle scheduled monitoring - check for active jobs
            return handle_scheduled_monitoring(output_bucket)
        
        print(f"Monitoring quantum job: {job_name}")
        
        # Get job status from Braket
        job_details = braket.get_job(jobArn=job_arn)
        job_status = job_details['status']
        
        print(f"Job status: {job_status}")
        
        # Handle different job states
        if job_status == 'COMPLETED':
            result = handle_completed_job(job_details, output_bucket)
            trigger_post_processing(job_name, output_bucket)
        elif job_status == 'FAILED':
            result = handle_failed_job(job_details, output_bucket)
        elif job_status in ['RUNNING', 'QUEUED']:
            result = handle_running_job(job_details)
        elif job_status == 'CANCELLED':
            result = handle_cancelled_job(job_details, output_bucket)
        else:
            result = {'status': job_status, 'message': f'Unknown job status: {job_status}'}
        
        # Send metrics to CloudWatch
        send_job_metrics(job_details)
        
        return {
            'statusCode': 200,
            'body': json.dumps(result)
        }
        
    except Exception as e:
        print(f"Error monitoring quantum job: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }

def handle_scheduled_monitoring(output_bucket):
    """Handle scheduled monitoring of all active jobs."""
    try:
        # Search for active jobs
        active_jobs = braket.search_jobs(
            filters=[
                {'name': 'status', 'values': ['RUNNING', 'QUEUED']}
            ],
            maxResults=50
        )
        
        monitored_jobs = []
        for job in active_jobs.get('jobs', []):
            job_name = job['jobName']
            job_arn = job['jobArn']
            
            # Monitor this job
            try:
                job_details = braket.get_job(jobArn=job_arn)
                status = job_details['status']
                
                if status == 'COMPLETED':
                    handle_completed_job(job_details, output_bucket)
                    trigger_post_processing(job_name, output_bucket)
                
                monitored_jobs.append({
                    'job_name': job_name,
                    'status': status
                })
                
            except Exception as e:
                print(f"Error monitoring job {job_name}: {str(e)}")
        
        return {
            'message': f'Monitored {len(monitored_jobs)} active jobs',
            'jobs': monitored_jobs
        }
        
    except Exception as e:
        print(f"Error in scheduled monitoring: {str(e)}")
        return {'error': str(e)}

def handle_completed_job(job_details, output_bucket):
    """Handle completed quantum job."""
    job_name = job_details['jobName']
    end_time = job_details.get('endedAt', datetime.utcnow().isoformat())
    
    # Calculate job duration
    start_time = datetime.fromisoformat(job_details['startedAt'].replace('Z', '+00:00'))
    end_time_dt = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
    duration = (end_time_dt - start_time).total_seconds()
    
    # Store job completion metadata
    completion_data = {
        'job_name': job_name,
        'status': 'COMPLETED',
        'duration_seconds': duration,
        'completed_at': end_time,
        'output_location': job_details.get('outputDataConfig', {}).get('s3Path'),
        'billing_duration': job_details.get('billableDuration', 0)
    }
    
    s3.put_object(
        Bucket=output_bucket,
        Key=f'jobs/{job_name}/completion_status.json',
        Body=json.dumps(completion_data, indent=2)
    )
    
    print(f"Job {job_name} completed in {duration:.2f} seconds")
    
    return {
        'status': 'COMPLETED',
        'duration': duration,
        'message': f'Quantum job completed successfully'
    }

def handle_failed_job(job_details, output_bucket):
    """Handle failed quantum job."""
    job_name = job_details['jobName']
    failure_reason = job_details.get('failureReason', 'Unknown failure')
    
    failure_data = {
        'job_name': job_name,
        'status': 'FAILED',
        'failure_reason': failure_reason,
        'failed_at': job_details.get('endedAt', datetime.utcnow().isoformat())
    }
    
    s3.put_object(
        Bucket=output_bucket,
        Key=f'jobs/{job_name}/failure_status.json',
        Body=json.dumps(failure_data, indent=2)
    )
    
    print(f"Job {job_name} failed: {failure_reason}")
    
    return {
        'status': 'FAILED',
        'failure_reason': failure_reason,
        'message': 'Quantum job failed'
    }

def handle_running_job(job_details):
    """Handle running quantum job."""
    job_name = job_details['jobName']
    started_at = job_details.get('startedAt')
    
    if started_at:
        start_time = datetime.fromisoformat(started_at.replace('Z', '+00:00'))
        elapsed = (datetime.utcnow().replace(tzinfo=start_time.tzinfo) - start_time).total_seconds()
        print(f"Job {job_name} running for {elapsed:.2f} seconds")
    
    return {
        'status': 'RUNNING',
        'message': 'Quantum job is still running'
    }

def handle_cancelled_job(job_details, output_bucket):
    """Handle cancelled quantum job."""
    job_name = job_details['jobName']
    
    cancellation_data = {
        'job_name': job_name,
        'status': 'CANCELLED',
        'cancelled_at': job_details.get('endedAt', datetime.utcnow().isoformat())
    }
    
    s3.put_object(
        Bucket=output_bucket,
        Key=f'jobs/{job_name}/cancellation_status.json',
        Body=json.dumps(cancellation_data, indent=2)
    )
    
    return {
        'status': 'CANCELLED',
        'message': 'Quantum job was cancelled'
    }

def trigger_post_processing(job_name, output_bucket):
    """Trigger post-processing Lambda function."""
    try:
        lambda_client.invoke(
            FunctionName=f'{os.environ.get("PROJECT_NAME")}-post-processing',
            InvocationType='Event',
            Payload=json.dumps({
                'job_name': job_name,
                'output_bucket': output_bucket
            })
        )
        print(f"Post-processing triggered for job {job_name}")
    except Exception as e:
        print(f"Error triggering post-processing: {str(e)}")

def send_job_metrics(job_details):
    """Send job metrics to CloudWatch."""
    try:
        status = job_details['status']
        job_name = job_details['jobName']
        
        metrics = [
            {
                'MetricName': 'JobStatusUpdate',
                'Value': 1,
                'Unit': 'Count',
                'Dimensions': [
                    {'Name': 'JobStatus', 'Value': status},
                    {'Name': 'JobName', 'Value': job_name}
                ]
            }
        ]
        
        if status == 'COMPLETED' and 'billableDuration' in job_details:
            metrics.append({
                'MetricName': 'JobDuration',
                'Value': job_details['billableDuration'],
                'Unit': 'Seconds',
                'Dimensions': [
                    {'Name': 'JobName', 'Value': job_name}
                ]
            })
        
        cloudwatch.put_metric_data(
            Namespace='QuantumPipeline',
            MetricData=metrics
        )
        
    except Exception as e:
        print(f"Error sending metrics: {str(e)}")
'''

    def _get_post_processing_code(self) -> str:
        """Get the Lambda function code for post-processing."""
        return '''
import json
import boto3
import numpy as np
import os
from datetime import datetime

s3 = boto3.client('s3')
cloudwatch = boto3.client('cloudwatch')

def lambda_handler(event, context):
    """Post-process quantum optimization results."""
    try:
        job_name = event.get('job_name')
        output_bucket = event.get('output_bucket', os.environ.get('OUTPUT_BUCKET'))
        
        print(f"Post-processing results for job: {job_name}")
        
        # Load quantum results from S3
        results = load_quantum_results(output_bucket, job_name)
        if not results:
            # Generate sample results for demonstration
            results = generate_sample_results()
        
        # Analyze optimization convergence
        analysis = analyze_optimization_results(results)
        
        # Create comprehensive report
        report = create_optimization_report(job_name, results, analysis)
        
        # Store analysis results
        store_analysis_results(output_bucket, job_name, report)
        
        # Send performance metrics
        send_performance_metrics(job_name, analysis)
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Post-processing completed successfully',
                'job_name': job_name,
                'convergence_achieved': analysis['convergence_achieved'],
                'final_cost': analysis['final_cost'],
                'optimization_efficiency': analysis['optimization_efficiency']
            })
        }
        
    except Exception as e:
        print(f"Error in post-processing: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }

def load_quantum_results(bucket, job_name):
    """Load quantum optimization results from S3."""
    try:
        # Try multiple possible result locations
        possible_keys = [
            f'jobs/{job_name}/quantum_results.json',
            f'jobs/{job_name}/output/quantum_results.json',
            f'{job_name}/quantum_results.json'
        ]
        
        for key in possible_keys:
            try:
                response = s3.get_object(Bucket=bucket, Key=key)
                return json.loads(response['Body'].read().decode('utf-8'))
            except s3.exceptions.NoSuchKey:
                continue
        
        return None
        
    except Exception as e:
        print(f"Error loading quantum results: {str(e)}")
        return None

def generate_sample_results():
    """Generate sample quantum optimization results for demonstration."""
    np.random.seed(42)
    iterations = 100
    initial_cost = 2.5
    final_cost = 0.25
    
    # Simulate optimization convergence
    cost_history = []
    for i in range(iterations):
        progress = i / iterations
        noise = np.random.normal(0, 0.1 * (1 - progress))
        cost = initial_cost * np.exp(-3 * progress) + noise
        cost_history.append(max(cost, final_cost))
    
    return {
        'optimized_parameters': np.random.uniform(0, 2*np.pi, 8).tolist(),
        'final_cost': final_cost,
        'cost_history': cost_history,
        'convergence_achieved': True
    }

def analyze_optimization_results(results):
    """Analyze quantum optimization performance and convergence."""
    cost_history = results.get('cost_history', [])
    final_cost = results.get('final_cost', float('inf'))
    convergence_achieved = results.get('convergence_achieved', False)
    
    if not cost_history:
        return {'error': 'No cost history available for analysis'}
    
    # Calculate optimization metrics
    initial_cost = cost_history[0]
    cost_reduction = initial_cost - final_cost
    cost_reduction_percent = (cost_reduction / initial_cost) * 100 if initial_cost > 0 else 0
    
    # Analyze convergence behavior
    convergence_rate = calculate_convergence_rate(cost_history)
    stability_metric = calculate_stability(cost_history[-10:] if len(cost_history) >= 10 else cost_history)
    
    # Calculate optimization efficiency
    iterations = len(cost_history)
    target_cost = 0.5
    convergence_iteration = find_convergence_iteration(cost_history, target_cost)
    efficiency = (iterations - convergence_iteration) / iterations if convergence_iteration < iterations else 0
    
    return {
        'initial_cost': initial_cost,
        'final_cost': final_cost,
        'cost_reduction': cost_reduction,
        'cost_reduction_percent': cost_reduction_percent,
        'convergence_achieved': convergence_achieved,
        'convergence_rate': convergence_rate,
        'stability_metric': stability_metric,
        'optimization_efficiency': efficiency,
        'total_iterations': iterations,
        'convergence_iteration': convergence_iteration
    }

def calculate_convergence_rate(cost_history):
    """Calculate the rate of convergence for optimization."""
    if len(cost_history) < 10:
        return 0
    
    # Calculate average improvement per iteration
    improvements = []
    for i in range(1, min(len(cost_history), 50)):
        if cost_history[i-1] > cost_history[i]:
            improvements.append(cost_history[i-1] - cost_history[i])
    
    return np.mean(improvements) if improvements else 0

def calculate_stability(recent_costs):
    """Calculate stability of recent optimization iterations."""
    if len(recent_costs) < 2:
        return 1.0
    
    variance = np.var(recent_costs)
    mean_cost = np.mean(recent_costs)
    
    # Coefficient of variation as stability metric
    stability = 1 / (1 + variance / (mean_cost ** 2)) if mean_cost > 0 else 0
    return stability

def find_convergence_iteration(cost_history, target_cost):
    """Find the iteration where optimization converged to target cost."""
    for i, cost in enumerate(cost_history):
        if cost <= target_cost:
            return i
    return len(cost_history)

def create_optimization_report(job_name, results, analysis):
    """Create comprehensive optimization report."""
    report = {
        'job_name': job_name,
        'analysis_timestamp': datetime.utcnow().isoformat(),
        'quantum_results': results,
        'performance_analysis': analysis,
        'summary': {
            'optimization_successful': analysis['convergence_achieved'],
            'cost_improvement': f"{analysis['cost_reduction_percent']:.2f}%",
            'final_cost': analysis['final_cost'],
            'efficiency_rating': get_efficiency_rating(analysis['optimization_efficiency'])
        },
        'recommendations': generate_recommendations(analysis)
    }
    
    return report

def get_efficiency_rating(efficiency):
    """Convert efficiency score to rating."""
    if efficiency >= 0.8:
        return 'Excellent'
    elif efficiency >= 0.6:
        return 'Good'
    elif efficiency >= 0.4:
        return 'Fair'
    else:
        return 'Poor'

def generate_recommendations(analysis):
    """Generate optimization recommendations based on analysis."""
    recommendations = []
    
    if analysis['convergence_rate'] < 0.01:
        recommendations.append("Consider increasing learning rate or using adaptive optimization")
    
    if analysis['stability_metric'] < 0.5:
        recommendations.append("Optimization shows instability - consider noise mitigation techniques")
    
    if analysis['optimization_efficiency'] < 0.5:
        recommendations.append("Low efficiency detected - review hyperparameters and algorithm design")
    
    if analysis['convergence_achieved']:
        recommendations.append("Optimization successful - results are ready for production use")
    else:
        recommendations.append("Optimization did not converge - increase iterations or adjust parameters")
    
    return recommendations

def store_analysis_results(bucket, job_name, report):
    """Store comprehensive analysis results in S3."""
    s3.put_object(
        Bucket=bucket,
        Key=f'jobs/{job_name}/analysis_report.json',
        Body=json.dumps(report, indent=2, default=str)
    )
    
    print(f"Analysis results stored for job {job_name}")

def send_performance_metrics(job_name, analysis):
    """Send performance metrics to CloudWatch."""
    try:
        metrics = [
            {
                'MetricName': 'OptimizationEfficiency',
                'Value': analysis['optimization_efficiency'],
                'Unit': 'Percent',
                'Dimensions': [{'Name': 'JobName', 'Value': job_name}]
            },
            {
                'MetricName': 'FinalCost',
                'Value': analysis['final_cost'],
                'Unit': 'None',
                'Dimensions': [{'Name': 'JobName', 'Value': job_name}]
            },
            {
                'MetricName': 'ConvergenceRate',
                'Value': analysis['convergence_rate'],
                'Unit': 'None',
                'Dimensions': [{'Name': 'JobName', 'Value': job_name}]
            }
        ]
        
        cloudwatch.put_metric_data(
            Namespace='QuantumPipeline',
            MetricData=metrics
        )
        
    except Exception as e:
        print(f"Error sending performance metrics: {str(e)}")
'''


class QuantumComputingApp(cdk.App):
    """CDK Application for quantum computing pipeline."""
    
    def __init__(self):
        super().__init__()
        
        # Get configuration from environment variables or use defaults
        project_name = self.node.try_get_context("project_name") or "quantum-pipeline"
        env_name = self.node.try_get_context("environment") or "dev"
        
        # Create the quantum computing pipeline stack
        QuantumComputingPipelineStack(
            self,
            f"QuantumComputingPipelineStack-{env_name}",
            project_name=f"{project_name}-{env_name}",
            env=cdk.Environment(
                account=os.environ.get("CDK_DEFAULT_ACCOUNT"),
                region=os.environ.get("CDK_DEFAULT_REGION", "us-east-1"),
            ),
            description="Hybrid quantum-classical computing pipeline with Amazon Braket and Lambda",
            tags={
                "Project": project_name,
                "Environment": env_name,
                "Purpose": "QuantumComputing",
                "ManagedBy": "CDK",
            },
        )


# Create and synthesize the CDK application
app = QuantumComputingApp()
app.synth()