#!/usr/bin/env python3
"""
CDK Python application for Cost-Optimized Analytics with S3 Tiering.

This application deploys the complete infrastructure for a cost-optimized analytics
solution using S3 Intelligent-Tiering, AWS Glue, Amazon Athena, and CloudWatch monitoring.
The solution automatically optimizes storage costs by moving data between access tiers
based on usage patterns while maintaining full analytical capabilities.

Author: Generated by AWS Recipe CDK Generator
Version: 1.0
"""

import os
from typing import Optional
import aws_cdk as cdk
from aws_cdk import (
    Stack,
    App,
    Environment,
    aws_s3 as s3,
    aws_glue as glue,
    aws_athena as athena,
    aws_iam as iam,
    aws_cloudwatch as cloudwatch,
    aws_ce as ce,
    aws_ssm as ssm,
    CfnOutput,
    Duration,
    RemovalPolicy,
    Tags
)
from constructs import Construct


class CostOptimizedAnalyticsStack(Stack):
    """
    AWS CDK Stack for Cost-Optimized Analytics with S3 Tiering.
    
    This stack creates:
    - S3 bucket with Intelligent-Tiering configuration
    - AWS Glue database and table for analytics
    - Amazon Athena workgroup with cost controls
    - CloudWatch dashboard for monitoring
    - Cost anomaly detection
    - IAM roles and policies
    """

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # Generate unique suffix for resource names
        self.unique_suffix = self.node.try_get_context("unique_suffix") or "demo"
        
        # Resource naming with unique suffix
        self.bucket_name = f"cost-optimized-analytics-{self.unique_suffix}"
        self.database_name = f"analytics_database_{self.unique_suffix}"
        self.workgroup_name = f"cost-optimized-workgroup-{self.unique_suffix}"
        
        # Create all infrastructure components
        self.analytics_bucket = self._create_s3_bucket()
        self.glue_database = self._create_glue_database()
        self.glue_table = self._create_glue_table()
        self.athena_workgroup = self._create_athena_workgroup()
        self.cloudwatch_dashboard = self._create_cloudwatch_dashboard()
        self.cost_anomaly_detector = self._create_cost_anomaly_detector()
        
        # Create outputs for reference
        self._create_outputs()
        
        # Apply tags to all resources
        self._apply_tags()

    def _create_s3_bucket(self) -> s3.Bucket:
        """
        Create S3 bucket with Intelligent-Tiering configuration.
        
        Features:
        - Server-side encryption with AES256
        - Versioning enabled for data protection
        - Intelligent-Tiering configuration with archive tiers
        - Lifecycle policy for automatic transitions
        - Public access blocked for security
        
        Returns:
            s3.Bucket: The created S3 bucket
        """
        # Create the main analytics bucket
        bucket = s3.Bucket(
            self, "AnalyticsBucket",
            bucket_name=self.bucket_name,
            versioned=True,
            encryption=s3.BucketEncryption.S3_MANAGED,
            block_public_access=s3.BlockPublicAccess.BLOCK_ALL,
            removal_policy=RemovalPolicy.DESTROY,
            auto_delete_objects=True,  # Enable for demo purposes
            enforce_ssl=True,
            intelligent_tiering_configurations=[
                s3.IntelligentTieringConfiguration(
                    id="cost-optimization-config",
                    status=s3.IntelligentTieringStatus.ENABLED,
                    prefix="analytics-data/",
                    archive_access_tier_time=Duration.days(90),
                    deep_archive_access_tier_time=Duration.days(180),
                    optional_fields=[
                        s3.IntelligentTieringConfiguration.OptionalFields.BUCKET_KEY_STATUS,
                        s3.IntelligentTieringConfiguration.OptionalFields.ENCRYPTION_STATUS
                    ]
                )
            ],
            lifecycle_rules=[
                s3.LifecycleRule(
                    id="intelligent-tiering-transition",
                    enabled=True,
                    prefix="analytics-data/",
                    transitions=[
                        s3.Transition(
                            storage_class=s3.StorageClass.INTELLIGENT_TIERING,
                            transition_after=Duration.days(0)
                        )
                    ]
                )
            ]
        )
        
        # Store bucket name in SSM Parameter for easy reference
        ssm.StringParameter(
            self, "BucketNameParameter",
            parameter_name=f"/cost-optimized-analytics/{self.unique_suffix}/bucket-name",
            string_value=bucket.bucket_name,
            description="S3 bucket name for cost-optimized analytics",
            tier=ssm.ParameterTier.STANDARD
        )
        
        return bucket

    def _create_glue_database(self) -> glue.CfnDatabase:
        """
        Create AWS Glue database for analytics metadata.
        
        The Glue database serves as a central metadata repository that enables
        seamless integration between S3 storage and analytics services like Athena.
        
        Returns:
            glue.CfnDatabase: The created Glue database
        """
        database = glue.CfnDatabase(
            self, "AnalyticsDatabase",
            catalog_id=self.account,
            database_input=glue.CfnDatabase.DatabaseInputProperty(
                name=self.database_name,
                description="Cost-optimized analytics database for transaction logs"
            )
        )
        
        # Store database name in SSM Parameter
        ssm.StringParameter(
            self, "DatabaseNameParameter",
            parameter_name=f"/cost-optimized-analytics/{self.unique_suffix}/database-name",
            string_value=database.database_input.name,
            description="Glue database name for analytics",
            tier=ssm.ParameterTier.STANDARD
        )
        
        return database

    def _create_glue_table(self) -> glue.CfnTable:
        """
        Create AWS Glue table for transaction logs analytics.
        
        The table schema defines the structure for CSV transaction log files
        containing timestamp, user_id, transaction_id, and amount columns.
        
        Returns:
            glue.CfnTable: The created Glue table
        """
        table = glue.CfnTable(
            self, "TransactionLogsTable",
            catalog_id=self.account,
            database_name=self.database_name,
            table_input=glue.CfnTable.TableInputProperty(
                name="transaction_logs",
                description="Transaction logs table for cost-optimized analytics",
                table_type="EXTERNAL_TABLE",
                storage_descriptor=glue.CfnTable.StorageDescriptorProperty(
                    columns=[
                        glue.CfnTable.ColumnProperty(name="timestamp", type="string"),
                        glue.CfnTable.ColumnProperty(name="user_id", type="string"),
                        glue.CfnTable.ColumnProperty(name="transaction_id", type="string"),
                        glue.CfnTable.ColumnProperty(name="amount", type="string")
                    ],
                    location=f"s3://{self.analytics_bucket.bucket_name}/analytics-data/",
                    input_format="org.apache.hadoop.mapred.TextInputFormat",
                    output_format="org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat",
                    serde_info=glue.CfnTable.SerdeInfoProperty(
                        serialization_library="org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
                        parameters={
                            "field.delim": ","
                        }
                    )
                )
            )
        )
        
        # Table depends on database creation
        table.add_dependency(self.glue_database)
        
        return table

    def _create_athena_workgroup(self) -> athena.CfnWorkGroup:
        """
        Create Amazon Athena workgroup with cost controls.
        
        Workgroups provide cost control and governance for Athena queries.
        The configuration includes query result caching, CloudWatch metrics,
        and data scan limits to prevent runaway costs.
        
        Returns:
            athena.CfnWorkGroup: The created Athena workgroup
        """
        workgroup = athena.CfnWorkGroup(
            self, "CostOptimizedWorkgroup",
            name=self.workgroup_name,
            description="Cost-optimized workgroup for analytics with data scan limits",
            state="ENABLED",
            work_group_configuration=athena.CfnWorkGroup.WorkGroupConfigurationProperty(
                result_configuration=athena.CfnWorkGroup.ResultConfigurationProperty(
                    output_location=f"s3://{self.analytics_bucket.bucket_name}/athena-results/",
                    encryption_configuration=athena.CfnWorkGroup.EncryptionConfigurationProperty(
                        encryption_option="SSE_S3"
                    )
                ),
                enforce_work_group_configuration=True,
                publish_cloud_watch_metrics=True,
                bytes_scanned_cutoff_per_query=1073741824,  # 1GB limit
                requester_pays_enabled=False,
                engine_version=athena.CfnWorkGroup.EngineVersionProperty(
                    selected_engine_version="Athena engine version 3"
                )
            )
        )
        
        # Store workgroup name in SSM Parameter
        ssm.StringParameter(
            self, "WorkgroupNameParameter",
            parameter_name=f"/cost-optimized-analytics/{self.unique_suffix}/workgroup-name",
            string_value=workgroup.name,
            description="Athena workgroup name for cost-optimized analytics",
            tier=ssm.ParameterTier.STANDARD
        )
        
        return workgroup

    def _create_cloudwatch_dashboard(self) -> cloudwatch.Dashboard:
        """
        Create CloudWatch dashboard for S3 Intelligent-Tiering monitoring.
        
        The dashboard provides visibility into storage distribution across tiers,
        enabling data-driven decisions about archiving policies and cost optimization.
        
        Returns:
            cloudwatch.Dashboard: The created CloudWatch dashboard
        """
        dashboard = cloudwatch.Dashboard(
            self, "S3CostOptimizationDashboard",
            dashboard_name=f"S3-Cost-Optimization-{self.unique_suffix}",
            default_interval=Duration.minutes(5)
        )
        
        # S3 Intelligent-Tiering storage distribution widget
        storage_widget = cloudwatch.GraphWidget(
            title="S3 Intelligent-Tiering Storage Distribution",
            width=24,
            height=6,
            left=[
                cloudwatch.Metric(
                    namespace="AWS/S3",
                    metric_name="BucketSizeBytes",
                    dimensions_map={
                        "BucketName": self.analytics_bucket.bucket_name,
                        "StorageType": "IntelligentTieringFAStorage"
                    },
                    label="Frequent Access Tier",
                    statistic="Average"
                ),
                cloudwatch.Metric(
                    namespace="AWS/S3",
                    metric_name="BucketSizeBytes",
                    dimensions_map={
                        "BucketName": self.analytics_bucket.bucket_name,
                        "StorageType": "IntelligentTieringIAStorage"
                    },
                    label="Infrequent Access Tier",
                    statistic="Average"
                ),
                cloudwatch.Metric(
                    namespace="AWS/S3",
                    metric_name="BucketSizeBytes",
                    dimensions_map={
                        "BucketName": self.analytics_bucket.bucket_name,
                        "StorageType": "IntelligentTieringAAStorage"
                    },
                    label="Archive Access Tier",
                    statistic="Average"
                ),
                cloudwatch.Metric(
                    namespace="AWS/S3",
                    metric_name="BucketSizeBytes",
                    dimensions_map={
                        "BucketName": self.analytics_bucket.bucket_name,
                        "StorageType": "IntelligentTieringAIAStorage"
                    },
                    label="Archive Instant Access",
                    statistic="Average"
                ),
                cloudwatch.Metric(
                    namespace="AWS/S3",
                    metric_name="BucketSizeBytes",
                    dimensions_map={
                        "BucketName": self.analytics_bucket.bucket_name,
                        "StorageType": "IntelligentTieringDAAStorage"
                    },
                    label="Deep Archive Access",
                    statistic="Average"
                )
            ]
        )
        
        # Athena query metrics widget
        athena_widget = cloudwatch.GraphWidget(
            title="Athena Query Performance Metrics",
            width=12,
            height=6,
            left=[
                cloudwatch.Metric(
                    namespace="AWS/Athena",
                    metric_name="DataScannedInBytes",
                    label="Data Scanned (Bytes)",
                    statistic="Sum"
                ),
                cloudwatch.Metric(
                    namespace="AWS/Athena",
                    metric_name="QueryExecutionTime",
                    label="Query Execution Time (ms)",
                    statistic="Average"
                )
            ]
        )
        
        # S3 request metrics widget
        requests_widget = cloudwatch.GraphWidget(
            title="S3 Request Metrics",
            width=12,
            height=6,
            left=[
                cloudwatch.Metric(
                    namespace="AWS/S3",
                    metric_name="AllRequests",
                    dimensions_map={
                        "BucketName": self.analytics_bucket.bucket_name
                    },
                    label="All Requests",
                    statistic="Sum"
                ),
                cloudwatch.Metric(
                    namespace="AWS/S3",
                    metric_name="GetRequests",
                    dimensions_map={
                        "BucketName": self.analytics_bucket.bucket_name
                    },
                    label="GET Requests",
                    statistic="Sum"
                )
            ]
        )
        
        # Add widgets to dashboard
        dashboard.add_widgets(storage_widget)
        dashboard.add_widgets(athena_widget, requests_widget)
        
        return dashboard

    def _create_cost_anomaly_detector(self) -> ce.CfnAnomalyDetector:
        """
        Create AWS Cost Explorer anomaly detector for S3 services.
        
        The anomaly detector monitors S3 spending patterns and alerts when
        unusual cost spikes occur, helping maintain cost optimization goals.
        
        Returns:
            ce.CfnAnomalyDetector: The created cost anomaly detector
        """
        detector = ce.CfnAnomalyDetector(
            self, "S3CostAnomalyDetector",
            detector_name=f"S3-Cost-Anomaly-{self.unique_suffix}",
            monitor_type="DIMENSIONAL",
            dimension_specification=ce.CfnAnomalyDetector.DimensionSpecificationProperty(
                dimension="SERVICE",
                values=["Amazon Simple Storage Service"]
            )
        )
        
        return detector

    def _create_outputs(self) -> None:
        """Create CloudFormation outputs for key resources."""
        CfnOutput(
            self, "BucketName",
            value=self.analytics_bucket.bucket_name,
            description="S3 bucket name for cost-optimized analytics",
            export_name=f"CostOptimizedAnalytics-{self.unique_suffix}-BucketName"
        )
        
        CfnOutput(
            self, "GlueDatabaseName",
            value=self.database_name,
            description="AWS Glue database name for analytics",
            export_name=f"CostOptimizedAnalytics-{self.unique_suffix}-DatabaseName"
        )
        
        CfnOutput(
            self, "AthenaWorkgroupName",
            value=self.workgroup_name,
            description="Amazon Athena workgroup name for cost-optimized queries",
            export_name=f"CostOptimizedAnalytics-{self.unique_suffix}-WorkgroupName"
        )
        
        CfnOutput(
            self, "CloudWatchDashboardUrl",
            value=f"https://console.aws.amazon.com/cloudwatch/home?region={self.region}#dashboards:name=S3-Cost-Optimization-{self.unique_suffix}",
            description="CloudWatch dashboard URL for monitoring S3 cost optimization",
            export_name=f"CostOptimizedAnalytics-{self.unique_suffix}-DashboardUrl"
        )
        
        CfnOutput(
            self, "AthenaQueryLocation",
            value=f"s3://{self.analytics_bucket.bucket_name}/athena-results/",
            description="S3 location for Athena query results",
            export_name=f"CostOptimizedAnalytics-{self.unique_suffix}-QueryLocation"
        )

    def _apply_tags(self) -> None:
        """Apply consistent tags to all resources in the stack."""
        Tags.of(self).add("Project", "CostOptimizedAnalytics")
        Tags.of(self).add("Environment", "Demo")
        Tags.of(self).add("Owner", "CDK-Generated")
        Tags.of(self).add("CostCenter", "Analytics")
        Tags.of(self).add("Recipe", "cost-optimized-analytics-s3-intelligent-tiering")


def main() -> None:
    """Main application entry point."""
    app = App()
    
    # Get environment configuration
    account = os.environ.get("CDK_DEFAULT_ACCOUNT")
    region = os.environ.get("CDK_DEFAULT_REGION", "us-east-1")
    
    # Get unique suffix from context or generate one
    unique_suffix = app.node.try_get_context("unique_suffix")
    if not unique_suffix:
        import random
        import string
        unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
        print(f"Generated unique suffix: {unique_suffix}")
    
    # Create the stack
    CostOptimizedAnalyticsStack(
        app, 
        "CostOptimizedAnalyticsStack",
        env=Environment(account=account, region=region),
        description="Cost-Optimized Analytics with S3 Tiering - CDK Python Implementation",
        unique_suffix=unique_suffix
    )
    
    # Synthesize the CloudFormation template
    app.synth()


if __name__ == "__main__":
    main()