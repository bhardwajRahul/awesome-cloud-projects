AWSTemplateFormatVersion: '2010-09-09'
Description: >
  Real-Time Stream Enrichment Pipeline with Kinesis Data Firehose and EventBridge Pipes.
  This template creates a serverless data enrichment pipeline that ingests streaming data,
  enriches it with reference data from DynamoDB using Lambda, and delivers to S3 via Firehose.

# ================================
# Parameters Section
# ================================
Parameters:
  ProjectName:
    Type: String
    Default: stream-enrichment
    Description: Base name for all resources (will be combined with random suffix)
    AllowedPattern: ^[a-z][a-z0-9-]*$
    ConstraintDescription: Must start with lowercase letter and contain only lowercase letters, numbers, and hyphens
    MinLength: 3
    MaxLength: 20

  Environment:
    Type: String
    Default: dev
    AllowedValues: [dev, test, prod]
    Description: Environment for deployment (affects resource sizing and retention)

  DataRetentionDays:
    Type: Number
    Default: 7
    MinValue: 1
    MaxValue: 365
    Description: Number of days to retain data in S3 before transitioning to cheaper storage

  FirehoseBufferSizeMB:
    Type: Number
    Default: 5
    MinValue: 1
    MaxValue: 128
    Description: Buffer size in MB for Firehose delivery to S3

  FirehoseBufferIntervalSeconds:
    Type: Number
    Default: 300
    MinValue: 60
    MaxValue: 900
    Description: Buffer interval in seconds for Firehose delivery to S3

  LambdaTimeout:
    Type: Number
    Default: 60
    MinValue: 30
    MaxValue: 900
    Description: Lambda function timeout in seconds

  LambdaMemorySize:
    Type: Number
    Default: 256
    AllowedValues: [128, 256, 512, 1024, 2048, 3008]
    Description: Lambda function memory size in MB

  EnableCloudWatchLogs:
    Type: String
    Default: 'true'
    AllowedValues: ['true', 'false']
    Description: Enable CloudWatch Logs for Lambda function

# ================================
# Conditions Section
# ================================
Conditions:
  CreateCloudWatchLogs: !Equals [!Ref EnableCloudWatchLogs, 'true']
  IsProduction: !Equals [!Ref Environment, 'prod']

# ================================
# Mappings Section
# ================================
Mappings:
  EnvironmentConfig:
    dev:
      KinesisRetentionHours: 24
      DynamoDBBillingMode: PAY_PER_REQUEST
    test:
      KinesisRetentionHours: 24
      DynamoDBBillingMode: PAY_PER_REQUEST
    prod:
      KinesisRetentionHours: 168
      DynamoDBBillingMode: PAY_PER_REQUEST

# ================================
# Resources Section
# ================================
Resources:
  # ================================
  # Random Suffix Generation
  # ================================
  RandomSuffixGenerator:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ServiceToken: !GetAtt RandomSuffixLambda.Arn

  RandomSuffixLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-random-suffix-generator'
      Runtime: python3.11
      Handler: index.handler
      Role: !GetAtt RandomSuffixLambdaRole.Arn
      Timeout: 30
      Code:
        ZipFile: |
          import random
          import string
          import cfnresponse
          
          def handler(event, context):
              try:
                  if event['RequestType'] == 'Create':
                      # Generate 6-character random suffix
                      suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {'RandomSuffix': suffix})
                  else:
                      # For Update/Delete, return the same suffix from the old resource
                      old_suffix = event.get('PhysicalResourceId', 'abc123')
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {'RandomSuffix': old_suffix}, old_suffix)
              except Exception as e:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(e)})

  RandomSuffixLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

  # ================================
  # S3 Bucket for Enriched Data
  # ================================
  EnrichedDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 
        - '${ProjectName}-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
            BucketKeyEnabled: true
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      VersioningConfiguration:
        Status: !If [IsProduction, Enabled, Suspended]
      LifecycleConfiguration:
        Rules:
          - Id: DataRetentionRule
            Status: Enabled
            Transitions:
              - TransitionInDays: !Ref DataRetentionDays
                StorageClass: STANDARD_IA
              - TransitionInDays: !If [IsProduction, 90, 30]
                StorageClass: GLACIER
          - Id: DeleteIncompleteMultipartUploads
            Status: Enabled
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      NotificationConfiguration:
        CloudWatchConfigurations:
          - Event: s3:ObjectCreated:*
            CloudWatchConfiguration:
              LogGroupName: !If 
                - CreateCloudWatchLogs
                - !Ref S3LogGroup
                - !Ref AWS::NoValue
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName
        - Key: Purpose
          Value: EnrichedDataStorage

  S3LogGroup:
    Type: AWS::Logs::LogGroup
    Condition: CreateCloudWatchLogs
    Properties:
      LogGroupName: !Sub '/aws/s3/${ProjectName}-enriched-data'
      RetentionInDays: !If [IsProduction, 90, 7]

  # ================================
  # DynamoDB Table for Reference Data
  # ================================
  ReferenceDataTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub 
        - '${ProjectName}-reference-data-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      BillingMode: !FindInMap [EnvironmentConfig, !Ref Environment, DynamoDBBillingMode]
      AttributeDefinitions:
        - AttributeName: productId
          AttributeType: S
      KeySchema:
        - AttributeName: productId
          KeyType: HASH
      PointInTimeRecoverySpecification:
        PointInTimeRecoveryEnabled: !If [IsProduction, true, false]
      SSESpecification:
        SSEEnabled: true
      StreamSpecification:
        StreamViewType: NEW_AND_OLD_IMAGES
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName
        - Key: Purpose
          Value: ReferenceData

  # Add sample data to DynamoDB table
  SampleDataLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-populate-reference-data'
      Runtime: python3.11
      Handler: index.handler
      Role: !GetAtt SampleDataLambdaRole.Arn
      Timeout: 60
      Code:
        ZipFile: !Sub |
          import boto3
          import cfnresponse
          import json
          
          def handler(event, context):
              try:
                  dynamodb = boto3.resource('dynamodb')
                  table = dynamodb.Table('${ReferenceDataTable}')
                  
                  if event['RequestType'] == 'Create':
                      # Add sample reference data
                      sample_data = [
                          {
                              'productId': 'PROD-001',
                              'productName': 'Smart Sensor',
                              'category': 'IoT Devices',
                              'price': '49.99'
                          },
                          {
                              'productId': 'PROD-002', 
                              'productName': 'Temperature Monitor',
                              'category': 'IoT Devices',
                              'price': '79.99'
                          },
                          {
                              'productId': 'PROD-003',
                              'productName': 'Humidity Sensor',
                              'category': 'IoT Devices',
                              'price': '39.99'
                          },
                          {
                              'productId': 'PROD-004',
                              'productName': 'Motion Detector',
                              'category': 'Security Devices',
                              'price': '89.99'
                          }
                      ]
                      
                      for item in sample_data:
                          table.put_item(Item=item)
                      
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {'Message': 'Sample data added successfully'})
                  else:
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {'Message': 'No action required'})
              except Exception as e:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(e)})

  PopulateReferenceData:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ServiceToken: !GetAtt SampleDataLambda.Arn
    DependsOn: ReferenceDataTable

  SampleDataLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: DynamoDBAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:GetItem
                Resource: !GetAtt ReferenceDataTable.Arn

  # ================================
  # Kinesis Data Stream for Raw Events
  # ================================
  RawEventsStream:
    Type: AWS::Kinesis::Stream
    Properties:
      Name: !Sub 
        - '${ProjectName}-raw-events-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      StreamModeDetails:
        StreamMode: ON_DEMAND
      RetentionPeriodHours: !FindInMap [EnvironmentConfig, !Ref Environment, KinesisRetentionHours]
      StreamEncryption:
        EncryptionType: KMS
        KeyId: alias/aws/kinesis
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName
        - Key: Purpose
          Value: RawEventStream

  # ================================
  # Lambda Function for Event Enrichment
  # ================================
  EnrichmentLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 
        - '${ProjectName}-lambda-enrichment-role-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: DynamoDBReadPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:GetItem
                  - dynamodb:Query
                  - dynamodb:BatchGetItem
                Resource: !GetAtt ReferenceDataTable.Arn
        - PolicyName: KinesisReadPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - kinesis:DescribeStream
                  - kinesis:GetRecords
                  - kinesis:GetShardIterator
                  - kinesis:ListStreams
                  - kinesis:SubscribeToShard
                Resource: !GetAtt RawEventsStream.Arn
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  EnrichmentLambdaLogGroup:
    Type: AWS::Logs::LogGroup
    Condition: CreateCloudWatchLogs
    Properties:
      LogGroupName: !Sub 
        - '/aws/lambda/${ProjectName}-enrich-events-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      RetentionInDays: !If [IsProduction, 90, 7]

  EnrichmentLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 
        - '${ProjectName}-enrich-events-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      Runtime: python3.11
      Handler: index.lambda_handler
      Role: !GetAtt EnrichmentLambdaRole.Arn
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemorySize
      Environment:
        Variables:
          TABLE_NAME: !Ref ReferenceDataTable
          LOG_LEVEL: !If [IsProduction, 'INFO', 'DEBUG']
      ReservedConcurrencyLimit: !If [IsProduction, 100, 10]
      Code:
        ZipFile: !Sub |
          import json
          import base64
          import boto3
          import os
          import logging
          from datetime import datetime
          from typing import Dict, List, Any
          
          # Configure logging
          log_level = os.environ.get('LOG_LEVEL', 'INFO')
          logging.basicConfig(level=getattr(logging, log_level))
          logger = logging.getLogger(__name__)
          
          # Initialize DynamoDB resource
          dynamodb = boto3.resource('dynamodb')
          table = dynamodb.Table(os.environ['TABLE_NAME'])
          
          def lambda_handler(event: List[Dict], context) -> List[Dict]:
              """
              Enriches streaming events with reference data from DynamoDB.
              
              Args:
                  event: List of records from EventBridge Pipes
                  context: Lambda context object
                  
              Returns:
                  List of enriched records
              """
              logger.info(f"Processing {len(event)} records")
              enriched_records = []
              
              for record in event:
                  try:
                      # Decode Kinesis data
                      if 'data' in record:
                          # Handle base64 encoded data from Kinesis
                          payload = json.loads(
                              base64.b64decode(record['data']).decode('utf-8')
                          )
                      else:
                          # Handle direct JSON payload
                          payload = record
                      
                      logger.debug(f"Processing record: {payload}")
                      
                      # Extract product ID for enrichment
                      product_id = payload.get('productId')
                      if product_id:
                          try:
                              # Lookup product details from DynamoDB
                              response = table.get_item(
                                  Key={'productId': product_id},
                                  ConsistentRead=False  # Use eventually consistent reads for better performance
                              )
                              
                              if 'Item' in response:
                                  # Enrich the payload with product information
                                  item = response['Item']
                                  payload['productName'] = item['productName']
                                  payload['category'] = item['category']
                                  payload['price'] = float(item['price'])
                                  payload['enrichmentTimestamp'] = datetime.utcnow().isoformat()
                                  payload['enrichmentStatus'] = 'success'
                                  logger.debug(f"Successfully enriched record for product {product_id}")
                              else:
                                  payload['enrichmentStatus'] = 'product_not_found'
                                  payload['enrichmentTimestamp'] = datetime.utcnow().isoformat()
                                  logger.warning(f"Product not found: {product_id}")
                          except Exception as e:
                              payload['enrichmentStatus'] = 'enrichment_error'
                              payload['enrichmentError'] = str(e)
                              payload['enrichmentTimestamp'] = datetime.utcnow().isoformat()
                              logger.error(f"Error enriching record for product {product_id}: {str(e)}")
                      else:
                          payload['enrichmentStatus'] = 'no_product_id'
                          payload['enrichmentTimestamp'] = datetime.utcnow().isoformat()
                          logger.warning("No productId found in record")
                      
                      # Add processing metadata
                      payload['processingTimestamp'] = datetime.utcnow().isoformat()
                      payload['processingRegion'] = os.environ.get('AWS_REGION', 'unknown')
                      
                      enriched_records.append(payload)
                      
                  except Exception as e:
                      logger.error(f"Failed to process record: {str(e)}")
                      # Add error record for monitoring
                      error_record = {
                          'originalRecord': record,
                          'enrichmentStatus': 'processing_error',
                          'enrichmentError': str(e),
                          'enrichmentTimestamp': datetime.utcnow().isoformat()
                      }
                      enriched_records.append(error_record)
              
              logger.info(f"Successfully processed {len(enriched_records)} records")
              return enriched_records
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName
        - Key: Purpose
          Value: EventEnrichment

  # ================================
  # Kinesis Data Firehose for S3 Delivery
  # ================================
  FirehoseDeliveryRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 
        - '${ProjectName}-firehose-delivery-role-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: FirehoseDeliveryPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:AbortMultipartUpload
                  - s3:GetBucketLocation
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:ListBucketMultipartUploads
                  - s3:PutObject
                  - s3:PutObjectAcl
                Resource:
                  - !GetAtt EnrichedDataBucket.Arn
                  - !Sub '${EnrichedDataBucket.Arn}/*'
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/kinesisfirehose/*'
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  FirehoseLogGroup:
    Type: AWS::Logs::LogGroup
    Condition: CreateCloudWatchLogs
    Properties:
      LogGroupName: !Sub 
        - '/aws/kinesisfirehose/${ProjectName}-event-ingestion-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      RetentionInDays: !If [IsProduction, 90, 7]

  FirehoseLogStream:
    Type: AWS::Logs::LogStream
    Condition: CreateCloudWatchLogs
    Properties:
      LogGroupName: !Ref FirehoseLogGroup
      LogStreamName: S3Delivery

  EventIngestionFirehose:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      DeliveryStreamName: !Sub 
        - '${ProjectName}-event-ingestion-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      DeliveryStreamType: DirectPut
      ExtendedS3DestinationConfiguration:
        BucketARN: !GetAtt EnrichedDataBucket.Arn
        RoleARN: !GetAtt FirehoseDeliveryRole.Arn
        Prefix: 'enriched-data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/'
        ErrorOutputPrefix: 'error-data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/'
        CompressionFormat: GZIP
        BufferingHints:
          SizeInMBs: !Ref FirehoseBufferSizeMB
          IntervalInSeconds: !Ref FirehoseBufferIntervalSeconds
        CloudWatchLoggingOptions:
          Enabled: !If [CreateCloudWatchLogs, true, false]
          LogGroupName: !If 
            - CreateCloudWatchLogs
            - !Ref FirehoseLogGroup
            - !Ref AWS::NoValue
          LogStreamName: !If 
            - CreateCloudWatchLogs
            - !Ref FirehoseLogStream
            - !Ref AWS::NoValue
        ProcessingConfiguration:
          Enabled: false
        S3BackupMode: Disabled
        DataFormatConversionConfiguration:
          Enabled: false
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName
        - Key: Purpose
          Value: DataDelivery

  # ================================
  # EventBridge Pipes IAM Role
  # ================================
  PipesExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 
        - '${ProjectName}-pipes-execution-role-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: pipes.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: PipesExecutionPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              # Kinesis Data Stream permissions
              - Effect: Allow
                Action:
                  - kinesis:DescribeStream
                  - kinesis:GetRecords
                  - kinesis:GetShardIterator
                  - kinesis:ListStreams
                  - kinesis:SubscribeToShard
                  - kinesis:DescribeStreamSummary
                  - kinesis:ListShards
                Resource: !GetAtt RawEventsStream.Arn
              # Lambda enrichment permissions
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: !GetAtt EnrichmentLambdaFunction.Arn
              # Firehose target permissions
              - Effect: Allow
                Action:
                  - firehose:PutRecord
                  - firehose:PutRecordBatch
                Resource: !GetAtt EventIngestionFirehose.Arn
              # CloudWatch Logs permissions
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/vendedlogs/pipes/*'
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # ================================
  # EventBridge Pipe
  # Note: EventBridge Pipes might not be available in CloudFormation yet
  # This is a placeholder showing the intended configuration
  # You may need to create this manually or use CDK/Terraform
  # ================================
  
  # Custom resource to create EventBridge Pipe via API
  EventBridgePipeCustomResource:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ServiceToken: !GetAtt PipeCreatorLambda.Arn
      PipeName: !Sub 
        - '${ProjectName}-enrichment-pipe-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      SourceArn: !GetAtt RawEventsStream.Arn
      EnrichmentArn: !GetAtt EnrichmentLambdaFunction.Arn
      TargetArn: !GetAtt EventIngestionFirehose.Arn
      RoleArn: !GetAtt PipesExecutionRole.Arn

  PipeCreatorLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-pipe-creator'
      Runtime: python3.11
      Handler: index.handler
      Role: !GetAtt PipeCreatorLambdaRole.Arn
      Timeout: 300
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import json
          import logging
          
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          
          def handler(event, context):
              try:
                  pipes_client = boto3.client('pipes')
                  props = event['ResourceProperties']
                  pipe_name = props['PipeName']
                  
                  if event['RequestType'] == 'Create':
                      logger.info(f"Creating EventBridge Pipe: {pipe_name}")
                      
                      # Create the pipe
                      response = pipes_client.create_pipe(
                          Name=pipe_name,
                          RoleArn=props['RoleArn'],
                          Source=props['SourceArn'],
                          SourceParameters={
                              'KinesisStreamParameters': {
                                  'StartingPosition': 'LATEST',
                                  'BatchSize': 10,
                                  'MaximumBatchingWindowInSeconds': 5
                              }
                          },
                          Enrichment=props['EnrichmentArn'],
                          Target=props['TargetArn'],
                          TargetParameters={
                              'KinesisStreamParameters': {
                                  'PartitionKey': '$.productId'
                              }
                          },
                          Tags={
                              'Environment': '${Environment}',
                              'Project': '${ProjectName}',
                              'Purpose': 'StreamEnrichment'
                          }
                      )
                      
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, 
                                     {'PipeArn': response['Arn']}, pipe_name)
                      
                  elif event['RequestType'] == 'Delete':
                      logger.info(f"Deleting EventBridge Pipe: {pipe_name}")
                      
                      try:
                          pipes_client.delete_pipe(Name=pipe_name)
                      except pipes_client.exceptions.NotFoundException:
                          logger.info(f"Pipe {pipe_name} not found, skipping deletion")
                      
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      
                  else:  # Update
                      logger.info(f"Update not implemented for pipe: {pipe_name}")
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, 
                                     {'PipeArn': f'arn:aws:pipes:${AWS::Region}:${AWS::AccountId}:pipe/{pipe_name}'})
                      
              except Exception as e:
                  logger.error(f"Error: {str(e)}")
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(e)})

  PipeCreatorLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: PipeManagementPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - pipes:CreatePipe
                  - pipes:DeletePipe
                  - pipes:DescribePipe
                  - pipes:UpdatePipe
                  - pipes:TagResource
                Resource: '*'
              - Effect: Allow
                Action:
                  - iam:PassRole
                Resource: !GetAtt PipesExecutionRole.Arn

  # ================================
  # CloudWatch Alarms for Monitoring
  # ================================
  LambdaErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: CreateCloudWatchLogs
    Properties:
      AlarmName: !Sub 
        - '${ProjectName}-lambda-errors-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      AlarmDescription: Monitor Lambda function errors
      MetricName: Errors
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 2
      Threshold: 5
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref EnrichmentLambdaFunction
      TreatMissingData: notBreaching

  FirehoseDeliveryFailureAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: CreateCloudWatchLogs
    Properties:
      AlarmName: !Sub 
        - '${ProjectName}-firehose-delivery-failures-${RandomSuffix}'
        - RandomSuffix: !GetAtt RandomSuffixGenerator.RandomSuffix
      AlarmDescription: Monitor Firehose delivery failures
      MetricName: DeliveryToS3.Records
      Namespace: AWS/KinesisFirehose
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 2
      Threshold: 1
      ComparisonOperator: LessThanThreshold
      Dimensions:
        - Name: DeliveryStreamName
          Value: !Ref EventIngestionFirehose
      TreatMissingData: breaching

# ================================
# Outputs Section
# ================================
Outputs:
  S3BucketName:
    Description: Name of the S3 bucket for enriched data
    Value: !Ref EnrichedDataBucket
    Export:
      Name: !Sub '${AWS::StackName}-S3Bucket'

  S3BucketArn:
    Description: ARN of the S3 bucket for enriched data
    Value: !GetAtt EnrichedDataBucket.Arn
    Export:
      Name: !Sub '${AWS::StackName}-S3BucketArn'

  DynamoDBTableName:
    Description: Name of the DynamoDB reference data table
    Value: !Ref ReferenceDataTable
    Export:
      Name: !Sub '${AWS::StackName}-DynamoDBTable'

  DynamoDBTableArn:
    Description: ARN of the DynamoDB reference data table
    Value: !GetAtt ReferenceDataTable.Arn
    Export:
      Name: !Sub '${AWS::StackName}-DynamoDBTableArn'

  KinesisStreamName:
    Description: Name of the Kinesis Data Stream for raw events
    Value: !Ref RawEventsStream
    Export:
      Name: !Sub '${AWS::StackName}-KinesisStream'

  KinesisStreamArn:
    Description: ARN of the Kinesis Data Stream for raw events
    Value: !GetAtt RawEventsStream.Arn
    Export:
      Name: !Sub '${AWS::StackName}-KinesisStreamArn'

  LambdaFunctionName:
    Description: Name of the Lambda enrichment function
    Value: !Ref EnrichmentLambdaFunction
    Export:
      Name: !Sub '${AWS::StackName}-LambdaFunction'

  LambdaFunctionArn:
    Description: ARN of the Lambda enrichment function
    Value: !GetAtt EnrichmentLambdaFunction.Arn
    Export:
      Name: !Sub '${AWS::StackName}-LambdaFunctionArn'

  FirehoseDeliveryStreamName:
    Description: Name of the Kinesis Data Firehose delivery stream
    Value: !Ref EventIngestionFirehose
    Export:
      Name: !Sub '${AWS::StackName}-FirehoseStream'

  FirehoseDeliveryStreamArn:
    Description: ARN of the Kinesis Data Firehose delivery stream
    Value: !GetAtt EventIngestionFirehose.Arn
    Export:
      Name: !Sub '${AWS::StackName}-FirehoseStreamArn'

  PipesExecutionRoleArn:
    Description: ARN of the EventBridge Pipes execution role
    Value: !GetAtt PipesExecutionRole.Arn
    Export:
      Name: !Sub '${AWS::StackName}-PipesRole'

  RandomSuffix:
    Description: Random suffix used for resource naming
    Value: !GetAtt RandomSuffixGenerator.RandomSuffix
    Export:
      Name: !Sub '${AWS::StackName}-RandomSuffix'

  # Test Commands Output
  TestCommands:
    Description: Commands to test the pipeline
    Value: !Sub 
      - |
        # Send test event to Kinesis Data Stream:
        aws kinesis put-record --stream-name ${StreamName} --data '{"eventId":"test-001","productId":"PROD-001","quantity":5,"timestamp":"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"}' --partition-key "PROD-001"
        
        # Check S3 bucket for enriched data:
        aws s3 ls s3://${BucketName}/enriched-data/ --recursive
        
        # Monitor Lambda function logs:
        aws logs tail /aws/lambda/${FunctionName} --follow
      - StreamName: !Ref RawEventsStream
        BucketName: !Ref EnrichedDataBucket
        FunctionName: !Ref EnrichmentLambdaFunction

  # Architecture Summary
  ArchitectureSummary:
    Description: Summary of deployed architecture
    Value: !Sub |
      Real-time stream enrichment pipeline deployed with:
      - Kinesis Data Stream: ${RawEventsStream}
      - Lambda Enrichment: ${EnrichmentLambdaFunction}
      - DynamoDB Reference: ${ReferenceDataTable}
      - Firehose Delivery: ${EventIngestionFirehose}
      - S3 Storage: ${EnrichedDataBucket}
      - EventBridge Pipe: Created via custom resource