AWSTemplateFormatVersion: '2010-09-09'
Description: 'Real-time Database Change Streams with DynamoDB Streams - Complete infrastructure for processing DynamoDB changes in real-time using Lambda, SNS, and S3 audit logging with enhanced security and monitoring'

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Application Configuration"
        Parameters:
          - ApplicationName
          - Environment
          - NotificationEmail
          - CostCenter
      - Label:
          default: "DynamoDB Configuration"
        Parameters:
          - TableReadCapacityUnits
          - TableWriteCapacityUnits
          - StreamViewType
          - EnablePointInTimeRecovery
          - EnableDeletionProtection
      - Label:
          default: "Lambda Configuration"
        Parameters:
          - LambdaTimeout
          - LambdaMemorySize
          - BatchSize
          - MaximumBatchingWindowInSeconds
          - MaximumRetryAttempts
          - ParallelizationFactor
          - ReservedConcurrency
      - Label:
          default: "Security Configuration"
        Parameters:
          - EnableEncryption
          - KMSKeyRotation
          - S3BucketVersioning
          - EnableAccessLogging
      - Label:
          default: "Monitoring Configuration"
        Parameters:
          - EnableDetailedMonitoring
          - AlarmEvaluationPeriods
          - DeadLetterQueueRetentionDays
          - LogRetentionDays
          - EnableXRayTracing
      - Label:
          default: "Network Configuration"
        Parameters:
          - VpcId
          - SubnetIds
          - EnableVpcEndpoints
    ParameterLabels:
      ApplicationName:
        default: "Application Name"
      Environment:
        default: "Environment"
      NotificationEmail:
        default: "Notification Email Address"
      CostCenter:
        default: "Cost Center"
      TableReadCapacityUnits:
        default: "DynamoDB Read Capacity Units"
      TableWriteCapacityUnits:
        default: "DynamoDB Write Capacity Units"
      StreamViewType:
        default: "Stream View Type"
      EnablePointInTimeRecovery:
        default: "Enable Point-in-Time Recovery"
      EnableDeletionProtection:
        default: "Enable Deletion Protection"
      LambdaTimeout:
        default: "Lambda Timeout (seconds)"
      LambdaMemorySize:
        default: "Lambda Memory Size (MB)"
      BatchSize:
        default: "Stream Processing Batch Size"
      MaximumBatchingWindowInSeconds:
        default: "Maximum Batching Window (seconds)"
      MaximumRetryAttempts:
        default: "Maximum Retry Attempts"
      ParallelizationFactor:
        default: "Parallelization Factor"
      ReservedConcurrency:
        default: "Lambda Reserved Concurrency"
      EnableEncryption:
        default: "Enable Encryption at Rest"
      KMSKeyRotation:
        default: "Enable KMS Key Rotation"
      S3BucketVersioning:
        default: "Enable S3 Bucket Versioning"
      EnableAccessLogging:
        default: "Enable S3 Access Logging"
      EnableDetailedMonitoring:
        default: "Enable Detailed CloudWatch Monitoring"
      AlarmEvaluationPeriods:
        default: "CloudWatch Alarm Evaluation Periods"
      DeadLetterQueueRetentionDays:
        default: "Dead Letter Queue Retention (days)"
      LogRetentionDays:
        default: "CloudWatch Log Retention (days)"
      EnableXRayTracing:
        default: "Enable AWS X-Ray Tracing"
      VpcId:
        default: "VPC ID (Optional)"
      SubnetIds:
        default: "Subnet IDs (Optional)"
      EnableVpcEndpoints:
        default: "Enable VPC Endpoints"

Parameters:
  ApplicationName:
    Type: String
    Default: 'stream-processor'
    Description: 'Name of the application (used in resource naming)'
    AllowedPattern: '[a-z][a-z0-9-]*'
    ConstraintDescription: 'Must start with lowercase letter and contain only lowercase letters, numbers, and hyphens'
    MinLength: 3
    MaxLength: 20
  
  Environment:
    Type: String
    Default: 'prod'
    Description: 'Environment name (dev, staging, prod)'
    AllowedValues:
      - dev
      - staging
      - prod
  
  NotificationEmail:
    Type: String
    Description: 'Email address for SNS notifications and CloudWatch alarms'
    AllowedPattern: '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    ConstraintDescription: 'Must be a valid email address'
  
  CostCenter:
    Type: String
    Default: 'engineering'
    Description: 'Cost center for billing and resource tracking'
    MinLength: 3
    MaxLength: 50
  
  TableReadCapacityUnits:
    Type: Number
    Default: 5
    Description: 'Read capacity units for DynamoDB table'
    MinValue: 1
    MaxValue: 40000
  
  TableWriteCapacityUnits:
    Type: Number
    Default: 5
    Description: 'Write capacity units for DynamoDB table'
    MinValue: 1
    MaxValue: 40000
  
  StreamViewType:
    Type: String
    Default: 'NEW_AND_OLD_IMAGES'
    Description: 'DynamoDB Stream view type'
    AllowedValues:
      - KEYS_ONLY
      - NEW_IMAGE
      - OLD_IMAGE
      - NEW_AND_OLD_IMAGES
  
  EnablePointInTimeRecovery:
    Type: String
    Default: 'true'
    Description: 'Enable point-in-time recovery for DynamoDB table'
    AllowedValues: ['true', 'false']
  
  EnableDeletionProtection:
    Type: String
    Default: 'true'
    Description: 'Enable deletion protection for DynamoDB table'
    AllowedValues: ['true', 'false']
  
  LambdaTimeout:
    Type: Number
    Default: 60
    Description: 'Lambda function timeout in seconds'
    MinValue: 3
    MaxValue: 900
  
  LambdaMemorySize:
    Type: Number
    Default: 256
    Description: 'Lambda function memory size in MB'
    AllowedValues: [128, 256, 512, 1024, 1536, 3008]
  
  BatchSize:
    Type: Number
    Default: 10
    Description: 'Number of records to process in each Lambda invocation'
    MinValue: 1
    MaxValue: 1000
  
  MaximumBatchingWindowInSeconds:
    Type: Number
    Default: 5
    Description: 'Maximum time to wait for batch accumulation'
    MinValue: 0
    MaxValue: 300
  
  MaximumRetryAttempts:
    Type: Number
    Default: 3
    Description: 'Maximum retry attempts for failed Lambda invocations'
    MinValue: 0
    MaxValue: 10000
  
  ParallelizationFactor:
    Type: Number
    Default: 2
    Description: 'Number of concurrent Lambda invocations per shard'
    MinValue: 1
    MaxValue: 10
  
  ReservedConcurrency:
    Type: Number
    Default: 100
    Description: 'Reserved concurrency for Lambda function'
    MinValue: 1
    MaxValue: 1000
  
  EnableEncryption:
    Type: String
    Default: 'true'
    Description: 'Enable encryption at rest for all resources'
    AllowedValues: ['true', 'false']
  
  KMSKeyRotation:
    Type: String
    Default: 'true'
    Description: 'Enable automatic KMS key rotation'
    AllowedValues: ['true', 'false']
  
  S3BucketVersioning:
    Type: String
    Default: 'true'
    Description: 'Enable S3 bucket versioning'
    AllowedValues: ['true', 'false']
  
  EnableAccessLogging:
    Type: String
    Default: 'true'
    Description: 'Enable S3 access logging'
    AllowedValues: ['true', 'false']
  
  EnableDetailedMonitoring:
    Type: String
    Default: 'true'
    Description: 'Enable detailed CloudWatch monitoring'
    AllowedValues: ['true', 'false']
  
  AlarmEvaluationPeriods:
    Type: Number
    Default: 1
    Description: 'Number of evaluation periods for CloudWatch alarms'
    MinValue: 1
    MaxValue: 10
  
  DeadLetterQueueRetentionDays:
    Type: Number
    Default: 14
    Description: 'Message retention period for dead letter queue in days'
    MinValue: 1
    MaxValue: 14
  
  LogRetentionDays:
    Type: Number
    Default: 30
    Description: 'CloudWatch log retention in days'
    AllowedValues: [1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, 3653]
  
  EnableXRayTracing:
    Type: String
    Default: 'true'
    Description: 'Enable AWS X-Ray tracing for Lambda'
    AllowedValues: ['true', 'false']
  
  VpcId:
    Type: String
    Default: ''
    Description: 'VPC ID for Lambda function (optional for VPC deployment)'
  
  SubnetIds:
    Type: CommaDelimitedList
    Default: ''
    Description: 'Subnet IDs for Lambda function (required if VPC is specified)'
  
  EnableVpcEndpoints:
    Type: String
    Default: 'false'
    Description: 'Create VPC endpoints for AWS services'
    AllowedValues: ['true', 'false']

Conditions:
  IsProduction: !Equals [!Ref Environment, 'prod']
  IsStaging: !Equals [!Ref Environment, 'staging']
  EnableMonitoring: !Equals [!Ref EnableDetailedMonitoring, 'true']
  EnableAdvancedRetry: !Not [!Equals [!Ref MaximumRetryAttempts, 0]]
  EnableEncryptionCondition: !Equals [!Ref EnableEncryption, 'true']
  EnableKeyRotation: !And [!Condition EnableEncryptionCondition, !Equals [!Ref KMSKeyRotation, 'true']]
  EnableVersioning: !Equals [!Ref S3BucketVersioning, 'true']
  EnableAccessLoggingCondition: !Equals [!Ref EnableAccessLogging, 'true']
  EnableXRayTracingCondition: !Equals [!Ref EnableXRayTracing, 'true']
  EnableVpcConfiguration: !Not [!Equals [!Ref VpcId, '']]
  EnableVpcEndpointsCondition: !And [!Condition EnableVpcConfiguration, !Equals [!Ref EnableVpcEndpoints, 'true']]
  EnablePITR: !Equals [!Ref EnablePointInTimeRecovery, 'true']
  EnableDeletionProtectionCondition: !Equals [!Ref EnableDeletionProtection, 'true']

Resources:
  # KMS Key for Encryption
  StreamProcessorKMSKey:
    Type: AWS::KMS::Key
    Condition: EnableEncryptionCondition
    Properties:
      Description: 'KMS key for DynamoDB Stream Processor encryption'
      EnableKeyRotation: !If [EnableKeyRotation, true, false]
      KeyPolicy:
        Version: '2012-10-17'
        Statement:
          - Sid: Enable IAM User Permissions
            Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: 'kms:*'
            Resource: '*'
          - Sid: Allow DynamoDB Service
            Effect: Allow
            Principal:
              Service: dynamodb.amazonaws.com
            Action:
              - kms:Decrypt
              - kms:GenerateDataKey
            Resource: '*'
          - Sid: Allow Lambda Service
            Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action:
              - kms:Decrypt
              - kms:GenerateDataKey
            Resource: '*'
          - Sid: Allow SNS Service
            Effect: Allow
            Principal:
              Service: sns.amazonaws.com
            Action:
              - kms:Decrypt
              - kms:GenerateDataKey
            Resource: '*'
          - Sid: Allow SQS Service
            Effect: Allow
            Principal:
              Service: sqs.amazonaws.com
            Action:
              - kms:Decrypt
              - kms:GenerateDataKey
            Resource: '*'
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-kms-key'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

  # KMS Key Alias
  StreamProcessorKMSKeyAlias:
    Type: AWS::KMS::Alias
    Condition: EnableEncryptionCondition
    Properties:
      AliasName: !Sub 'alias/${ApplicationName}-stream-processor-${Environment}'
      TargetKeyId: !Ref StreamProcessorKMSKey

  # S3 Bucket for Access Logs
  AccessLogsBucket:
    Type: AWS::S3::Bucket
    Condition: EnableAccessLoggingCondition
    Properties:
      BucketName: !Sub '${ApplicationName}-access-logs-${AWS::AccountId}-${AWS::Region}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: !If [EnableEncryptionCondition, 'aws:kms', 'AES256']
              KMSMasterKeyID: !If [EnableEncryptionCondition, !Ref StreamProcessorKMSKey, !Ref 'AWS::NoValue']
            BucketKeyEnabled: true
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: DeleteAccessLogs
            Status: Enabled
            ExpirationInDays: 90
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-access-logs'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

  # S3 Bucket for Audit Logs
  AuditLogsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${ApplicationName}-activity-audit-${AWS::AccountId}-${AWS::Region}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: !If [EnableEncryptionCondition, 'aws:kms', 'AES256']
              KMSMasterKeyID: !If [EnableEncryptionCondition, !Ref StreamProcessorKMSKey, !Ref 'AWS::NoValue']
            BucketKeyEnabled: true
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      VersioningConfiguration:
        Status: !If [EnableVersioning, 'Enabled', 'Suspended']
      LoggingConfiguration: !If
        - EnableAccessLoggingCondition
        - DestinationBucketName: !Ref AccessLogsBucket
          LogFilePrefix: 'audit-bucket-access/'
        - !Ref 'AWS::NoValue'
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldVersions
            Status: Enabled
            NoncurrentVersionExpirationInDays: 30
          - Id: TransitionToIA
            Status: Enabled
            TransitionInDays: 30
            StorageClass: STANDARD_IA
          - Id: TransitionToGlacier
            Status: Enabled
            TransitionInDays: 90
            StorageClass: GLACIER
          - Id: DeleteIncompleteMultipartUploads
            Status: Enabled
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      NotificationConfiguration:
        CloudWatchConfigurations:
          - Event: 's3:ObjectCreated:*'
            CloudWatchConfiguration:
              LogGroupName: !Ref AuditLogGroup
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-audit-logs'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: Purpose
          Value: 'DynamoDB Stream Audit Logging'

  # S3 Bucket Policy
  AuditLogsBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref AuditLogsBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: DenyInsecureConnections
            Effect: Deny
            Principal: '*'
            Action: 's3:*'
            Resource:
              - !Sub '${AuditLogsBucket}/*'
              - !Ref AuditLogsBucket
            Condition:
              Bool:
                'aws:SecureTransport': 'false'
          - Sid: DenyUnencryptedObjectUploads
            Effect: Deny
            Principal: '*'
            Action: 's3:PutObject'
            Resource: !Sub '${AuditLogsBucket}/*'
            Condition:
              StringNotEquals:
                's3:x-amz-server-side-encryption': !If [EnableEncryptionCondition, 'aws:kms', 'AES256']
          - Sid: AllowLambdaAccess
            Effect: Allow
            Principal:
              AWS: !GetAtt StreamProcessorRole.Arn
            Action:
              - 's3:PutObject'
              - 's3:PutObjectAcl'
              - 's3:GetObject'
            Resource: !Sub '${AuditLogsBucket}/*'

  # DynamoDB Table with Streams
  UserActivitiesTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub '${ApplicationName}-user-activities-${Environment}'
      AttributeDefinitions:
        - AttributeName: UserId
          AttributeType: S
        - AttributeName: ActivityId
          AttributeType: S
        - AttributeName: Timestamp
          AttributeType: N
      KeySchema:
        - AttributeName: UserId
          KeyType: HASH
        - AttributeName: ActivityId
          KeyType: RANGE
      GlobalSecondaryIndexes:
        - IndexName: TimestampIndex
          KeySchema:
            - AttributeName: UserId
              KeyType: HASH
            - AttributeName: Timestamp
              KeyType: RANGE
          Projection:
            ProjectionType: ALL
          ProvisionedThroughput:
            ReadCapacityUnits: !Ref TableReadCapacityUnits
            WriteCapacityUnits: !Ref TableWriteCapacityUnits
      BillingMode: PROVISIONED
      ProvisionedThroughput:
        ReadCapacityUnits: !Ref TableReadCapacityUnits
        WriteCapacityUnits: !Ref TableWriteCapacityUnits
      StreamSpecification:
        StreamViewType: !Ref StreamViewType
      PointInTimeRecoverySpecification:
        PointInTimeRecoveryEnabled: !If [EnablePITR, true, false]
      DeletionProtectionEnabled: !If [EnableDeletionProtectionCondition, true, false]
      SSESpecification: !If
        - EnableEncryptionCondition
        - SSEEnabled: true
          KMSMasterKeyId: !Ref StreamProcessorKMSKey
        - !Ref 'AWS::NoValue'
      ContributorInsightsSpecification:
        Enabled: !If [EnableMonitoring, true, false]
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-user-activities'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: Purpose
          Value: 'User Activities with Stream Processing'

  # SNS Topic for Notifications
  ActivityNotificationsTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub '${ApplicationName}-activity-notifications-${Environment}'
      DisplayName: 'DynamoDB Activity Notifications'
      KmsMasterKeyId: !If [EnableEncryptionCondition, !Ref StreamProcessorKMSKey, 'alias/aws/sns']
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-notifications'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: Purpose
          Value: 'Real-time Activity Notifications'

  # SNS Topic Policy
  ActivityNotificationsTopicPolicy:
    Type: AWS::SNS::TopicPolicy
    Properties:
      Topics:
        - !Ref ActivityNotificationsTopic
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: AllowLambdaPublish
            Effect: Allow
            Principal:
              AWS: !GetAtt StreamProcessorRole.Arn
            Action:
              - 'sns:Publish'
            Resource: !Ref ActivityNotificationsTopic
          - Sid: AllowCloudWatchAlarms
            Effect: Allow
            Principal:
              Service: cloudwatch.amazonaws.com
            Action:
              - 'sns:Publish'
            Resource: !Ref ActivityNotificationsTopic

  # SNS Email Subscription
  EmailSubscription:
    Type: AWS::SNS::Subscription
    Properties:
      TopicArn: !Ref ActivityNotificationsTopic
      Protocol: email
      Endpoint: !Ref NotificationEmail

  # Dead Letter Queue
  DeadLetterQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: !Sub '${ApplicationName}-stream-processor-dlq-${Environment}'
      MessageRetentionPeriod: !Ref DeadLetterQueueRetentionDays
      KmsMasterKeyId: !If [EnableEncryptionCondition, !Ref StreamProcessorKMSKey, 'alias/aws/sqs']
      VisibilityTimeoutSeconds: 300
      ReceiveMessageWaitTimeSeconds: 20
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-dlq'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: Purpose
          Value: 'Failed Stream Processing Records'

  # CloudWatch Log Group for Lambda
  StreamProcessorLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/${ApplicationName}-stream-processor-${Environment}'
      RetentionInDays: !Ref LogRetentionDays
      KmsKeyId: !If [EnableEncryptionCondition, !GetAtt StreamProcessorKMSKey.Arn, !Ref 'AWS::NoValue']
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-lambda-logs'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

  # CloudWatch Log Group for Audit Events
  AuditLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/dynamodb/${ApplicationName}-audit-${Environment}'
      RetentionInDays: !If [IsProduction, 90, 30]
      KmsKeyId: !If [EnableEncryptionCondition, !GetAtt StreamProcessorKMSKey.Arn, !Ref 'AWS::NoValue']
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-audit-logs'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

  # Security Group for Lambda (if VPC is used)
  LambdaSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Condition: EnableVpcConfiguration
    Properties:
      GroupDescription: 'Security group for DynamoDB Stream Processor Lambda'
      VpcId: !Ref VpcId
      SecurityGroupEgress:
        - IpProtocol: tcp
          FromPort: 443
          ToPort: 443
          CidrIp: 0.0.0.0/0
          Description: 'HTTPS outbound for AWS services'
        - IpProtocol: tcp
          FromPort: 53
          ToPort: 53
          CidrIp: 0.0.0.0/0
          Description: 'DNS TCP'
        - IpProtocol: udp
          FromPort: 53
          ToPort: 53
          CidrIp: 0.0.0.0/0
          Description: 'DNS UDP'
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-lambda-sg'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

  # VPC Endpoints for AWS Services (if VPC is used)
  S3VpcEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Condition: EnableVpcEndpointsCondition
    Properties:
      VpcId: !Ref VpcId
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.s3'
      VpcEndpointType: Gateway
      RouteTableIds: []

  DynamoDBVpcEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Condition: EnableVpcEndpointsCondition
    Properties:
      VpcId: !Ref VpcId
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.dynamodb'
      VpcEndpointType: Gateway
      RouteTableIds: []

  SNSVpcEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Condition: EnableVpcEndpointsCondition
    Properties:
      VpcId: !Ref VpcId
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.sns'
      VpcEndpointType: Interface
      SubnetIds: !Ref SubnetIds
      SecurityGroupIds:
        - !Ref LambdaSecurityGroup

  # IAM Role for Lambda Stream Processor
  StreamProcessorRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ApplicationName}-stream-processor-role-${Environment}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaDynamoDBExecutionRole
        - !If [EnableVpcConfiguration, 'arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole', !Ref 'AWS::NoValue']
        - !If [EnableXRayTracingCondition, 'arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess', !Ref 'AWS::NoValue']
      Policies:
        - PolicyName: StreamProcessorCustomPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'sns:Publish'
                Resource: !Ref ActivityNotificationsTopic
              - Effect: Allow
                Action:
                  - 's3:PutObject'
                  - 's3:PutObjectAcl'
                  - 's3:GetObject'
                Resource: !Sub '${AuditLogsBucket}/*'
              - Effect: Allow
                Action:
                  - 'sqs:SendMessage'
                  - 'sqs:GetQueueAttributes'
                Resource: !GetAtt DeadLetterQueue.Arn
              - Effect: Allow
                Action:
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource:
                  - !Sub '${StreamProcessorLogGroup}:*'
                  - !Sub '${AuditLogGroup}:*'
              - !If
                - EnableEncryptionCondition
                - Effect: Allow
                  Action:
                    - 'kms:Decrypt'
                    - 'kms:GenerateDataKey'
                    - 'kms:DescribeKey'
                  Resource: !GetAtt StreamProcessorKMSKey.Arn
                - !Ref 'AWS::NoValue'
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-lambda-role'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

  # Lambda Function for Stream Processing
  StreamProcessorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ApplicationName}-stream-processor-${Environment}'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt StreamProcessorRole.Arn
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemorySize
      ReservedConcurrencyLimit: !Ref ReservedConcurrency
      DeadLetterConfig:
        TargetArn: !GetAtt DeadLetterQueue.Arn
      VpcConfig: !If
        - EnableVpcConfiguration
        - SecurityGroupIds:
            - !Ref LambdaSecurityGroup
          SubnetIds: !Ref SubnetIds
        - !Ref 'AWS::NoValue'
      TracingConfig:
        Mode: !If [EnableXRayTracingCondition, 'Active', 'PassThrough']
      Environment:
        Variables:
          SNS_TOPIC_ARN: !Ref ActivityNotificationsTopic
          S3_BUCKET_NAME: !Ref AuditLogsBucket
          ENVIRONMENT: !Ref Environment
          LOG_LEVEL: !If [IsProduction, 'INFO', 'DEBUG']
          KMS_KEY_ID: !If [EnableEncryptionCondition, !Ref StreamProcessorKMSKey, '']
          COST_CENTER: !Ref CostCenter
      Code:
        ZipFile: |
          import json
          import boto3
          import logging
          from datetime import datetime
          from decimal import Decimal
          import os
          import traceback
          from botocore.exceptions import ClientError
          
          # Configure logging
          logger = logging.getLogger()
          logger.setLevel(os.environ.get('LOG_LEVEL', 'INFO'))
          
          # Initialize AWS clients with retry configuration
          session = boto3.Session()
          sns = session.client('sns', config=boto3.session.Config(
              retries={'max_attempts': 3, 'mode': 'adaptive'}
          ))
          s3 = session.client('s3', config=boto3.session.Config(
              retries={'max_attempts': 3, 'mode': 'adaptive'}
          ))
          
          # Environment variables
          SNS_TOPIC_ARN = os.environ['SNS_TOPIC_ARN']
          S3_BUCKET_NAME = os.environ['S3_BUCKET_NAME']
          ENVIRONMENT = os.environ.get('ENVIRONMENT', 'dev')
          KMS_KEY_ID = os.environ.get('KMS_KEY_ID', '')
          COST_CENTER = os.environ.get('COST_CENTER', 'engineering')
          
          # Metrics tracking
          metrics = {
              'processed_records': 0,
              'failed_records': 0,
              'insert_events': 0,
              'modify_events': 0,
              'remove_events': 0
          }
          
          def decimal_default(obj):
              """JSON serializer for objects not serializable by default json code"""
              if isinstance(obj, Decimal):
                  # Convert Decimal to float, but handle special cases
                  if obj % 1 == 0:
                      return int(obj)
                  return float(obj)
              raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
          
          def lambda_handler(event, context):
              """Main Lambda handler for DynamoDB stream processing"""
              try:
                  logger.info(f"Processing {len(event['Records'])} stream records")
                  logger.debug(f"Lambda context: {vars(context)}")
                  
                  # Initialize metrics
                  metrics['processed_records'] = 0
                  metrics['failed_records'] = 0
                  metrics['insert_events'] = 0
                  metrics['modify_events'] = 0
                  metrics['remove_events'] = 0
                  
                  failed_records = []
                  
                  for i, record in enumerate(event['Records']):
                      try:
                          process_stream_record(record, i)
                          metrics['processed_records'] += 1
                          
                          # Track event types
                          event_name = record.get('eventName', '')
                          if event_name == 'INSERT':
                              metrics['insert_events'] += 1
                          elif event_name == 'MODIFY':
                              metrics['modify_events'] += 1
                          elif event_name == 'REMOVE':
                              metrics['remove_events'] += 1
                              
                      except Exception as e:
                          logger.error(f"Error processing record {i}: {str(e)}", exc_info=True)
                          metrics['failed_records'] += 1
                          failed_records.append({
                              'record_index': i,
                              'error': str(e),
                              'record_id': record.get('dynamodb', {}).get('SequenceNumber', 'unknown')
                          })
                  
                  # Log final metrics
                  logger.info(f"Processing complete. Metrics: {metrics}")
                  
                  # If there are failed records, raise exception to trigger retry
                  if failed_records:
                      error_msg = f"Failed to process {len(failed_records)} records: {failed_records}"
                      logger.error(error_msg)
                      raise Exception(error_msg)
                  
                  return {
                      'statusCode': 200,
                      'body': json.dumps({
                          'message': f'Successfully processed {metrics["processed_records"]} records',
                          'metrics': metrics,
                          'timestamp': datetime.utcnow().isoformat()
                      })
                  }
                  
              except Exception as e:
                  logger.error(f"Fatal error in lambda_handler: {str(e)}", exc_info=True)
                  # Send critical error notification
                  try:
                      send_critical_error_notification(str(e), context)
                  except:
                      logger.error("Failed to send critical error notification", exc_info=True)
                  raise e
          
          def process_stream_record(record, record_index):
              """Process individual DynamoDB stream record"""
              try:
                  # Extract basic record information
                  event_name = record['eventName']
                  event_source = record.get('eventSource', 'aws:dynamodb')
                  aws_region = record.get('awsRegion', 'unknown')
                  
                  # Extract key information
                  dynamodb_data = record.get('dynamodb', {})
                  keys = dynamodb_data.get('Keys', {})
                  
                  if 'UserId' not in keys or 'ActivityId' not in keys:
                      logger.warning(f"Record {record_index} missing required keys: {keys}")
                      return
                  
                  user_id = keys['UserId']['S']
                  activity_id = keys['ActivityId']['S']
                  
                  logger.info(f"Processing {event_name} event for user {user_id}, activity {activity_id}")
                  
                  # Create comprehensive audit record
                  audit_record = {
                      'recordIndex': record_index,
                      'timestamp': datetime.utcnow().isoformat(),
                      'eventName': event_name,
                      'eventSource': event_source,
                      'awsRegion': aws_region,
                      'userId': user_id,
                      'activityId': activity_id,
                      'environment': ENVIRONMENT,
                      'costCenter': COST_CENTER,
                      'processingContext': {
                          'lambdaRequestId': os.environ.get('AWS_REQUEST_ID', 'unknown'),
                          'lambdaFunctionName': os.environ.get('AWS_LAMBDA_FUNCTION_NAME', 'unknown'),
                          'lambdaFunctionVersion': os.environ.get('AWS_LAMBDA_FUNCTION_VERSION', 'unknown')
                      }
                  }
                  
                  # Add sequence information for ordering
                  if 'SequenceNumber' in dynamodb_data:
                      audit_record['sequenceNumber'] = dynamodb_data['SequenceNumber']
                  if 'ApproximateCreationDateTime' in dynamodb_data:
                      audit_record['approximateCreationDateTime'] = dynamodb_data['ApproximateCreationDateTime']
                  
                  # Add old and new images if available
                  if 'OldImage' in dynamodb_data:
                      audit_record['oldImage'] = dynamodb_data['OldImage']
                  if 'NewImage' in dynamodb_data:
                      audit_record['newImage'] = dynamodb_data['NewImage']
                  
                  # Add size information
                  if 'SizeBytes' in dynamodb_data:
                      audit_record['sizeBytes'] = dynamodb_data['SizeBytes']
                  
                  # Store audit record in S3
                  store_audit_record(audit_record)
                  
                  # Send notification based on event type
                  notification_message = generate_notification_message(event_name, user_id, activity_id, audit_record)
                  send_notification(notification_message, audit_record)
                  
              except Exception as e:
                  logger.error(f"Error processing stream record {record_index}: {str(e)}", exc_info=True)
                  raise e
          
          def store_audit_record(audit_record):
              """Store audit record in S3 with partitioned structure"""
              try:
                  # Generate S3 key with date partitioning for efficient querying
                  timestamp = datetime.utcnow()
                  date_partition = timestamp.strftime('%Y/%m/%d/%H')
                  file_timestamp = timestamp.strftime('%Y%m%d-%H%M%S-%f')[:-3]  # Include milliseconds
                  s3_key = f"audit-logs/{date_partition}/{audit_record['userId']}-{audit_record['activityId']}-{file_timestamp}.json"
                  
                  # Add metadata for S3 object
                  metadata = {
                      'event-name': audit_record['eventName'],
                      'user-id': audit_record['userId'],
                      'activity-id': audit_record['activityId'],
                      'environment': audit_record['environment'],
                      'cost-center': audit_record['costCenter'],
                      'sequence-number': audit_record.get('sequenceNumber', 'unknown')
                  }
                  
                  # Prepare S3 put object parameters
                  put_params = {
                      'Bucket': S3_BUCKET_NAME,
                      'Key': s3_key,
                      'Body': json.dumps(audit_record, default=decimal_default, indent=2),
                      'ContentType': 'application/json',
                      'Metadata': metadata
                  }
                  
                  # Add encryption if KMS key is provided
                  if KMS_KEY_ID:
                      put_params['ServerSideEncryption'] = 'aws:kms'
                      put_params['SSEKMSKeyId'] = KMS_KEY_ID
                  else:
                      put_params['ServerSideEncryption'] = 'AES256'
                  
                  # Store in S3
                  response = s3.put_object(**put_params)
                  
                  logger.info(f"Audit record stored: s3://{S3_BUCKET_NAME}/{s3_key} (ETag: {response.get('ETag', 'unknown')})")
                  
              except ClientError as e:
                  error_code = e.response['Error']['Code']
                  logger.error(f"S3 ClientError {error_code}: {str(e)}")
                  raise e
              except Exception as e:
                  logger.error(f"Failed to store audit record: {str(e)}", exc_info=True)
                  raise e
          
          def generate_notification_message(event_name, user_id, activity_id, audit_record):
              """Generate contextual notification message based on event type"""
              base_info = f"User: {user_id}, Activity: {activity_id}"
              
              if event_name == 'INSERT':
                  return f"New activity created - {base_info}"
              elif event_name == 'MODIFY':
                  return f"Activity updated - {base_info}"
              elif event_name == 'REMOVE':
                  return f"Activity deleted - {base_info}"
              else:
                  return f"Unknown event '{event_name}' - {base_info}"
          
          def send_notification(message, audit_record):
              """Send SNS notification with structured message"""
              try:
                  # Create structured notification payload
                  notification_payload = {
                      'message': message,
                      'details': {
                          'eventName': audit_record['eventName'],
                          'userId': audit_record['userId'],
                          'activityId': audit_record['activityId'],
                          'timestamp': audit_record['timestamp'],
                          'environment': audit_record['environment'],
                          'sequenceNumber': audit_record.get('sequenceNumber', 'unknown'),
                          'awsRegion': audit_record.get('awsRegion', 'unknown')
                      },
                      'metadata': {
                          'source': 'DynamoDB-Stream-Processor',
                          'version': '1.0',
                          'costCenter': audit_record['costCenter']
                      }
                  }
                  
                  # Determine message urgency based on event type
                  subject_prefix = "INFO"
                  if audit_record['eventName'] == 'REMOVE':
                      subject_prefix = "WARN"
                  
                  # Send SNS notification
                  response = sns.publish(
                      TopicArn=SNS_TOPIC_ARN,
                      Message=json.dumps(notification_payload, default=decimal_default, indent=2),
                      Subject=f"[{subject_prefix}] DynamoDB Activity: {audit_record['eventName']} [{ENVIRONMENT.upper()}]",
                      MessageAttributes={
                          'eventName': {
                              'DataType': 'String',
                              'StringValue': audit_record['eventName']
                          },
                          'environment': {
                              'DataType': 'String',
                              'StringValue': audit_record['environment']
                          },
                          'userId': {
                              'DataType': 'String',
                              'StringValue': audit_record['userId']
                          },
                          'costCenter': {
                              'DataType': 'String',
                              'StringValue': audit_record['costCenter']
                          }
                      }
                  )
                  
                  logger.info(f"Notification sent successfully: {message} (MessageId: {response['MessageId']})")
                  
              except ClientError as e:
                  error_code = e.response['Error']['Code']
                  logger.error(f"SNS ClientError {error_code}: {str(e)}")
                  raise e
              except Exception as e:
                  logger.error(f"Failed to send notification: {str(e)}", exc_info=True)
                  raise e
          
          def send_critical_error_notification(error_message, context):
              """Send critical error notification"""
              try:
                  critical_payload = {
                      'alertType': 'CRITICAL_ERROR',
                      'service': 'DynamoDB-Stream-Processor',
                      'environment': ENVIRONMENT,
                      'error': error_message,
                      'context': {
                          'functionName': context.function_name,
                          'functionVersion': context.function_version,
                          'requestId': context.aws_request_id,
                          'remainingTime': context.get_remaining_time_in_millis()
                      },
                      'timestamp': datetime.utcnow().isoformat()
                  }
                  
                  sns.publish(
                      TopicArn=SNS_TOPIC_ARN,
                      Message=json.dumps(critical_payload, indent=2),
                      Subject=f"[CRITICAL] Lambda Error in {ENVIRONMENT.upper()} - DynamoDB Stream Processor",
                      MessageAttributes={
                          'alertType': {
                              'DataType': 'String',
                              'StringValue': 'CRITICAL_ERROR'
                          },
                          'environment': {
                              'DataType': 'String',
                              'StringValue': ENVIRONMENT
                          }
                      }
                  )
                  
              except Exception as e:
                  logger.error(f"Failed to send critical error notification: {str(e)}")
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-stream-processor'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: Purpose
          Value: 'DynamoDB Stream Processing'

  # Event Source Mapping for DynamoDB Streams
  StreamEventSourceMapping:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      EventSourceArn: !GetAtt UserActivitiesTable.StreamArn
      FunctionName: !Ref StreamProcessorFunction
      StartingPosition: LATEST
      BatchSize: !Ref BatchSize
      MaximumBatchingWindowInSeconds: !Ref MaximumBatchingWindowInSeconds
      MaximumRecordAgeInSeconds: 3600
      BisectBatchOnFunctionError: true
      MaximumRetryAttempts: !If [EnableAdvancedRetry, !Ref MaximumRetryAttempts, !Ref 'AWS::NoValue']
      ParallelizationFactor: !Ref ParallelizationFactor
      FilterCriteria:
        Filters:
          - Pattern: '{"eventName": ["INSERT", "MODIFY", "REMOVE"]}'

  # CloudWatch Alarms
  LambdaErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: EnableMonitoring
    Properties:
      AlarmName: !Sub '${ApplicationName}-lambda-errors-${Environment}'
      AlarmDescription: 'Lambda function errors detected'
      MetricName: Errors
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: !Ref AlarmEvaluationPeriods
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      AlarmActions:
        - !Ref ActivityNotificationsTopic
      OKActions:
        - !Ref ActivityNotificationsTopic
      Dimensions:
        - Name: FunctionName
          Value: !Ref StreamProcessorFunction
      TreatMissingData: notBreaching
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-lambda-errors'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

  LambdaThrottleAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: EnableMonitoring
    Properties:
      AlarmName: !Sub '${ApplicationName}-lambda-throttles-${Environment}'
      AlarmDescription: 'Lambda function throttles detected'
      MetricName: Throttles
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: !Ref AlarmEvaluationPeriods
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      AlarmActions:
        - !Ref ActivityNotificationsTopic
      Dimensions:
        - Name: FunctionName
          Value: !Ref StreamProcessorFunction
      TreatMissingData: notBreaching
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-lambda-throttles'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

  LambdaDurationAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: EnableMonitoring
    Properties:
      AlarmName: !Sub '${ApplicationName}-lambda-duration-${Environment}'
      AlarmDescription: 'Lambda function duration approaching timeout'
      MetricName: Duration
      Namespace: AWS/Lambda
      Statistic: Average
      Period: 300
      EvaluationPeriods: 2
      Threshold: !Ref LambdaTimeout
      ComparisonOperator: GreaterThanThreshold
      AlarmActions:
        - !Ref ActivityNotificationsTopic
      Dimensions:
        - Name: FunctionName
          Value: !Ref StreamProcessorFunction
      TreatMissingData: notBreaching
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-lambda-duration'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

  DeadLetterQueueAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: EnableMonitoring
    Properties:
      AlarmName: !Sub '${ApplicationName}-dlq-messages-${Environment}'
      AlarmDescription: 'Dead letter queue contains messages'
      MetricName: ApproximateNumberOfVisibleMessages
      Namespace: AWS/SQS
      Statistic: Average
      Period: 300
      EvaluationPeriods: !Ref AlarmEvaluationPeriods
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      AlarmActions:
        - !Ref ActivityNotificationsTopic
      Dimensions:
        - Name: QueueName
          Value: !GetAtt DeadLetterQueue.QueueName
      TreatMissingData: notBreaching
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-dlq-messages'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

  DynamoDBThrottleAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: EnableMonitoring
    Properties:
      AlarmName: !Sub '${ApplicationName}-dynamodb-throttles-${Environment}'
      AlarmDescription: 'DynamoDB table throttles detected'
      MetricName: ThrottledRequests
      Namespace: AWS/DynamoDB
      Statistic: Sum
      Period: 300
      EvaluationPeriods: !Ref AlarmEvaluationPeriods
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      AlarmActions:
        - !Ref ActivityNotificationsTopic
      Dimensions:
        - Name: TableName
          Value: !Ref UserActivitiesTable
      TreatMissingData: notBreaching
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-dynamodb-throttles'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

  DynamoDBConsumedReadCapacityAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: EnableMonitoring
    Properties:
      AlarmName: !Sub '${ApplicationName}-dynamodb-read-capacity-${Environment}'
      AlarmDescription: 'DynamoDB table read capacity utilization high'
      MetricName: ConsumedReadCapacityUnits
      Namespace: AWS/DynamoDB
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 2
      Threshold: !Ref TableReadCapacityUnits
      ComparisonOperator: GreaterThanThreshold
      AlarmActions:
        - !Ref ActivityNotificationsTopic
      Dimensions:
        - Name: TableName
          Value: !Ref UserActivitiesTable
      TreatMissingData: notBreaching
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-dynamodb-read-capacity'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

  DynamoDBConsumedWriteCapacityAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: EnableMonitoring
    Properties:
      AlarmName: !Sub '${ApplicationName}-dynamodb-write-capacity-${Environment}'
      AlarmDescription: 'DynamoDB table write capacity utilization high'
      MetricName: ConsumedWriteCapacityUnits
      Namespace: AWS/DynamoDB
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 2
      Threshold: !Ref TableWriteCapacityUnits
      ComparisonOperator: GreaterThanThreshold
      AlarmActions:
        - !Ref ActivityNotificationsTopic
      Dimensions:
        - Name: TableName
          Value: !Ref UserActivitiesTable
      TreatMissingData: notBreaching
      Tags:
        - Key: Name
          Value: !Sub '${ApplicationName}-dynamodb-write-capacity'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter

# Outputs section provides essential information for integration and verification
Outputs:
  StackName:
    Description: 'Name of this CloudFormation stack'
    Value: !Ref 'AWS::StackName'
    Export:
      Name: !Sub '${AWS::StackName}-StackName'

  DynamoDBTableName:
    Description: 'Name of the DynamoDB table with streams enabled'
    Value: !Ref UserActivitiesTable
    Export:
      Name: !Sub '${AWS::StackName}-DynamoDBTable'

  DynamoDBTableArn:
    Description: 'ARN of the DynamoDB table'
    Value: !GetAtt UserActivitiesTable.Arn
    Export:
      Name: !Sub '${AWS::StackName}-DynamoDBTableArn'

  DynamoDBStreamArn:
    Description: 'ARN of the DynamoDB stream'
    Value: !GetAtt UserActivitiesTable.StreamArn
    Export:
      Name: !Sub '${AWS::StackName}-DynamoDBStreamArn'

  LambdaFunctionName:
    Description: 'Name of the Lambda stream processor function'
    Value: !Ref StreamProcessorFunction
    Export:
      Name: !Sub '${AWS::StackName}-LambdaFunction'

  LambdaFunctionArn:
    Description: 'ARN of the Lambda stream processor function'
    Value: !GetAtt StreamProcessorFunction.Arn
    Export:
      Name: !Sub '${AWS::StackName}-LambdaFunctionArn'

  SNSTopicArn:
    Description: 'ARN of the SNS topic for notifications'
    Value: !Ref ActivityNotificationsTopic
    Export:
      Name: !Sub '${AWS::StackName}-SNSTopic'

  S3BucketName:
    Description: 'Name of the S3 bucket for audit logs'
    Value: !Ref AuditLogsBucket
    Export:
      Name: !Sub '${AWS::StackName}-S3Bucket'

  S3BucketArn:
    Description: 'ARN of the S3 bucket for audit logs'
    Value: !GetAtt AuditLogsBucket.Arn
    Export:
      Name: !Sub '${AWS::StackName}-S3BucketArn'

  DeadLetterQueueUrl:
    Description: 'URL of the dead letter queue'
    Value: !Ref DeadLetterQueue
    Export:
      Name: !Sub '${AWS::StackName}-DeadLetterQueue'

  DeadLetterQueueArn:
    Description: 'ARN of the dead letter queue'
    Value: !GetAtt DeadLetterQueue.Arn
    Export:
      Name: !Sub '${AWS::StackName}-DeadLetterQueueArn'

  EventSourceMappingId:
    Description: 'ID of the Lambda event source mapping'
    Value: !Ref StreamEventSourceMapping
    Export:
      Name: !Sub '${AWS::StackName}-EventSourceMapping'

  StreamProcessorRoleArn:
    Description: 'ARN of the Lambda execution role'
    Value: !GetAtt StreamProcessorRole.Arn
    Export:
      Name: !Sub '${AWS::StackName}-LambdaRole'

  KMSKeyId:
    Condition: EnableEncryptionCondition
    Description: 'ID of the KMS key used for encryption'
    Value: !Ref StreamProcessorKMSKey
    Export:
      Name: !Sub '${AWS::StackName}-KMSKey'

  KMSKeyArn:
    Condition: EnableEncryptionCondition
    Description: 'ARN of the KMS key used for encryption'
    Value: !GetAtt StreamProcessorKMSKey.Arn
    Export:
      Name: !Sub '${AWS::StackName}-KMSKeyArn'

  TestCommands:
    Description: 'AWS CLI commands to test the stream processing'
    Value: !Sub |
      # Insert test data:
      aws dynamodb put-item --table-name ${UserActivitiesTable} --item '{"UserId":{"S":"user123"},"ActivityId":{"S":"activity001"},"ActivityType":{"S":"LOGIN"},"Timestamp":{"N":"'$(date +%s)'"},"IPAddress":{"S":"192.168.1.100"}}'
      
      # Update test data:
      aws dynamodb update-item --table-name ${UserActivitiesTable} --key '{"UserId":{"S":"user123"},"ActivityId":{"S":"activity001"}}' --update-expression "SET ActivityType = :type, #ts = :ts" --expression-attribute-names '{"#ts": "Timestamp"}' --expression-attribute-values '{":type":{"S":"LOGOUT"},":ts":{"N":"'$(date +%s)'"}}'
      
      # Delete test data:
      aws dynamodb delete-item --table-name ${UserActivitiesTable} --key '{"UserId":{"S":"user123"},"ActivityId":{"S":"activity001"}}'
      
      # Check Lambda logs:
      aws logs filter-log-events --log-group-name /aws/lambda/${StreamProcessorFunction} --start-time $(date -d '5 minutes ago' +%s)000
      
      # List audit records:
      aws s3 ls s3://${AuditLogsBucket}/audit-logs/ --recursive
      
      # Check dead letter queue:
      aws sqs receive-message --queue-url ${DeadLetterQueue} --max-number-of-messages 10
      
      # View CloudWatch metrics:
      aws cloudwatch get-metric-statistics --namespace AWS/Lambda --metric-name Invocations --dimensions Name=FunctionName,Value=${StreamProcessorFunction} --start-time $(date -d '1 hour ago' --iso-8601) --end-time $(date --iso-8601) --period 300 --statistics Sum

  DeploymentInstructions:
    Description: 'Instructions for deploying this stack'
    Value: !Sub |
      1. Deploy the stack:
         aws cloudformation create-stack \
           --stack-name ${AWS::StackName} \
           --template-body file://cloudformation.yaml \
           --parameters ParameterKey=NotificationEmail,ParameterValue=your-email@example.com \
           --capabilities CAPABILITY_NAMED_IAM \
           --tags Key=Environment,Value=${Environment} Key=CostCenter,Value=${CostCenter}
      
      2. Wait for stack creation:
         aws cloudformation wait stack-create-complete --stack-name ${AWS::StackName}
      
      3. Confirm SNS subscription in your email
      
      4. Test with sample data using the TestCommands output
      
      5. Monitor CloudWatch logs and metrics for stream processing activity
      
      6. Review S3 audit logs and CloudWatch alarms
      
      7. Clean up (when done):
         aws cloudformation delete-stack --stack-name ${AWS::StackName}

  MonitoringDashboard:
    Description: 'CloudWatch dashboard for monitoring stream processing'
    Value: !Sub |
      Create a CloudWatch dashboard with these widgets:
      - Lambda Function Metrics: Invocations, Errors, Duration, Throttles
      - DynamoDB Metrics: ConsumedReadCapacityUnits, ConsumedWriteCapacityUnits, ThrottledRequests
      - SQS Metrics: ApproximateNumberOfVisibleMessages (DLQ)
      - S3 Metrics: NumberOfObjects, BucketSizeBytes
      
      Dashboard URL: https://${AWS::Region}.console.aws.amazon.com/cloudwatch/home?region=${AWS::Region}#dashboards:

  CostOptimizationTips:
    Description: 'Tips for optimizing costs'
    Value: |
      Cost Optimization Recommendations:
      1. Monitor DynamoDB capacity utilization and consider auto-scaling
      2. Use S3 Intelligent Tiering for audit logs older than 30 days
      3. Adjust Lambda memory size based on CloudWatch performance metrics
      4. Consider reserved capacity for predictable DynamoDB workloads
      5. Enable S3 lifecycle policies to transition old logs to cheaper storage classes
      6. Monitor CloudWatch log retention and adjust based on compliance requirements
      7. Use AWS Cost Explorer to track spending trends by service and tag