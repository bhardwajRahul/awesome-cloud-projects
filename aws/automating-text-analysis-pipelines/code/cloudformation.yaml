AWSTemplateFormatVersion: '2010-09-09'
Description: 'Infrastructure as Code for Automating Text Analysis Pipelines with Amazon Comprehend - Complete serverless NLP solution with real-time and batch processing capabilities'

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Project Configuration"
        Parameters:
          - ProjectName
          - Environment
      - Label:
          default: "S3 Configuration"
        Parameters:
          - InputBucketName
          - OutputBucketName
          - EnableVersioning
          - EnableEncryption
      - Label:
          default: "Lambda Configuration"
        Parameters:
          - LambdaTimeout
          - LambdaMemorySize
          - PythonRuntime
      - Label:
          default: "Comprehend Configuration"
        Parameters:
          - EnableCustomClassification
          - LanguageCode
      - Label:
          default: "Monitoring and Logging"
        Parameters:
          - EnableCloudWatchLogs
          - LogRetentionDays
          - EnableXRayTracing
    ParameterLabels:
      ProjectName:
        default: "Project Name"
      Environment:
        default: "Environment"
      InputBucketName:
        default: "Input Bucket Name"
      OutputBucketName:
        default: "Output Bucket Name"
      EnableVersioning:
        default: "Enable S3 Versioning"
      EnableEncryption:
        default: "Enable S3 Encryption"
      LambdaTimeout:
        default: "Lambda Timeout (seconds)"
      LambdaMemorySize:
        default: "Lambda Memory Size (MB)"
      PythonRuntime:
        default: "Python Runtime Version"
      EnableCustomClassification:
        default: "Enable Custom Classification"
      LanguageCode:
        default: "Default Language Code"
      EnableCloudWatchLogs:
        default: "Enable CloudWatch Logs"
      LogRetentionDays:
        default: "Log Retention (days)"
      EnableXRayTracing:
        default: "Enable X-Ray Tracing"

Parameters:
  ProjectName:
    Type: String
    Default: 'comprehend-nlp-pipeline'
    Description: 'Name of the project used for resource naming and tagging'
    AllowedPattern: '^[a-z0-9-]+$'
    ConstraintDescription: 'Must contain only lowercase letters, numbers, and hyphens'
    MinLength: 3
    MaxLength: 50

  Environment:
    Type: String
    Default: 'dev'
    Description: 'Environment name (dev, test, prod)'
    AllowedValues:
      - dev
      - test
      - staging
      - prod

  InputBucketName:
    Type: String
    Default: ''
    Description: 'Name for the input S3 bucket (leave empty for auto-generated name)'
    AllowedPattern: '^[a-z0-9.-]*$'
    ConstraintDescription: 'Must contain only lowercase letters, numbers, periods, and hyphens'

  OutputBucketName:
    Type: String
    Default: ''
    Description: 'Name for the output S3 bucket (leave empty for auto-generated name)'
    AllowedPattern: '^[a-z0-9.-]*$'
    ConstraintDescription: 'Must contain only lowercase letters, numbers, periods, and hyphens'

  EnableVersioning:
    Type: String
    Default: 'true'
    Description: 'Enable versioning on S3 buckets'
    AllowedValues:
      - 'true'
      - 'false'

  EnableEncryption:
    Type: String
    Default: 'true'
    Description: 'Enable server-side encryption on S3 buckets'
    AllowedValues:
      - 'true'
      - 'false'

  LambdaTimeout:
    Type: Number
    Default: 60
    Description: 'Lambda function timeout in seconds'
    MinValue: 30
    MaxValue: 900

  LambdaMemorySize:
    Type: Number
    Default: 256
    Description: 'Lambda function memory size in MB'
    AllowedValues:
      - 128
      - 256
      - 512
      - 1024
      - 2048
      - 3008

  PythonRuntime:
    Type: String
    Default: 'python3.9'
    Description: 'Python runtime version for Lambda function'
    AllowedValues:
      - python3.8
      - python3.9
      - python3.10
      - python3.11

  EnableCustomClassification:
    Type: String
    Default: 'true'
    Description: 'Deploy resources for custom document classification'
    AllowedValues:
      - 'true'
      - 'false'

  LanguageCode:
    Type: String
    Default: 'en'
    Description: 'Default language code for Comprehend analysis'
    AllowedValues:
      - en
      - es
      - fr
      - de
      - it
      - pt
      - ar
      - hi
      - ja
      - ko
      - zh
      - zh-TW

  EnableCloudWatchLogs:
    Type: String
    Default: 'true'
    Description: 'Enable CloudWatch Logs for Lambda function'
    AllowedValues:
      - 'true'
      - 'false'

  LogRetentionDays:
    Type: Number
    Default: 14
    Description: 'CloudWatch Logs retention period in days'
    AllowedValues:
      - 1
      - 3
      - 5
      - 7
      - 14
      - 30
      - 60
      - 90
      - 120
      - 150
      - 180
      - 365
      - 400
      - 545
      - 731
      - 1827
      - 3653

  EnableXRayTracing:
    Type: String
    Default: 'false'
    Description: 'Enable AWS X-Ray tracing for Lambda function'
    AllowedValues:
      - 'true'
      - 'false'

Conditions:
  # S3 Configuration Conditions
  CreateInputBucketName: !Equals [!Ref InputBucketName, '']
  CreateOutputBucketName: !Equals [!Ref OutputBucketName, '']
  EnableS3Versioning: !Equals [!Ref EnableVersioning, 'true']
  EnableS3Encryption: !Equals [!Ref EnableEncryption, 'true']
  
  # Lambda Configuration Conditions
  EnableLogs: !Equals [!Ref EnableCloudWatchLogs, 'true']
  EnableTracing: !Equals [!Ref EnableXRayTracing, 'true']
  
  # Comprehend Configuration Conditions
  DeployCustomClassification: !Equals [!Ref EnableCustomClassification, 'true']
  
  # Environment Conditions
  IsProduction: !Equals [!Ref Environment, 'prod']

Resources:
  # ================================
  # S3 Buckets for Data Storage
  # ================================
  
  # Input bucket for storing raw text data and documents
  InputBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !If
        - CreateInputBucketName
        - !Sub '${ProjectName}-input-${Environment}-${AWS::AccountId}-${AWS::Region}'
        - !Ref InputBucketName
      BucketEncryption: !If
        - EnableS3Encryption
        - ServerSideEncryptionConfiguration:
            - ServerSideEncryptionByDefault:
                SSEAlgorithm: AES256
              BucketKeyEnabled: true
        - !Ref AWS::NoValue
      VersioningConfiguration: !If
        - EnableS3Versioning
        - Status: Enabled
        - !Ref AWS::NoValue
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      NotificationConfiguration:
        LambdaConfigurations:
          - Event: s3:ObjectCreated:*
            Function: !GetAtt ComprehendProcessorFunction.Arn
            Filter:
              S3Key:
                Rules:
                  - Name: prefix
                    Value: 'input/'
                  - Name: suffix
                    Value: '.txt'
      LifecycleConfiguration:
        Rules:
          - Id: DeleteIncompleteMultipartUploads
            Status: Enabled
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
          - Id: TransitionToIA
            Status: Enabled
            Transition:
              StorageClass: STANDARD_IA
              TransitionInDays: 30
          - Id: TransitionToGlacier
            Status: Enabled
            Transition:
              StorageClass: GLACIER
              TransitionInDays: 90
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-input-bucket'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName
        - Key: Purpose
          Value: 'NLP input data storage'

  # Output bucket for storing processed results and analysis
  OutputBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !If
        - CreateOutputBucketName
        - !Sub '${ProjectName}-output-${Environment}-${AWS::AccountId}-${AWS::Region}'
        - !Ref OutputBucketName
      BucketEncryption: !If
        - EnableS3Encryption
        - ServerSideEncryptionConfiguration:
            - ServerSideEncryptionByDefault:
                SSEAlgorithm: AES256
              BucketKeyEnabled: true
        - !Ref AWS::NoValue
      VersioningConfiguration: !If
        - EnableS3Versioning
        - Status: Enabled
        - !Ref AWS::NoValue
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: DeleteIncompleteMultipartUploads
            Status: Enabled
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
          - Id: TransitionToIA
            Status: Enabled
            Transition:
              StorageClass: STANDARD_IA
              TransitionInDays: 30
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-output-bucket'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName
        - Key: Purpose
          Value: 'NLP output data storage'

  # ================================
  # IAM Roles and Policies
  # ================================
  
  # IAM Role for Lambda function with Comprehend and S3 permissions
  ComprehendLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-lambda-role-${Environment}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - !If
          - EnableTracing
          - arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess
          - !Ref AWS::NoValue
      Policies:
        - PolicyName: ComprehendS3AccessPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              # Amazon Comprehend permissions for text analysis
              - Effect: Allow
                Action:
                  - comprehend:DetectSentiment
                  - comprehend:DetectEntities
                  - comprehend:DetectKeyPhrases
                  - comprehend:DetectLanguage
                  - comprehend:DetectSyntax
                  - comprehend:DetectTargetedSentiment
                  - comprehend:DetectToxicContent
                  - comprehend:ClassifyDocument
                Resource: '*'
              # S3 permissions for input and output buckets
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                Resource:
                  - !Sub '${InputBucket}/*'
                  - !Sub '${OutputBucket}/*'
              - Effect: Allow
                Action:
                  - s3:ListBucket
                Resource:
                  - !GetAtt InputBucket.Arn
                  - !GetAtt OutputBucket.Arn
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-lambda-execution-role'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # IAM Role for Comprehend batch processing jobs
  ComprehendServiceRole:
    Type: AWS::IAM::Role
    Condition: DeployCustomClassification
    Properties:
      RoleName: !Sub '${ProjectName}-comprehend-service-role-${Environment}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: comprehend.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: ComprehendBatchS3AccessPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              # S3 read permissions for input data
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                Resource:
                  - !GetAtt InputBucket.Arn
                  - !Sub '${InputBucket}/*'
              # S3 write permissions for output data
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource:
                  - !Sub '${OutputBucket}/*'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-comprehend-service-role'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # ================================
  # Lambda Function for Real-time Processing
  # ================================
  
  # CloudWatch Log Group for Lambda function
  ComprehendProcessorLogGroup:
    Type: AWS::Logs::LogGroup
    Condition: EnableLogs
    Properties:
      LogGroupName: !Sub '/aws/lambda/${ProjectName}-processor-${Environment}'
      RetentionInDays: !Ref LogRetentionDays
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-lambda-logs'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # Lambda function for real-time text processing with Comprehend
  ComprehendProcessorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-processor-${Environment}'
      Runtime: !Ref PythonRuntime
      Handler: index.lambda_handler
      Role: !GetAtt ComprehendLambdaExecutionRole.Arn
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemorySize
      TracingConfig: !If
        - EnableTracing
        - Mode: Active
        - !Ref AWS::NoValue
      Environment:
        Variables:
          OUTPUT_BUCKET: !Ref OutputBucket
          DEFAULT_LANGUAGE_CODE: !Ref LanguageCode
          PROJECT_NAME: !Ref ProjectName
          ENVIRONMENT: !Ref Environment
      Code:
        ZipFile: |
          import json
          import boto3
          import uuid
          import os
          from datetime import datetime
          from urllib.parse import unquote_plus
          import logging

          # Configure logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          # Initialize AWS clients
          comprehend = boto3.client('comprehend')
          s3 = boto3.client('s3')

          def lambda_handler(event, context):
              """
              Main Lambda handler for processing text with Amazon Comprehend.
              Supports both S3 event triggers and direct API invocation.
              """
              try:
                  # Determine input source (S3 event or direct invocation)
                  if 'Records' in event:
                      # S3 event trigger
                      record = event['Records'][0]
                      bucket = record['s3']['bucket']['name']
                      key = unquote_plus(record['s3']['object']['key'])
                      
                      logger.info(f"Processing S3 object: s3://{bucket}/{key}")
                      
                      # Get text from S3
                      response = s3.get_object(Bucket=bucket, Key=key)
                      text = response['Body'].read().decode('utf-8')
                      
                      # Use configured output bucket
                      output_bucket = os.environ['OUTPUT_BUCKET']
                      
                  else:
                      # Direct invocation
                      text = event.get('text', '')
                      output_bucket = event.get('output_bucket', os.environ['OUTPUT_BUCKET'])
                      bucket = event.get('source_bucket', 'direct-invocation')
                      key = event.get('source_key', 'direct-input')
                  
                  if not text:
                      logger.error("No text provided for processing")
                      return {
                          'statusCode': 400,
                          'body': json.dumps({'error': 'No text provided'})
                      }
                  
                  # Truncate text if too long for Comprehend (5000 bytes limit for real-time)
                  if len(text.encode('utf-8')) > 5000:
                      text = text[:4900]  # Leave some buffer for multibyte characters
                      logger.warning("Text truncated to meet Comprehend limits")
                  
                  # Detect language first
                  try:
                      language_response = comprehend.detect_dominant_language(Text=text)
                      language_code = language_response['Languages'][0]['LanguageCode']
                      language_confidence = language_response['Languages'][0]['Score']
                  except Exception as e:
                      logger.warning(f"Language detection failed: {str(e)}, using default")
                      language_code = os.environ.get('DEFAULT_LANGUAGE_CODE', 'en')
                      language_confidence = 0.0
                  
                  # Initialize results dictionary
                  results = {
                      'timestamp': datetime.now().isoformat(),
                      'source': {
                          'bucket': bucket,
                          'key': key
                      },
                      'text_length': len(text),
                      'language': {
                          'code': language_code,
                          'confidence': language_confidence
                      },
                      'analysis': {}
                  }
                  
                  # Perform sentiment analysis
                  try:
                      sentiment_response = comprehend.detect_sentiment(
                          Text=text,
                          LanguageCode=language_code
                      )
                      results['analysis']['sentiment'] = {
                          'sentiment': sentiment_response['Sentiment'],
                          'scores': sentiment_response['SentimentScore']
                      }
                      logger.info(f"Sentiment detected: {sentiment_response['Sentiment']}")
                  except Exception as e:
                      logger.error(f"Sentiment analysis failed: {str(e)}")
                      results['analysis']['sentiment'] = {'error': str(e)}
                  
                  # Extract entities
                  try:
                      entities_response = comprehend.detect_entities(
                          Text=text,
                          LanguageCode=language_code
                      )
                      results['analysis']['entities'] = entities_response['Entities']
                      logger.info(f"Detected {len(entities_response['Entities'])} entities")
                  except Exception as e:
                      logger.error(f"Entity detection failed: {str(e)}")
                      results['analysis']['entities'] = {'error': str(e)}
                  
                  # Extract key phrases
                  try:
                      keyphrases_response = comprehend.detect_key_phrases(
                          Text=text,
                          LanguageCode=language_code
                      )
                      results['analysis']['key_phrases'] = keyphrases_response['KeyPhrases']
                      logger.info(f"Detected {len(keyphrases_response['KeyPhrases'])} key phrases")
                  except Exception as e:
                      logger.error(f"Key phrase detection failed: {str(e)}")
                      results['analysis']['key_phrases'] = {'error': str(e)}
                  
                  # Detect syntax (optional - may not be available for all languages)
                  if language_code in ['en', 'es', 'fr', 'de', 'it', 'pt']:
                      try:
                          syntax_response = comprehend.detect_syntax(
                              Text=text,
                              LanguageCode=language_code
                          )
                          results['analysis']['syntax'] = syntax_response['SyntaxTokens'][:50]  # Limit for size
                          logger.info(f"Detected {len(syntax_response['SyntaxTokens'])} syntax tokens")
                      except Exception as e:
                          logger.warning(f"Syntax detection failed: {str(e)}")
                          results['analysis']['syntax'] = {'error': str(e)}
                  
                  # Save results to S3 output bucket
                  try:
                      output_key = f"processed/{datetime.now().strftime('%Y/%m/%d')}/{uuid.uuid4()}.json"
                      s3.put_object(
                          Bucket=output_bucket,
                          Key=output_key,
                          Body=json.dumps(results, indent=2, default=str),
                          ContentType='application/json',
                          Metadata={
                              'source-bucket': bucket,
                              'source-key': key,
                              'processing-timestamp': datetime.now().isoformat(),
                              'sentiment': results['analysis'].get('sentiment', {}).get('sentiment', 'unknown')
                          }
                      )
                      logger.info(f"Results saved to s3://{output_bucket}/{output_key}")
                      results['output_location'] = f"s3://{output_bucket}/{output_key}"
                  except Exception as e:
                      logger.error(f"Failed to save results to S3: {str(e)}")
                      results['output_error'] = str(e)
                  
                  return {
                      'statusCode': 200,
                      'headers': {
                          'Content-Type': 'application/json',
                          'Access-Control-Allow-Origin': '*'
                      },
                      'body': json.dumps(results, default=str)
                  }
                  
              except Exception as e:
                  logger.error(f"Unexpected error processing text: {str(e)}")
                  return {
                      'statusCode': 500,
                      'headers': {
                          'Content-Type': 'application/json',
                          'Access-Control-Allow-Origin': '*'
                      },
                      'body': json.dumps({
                          'error': str(e),
                          'timestamp': datetime.now().isoformat()
                      })
                  }
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-processor-function'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # S3 Bucket Permission for Lambda to be invoked by S3 events
  S3LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ComprehendProcessorFunction
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceAccount: !Ref AWS::AccountId
      SourceArn: !GetAtt InputBucket.Arn

  # ================================
  # CloudWatch Alarms and Monitoring
  # ================================
  
  # CloudWatch Alarm for Lambda function errors
  LambdaErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: EnableLogs
    Properties:
      AlarmName: !Sub '${ProjectName}-lambda-errors-${Environment}'
      AlarmDescription: 'Alarm for Lambda function errors in NLP pipeline'
      MetricName: Errors
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 2
      Threshold: 5
      ComparisonOperator: GreaterThanOrEqualToThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref ComprehendProcessorFunction
      TreatMissingData: notBreaching
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-lambda-error-alarm'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # CloudWatch Alarm for Lambda function duration
  LambdaDurationAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: EnableLogs
    Properties:
      AlarmName: !Sub '${ProjectName}-lambda-duration-${Environment}'
      AlarmDescription: 'Alarm for Lambda function high duration'
      MetricName: Duration
      Namespace: AWS/Lambda
      Statistic: Average
      Period: 300
      EvaluationPeriods: 3
      Threshold: !Sub '${LambdaTimeout}000'  # Convert to milliseconds
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref ComprehendProcessorFunction
      TreatMissingData: notBreaching
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-lambda-duration-alarm'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # ================================
  # Custom Resource for Sample Data
  # ================================
  
  # Lambda function for custom resource to upload sample data
  SampleDataUploaderFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-sample-uploader-${Environment}'
      Runtime: !Ref PythonRuntime
      Handler: index.lambda_handler
      Role: !GetAtt ComprehendLambdaExecutionRole.Arn
      Timeout: 60
      Code:
        ZipFile: |
          import json
          import boto3
          import cfnresponse
          import logging

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          s3 = boto3.client('s3')

          def lambda_handler(event, context):
              """Custom resource to upload sample data for testing"""
              try:
                  bucket_name = event['ResourceProperties']['BucketName']
                  
                  if event['RequestType'] == 'Create':
                      # Sample review data for testing
                      sample_data = [
                          "The customer service at this restaurant was absolutely terrible. The food was cold and the staff was rude.",
                          "I love this product! The quality is amazing and the delivery was super fast. Highly recommend!",
                          "The hotel room was okay, nothing special. The location was convenient but the amenities were lacking.",
                          "Outstanding experience! The team went above and beyond to help us. Five stars!",
                          "Very disappointed with the purchase. The item broke after just one week of use.",
                          "Great value for money. The product works exactly as described and arrived quickly.",
                          "Poor quality control. The item had several defects right out of the box.",
                          "Excellent customer support team. They resolved my issue within 24 hours.",
                          "The interface is confusing and difficult to navigate. Needs improvement.",
                          "Amazing product! This has exceeded all my expectations. Will definitely buy again."
                      ]
                      
                      # Upload sample files
                      for i, text in enumerate(sample_data, 1):
                          s3.put_object(
                              Bucket=bucket_name,
                              Key=f'input/sample-review-{i:02d}.txt',
                              Body=text,
                              ContentType='text/plain'
                          )
                      
                      # Upload batch processing format
                      batch_content = '\n'.join(sample_data)
                      s3.put_object(
                          Bucket=bucket_name,
                          Key='input/batch-reviews.txt',
                          Body=batch_content,
                          ContentType='text/plain'
                      )
                      
                      logger.info(f"Successfully uploaded sample data to {bucket_name}")
                      
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {
                      'Message': 'Sample data uploaded successfully'
                  })
                  
              except Exception as e:
                  logger.error(f"Error: {str(e)}")
                  cfnresponse.send(event, context, cfnresponse.FAILED, {
                      'Message': str(e)
                  })

  # Custom resource to trigger sample data upload
  SampleDataUpload:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ServiceToken: !GetAtt SampleDataUploaderFunction.Arn
      BucketName: !Ref InputBucket

# ================================
# Stack Outputs
# ================================

Outputs:
  # S3 Bucket Information
  InputBucketName:
    Description: 'Name of the S3 input bucket for text data'
    Value: !Ref InputBucket
    Export:
      Name: !Sub '${AWS::StackName}-InputBucket'

  InputBucketArn:
    Description: 'ARN of the S3 input bucket'
    Value: !GetAtt InputBucket.Arn
    Export:
      Name: !Sub '${AWS::StackName}-InputBucketArn'

  OutputBucketName:
    Description: 'Name of the S3 output bucket for processed results'
    Value: !Ref OutputBucket
    Export:
      Name: !Sub '${AWS::StackName}-OutputBucket'

  OutputBucketArn:
    Description: 'ARN of the S3 output bucket'
    Value: !GetAtt OutputBucket.Arn
    Export:
      Name: !Sub '${AWS::StackName}-OutputBucketArn'

  # Lambda Function Information
  LambdaFunctionName:
    Description: 'Name of the Lambda function for real-time processing'
    Value: !Ref ComprehendProcessorFunction
    Export:
      Name: !Sub '${AWS::StackName}-LambdaFunction'

  LambdaFunctionArn:
    Description: 'ARN of the Lambda function'
    Value: !GetAtt ComprehendProcessorFunction.Arn
    Export:
      Name: !Sub '${AWS::StackName}-LambdaFunctionArn'

  # IAM Role Information
  LambdaExecutionRoleArn:
    Description: 'ARN of the Lambda execution role'
    Value: !GetAtt ComprehendLambdaExecutionRole.Arn
    Export:
      Name: !Sub '${AWS::StackName}-LambdaExecutionRole'

  ComprehendServiceRoleArn:
    Description: 'ARN of the Comprehend service role for batch processing'
    Value: !If
      - DeployCustomClassification
      - !GetAtt ComprehendServiceRole.Arn
      - 'Not deployed - custom classification disabled'
    Export:
      Name: !Sub '${AWS::StackName}-ComprehendServiceRole'

  # CloudWatch Information
  LogGroupName:
    Description: 'Name of the CloudWatch Log Group'
    Value: !If
      - EnableLogs
      - !Ref ComprehendProcessorLogGroup
      - 'Not created - logging disabled'
    Export:
      Name: !Sub '${AWS::StackName}-LogGroup'

  # Testing Information
  TestCommand:
    Description: 'AWS CLI command to test the Lambda function'
    Value: !Sub |
      aws lambda invoke \
        --function-name ${ComprehendProcessorFunction} \
        --payload '{"text": "I love this product! Great quality and fast delivery."}' \
        --cli-binary-format raw-in-base64-out \
        response.json && cat response.json

  SampleDataLocation:
    Description: 'S3 location of uploaded sample data for testing'
    Value: !Sub 's3://${InputBucket}/input/'

  # Batch Processing Information
  BatchProcessingCommand:
    Description: 'Command to start batch sentiment analysis job'
    Value: !If
      - DeployCustomClassification
      - !Sub |
          aws comprehend start-sentiment-detection-job \
            --input-data-config S3Uri=s3://${InputBucket}/input/,InputFormat=ONE_DOC_PER_LINE \
            --output-data-config S3Uri=s3://${OutputBucket}/batch-sentiment/ \
            --data-access-role-arn ${ComprehendServiceRole.Arn} \
            --job-name comprehend-sentiment-${Environment} \
            --language-code ${LanguageCode}
      - 'Custom classification disabled - cannot run batch jobs'

  # Architecture Information
  ArchitectureDescription:
    Description: 'Description of the deployed NLP pipeline architecture'
    Value: 'Serverless NLP pipeline with S3 storage, Lambda processing, and Amazon Comprehend analysis'

  # Cost Optimization
  CostOptimizationTips:
    Description: 'Tips for optimizing costs'
    Value: 'Use S3 lifecycle policies, monitor Comprehend usage, optimize Lambda memory allocation'

  # Next Steps
  NextSteps:
    Description: 'Recommended next steps after deployment'
    Value: 'Upload text files to input bucket, monitor CloudWatch logs, analyze results in output bucket'