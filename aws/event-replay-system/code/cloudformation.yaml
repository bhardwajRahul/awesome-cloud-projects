AWSTemplateFormatVersion: '2010-09-09'
Description: 'EventBridge Archive Event Replay Mechanism - Creates a comprehensive event replay system with selective archiving, Lambda processing, and monitoring capabilities'

Parameters:
  EnvironmentName:
    Type: String
    Default: 'demo'
    Description: 'Environment name for resource naming (e.g., demo, staging, prod)'
    MinLength: 1
    MaxLength: 20
    AllowedPattern: '^[a-zA-Z0-9-]+$'
    ConstraintDescription: 'Must contain only alphanumeric characters and hyphens'

  ArchiveRetentionDays:
    Type: Number
    Default: 30
    Description: 'Number of days to retain archived events (1-10950)'
    MinValue: 1
    MaxValue: 10950
    ConstraintDescription: 'Must be between 1 and 10950 days'

  LambdaTimeout:
    Type: Number
    Default: 30
    Description: 'Lambda function timeout in seconds (1-900)'
    MinValue: 1
    MaxValue: 900
    ConstraintDescription: 'Must be between 1 and 900 seconds'

  LambdaMemorySize:
    Type: Number
    Default: 256
    Description: 'Lambda function memory size in MB (128-10240)'
    MinValue: 128
    MaxValue: 10240
    ConstraintDescription: 'Must be between 128 and 10240 MB'

  EnableCloudWatchAlarms:
    Type: String
    Default: 'true'
    Description: 'Enable CloudWatch alarms for monitoring'
    AllowedValues: ['true', 'false']

  LogRetentionDays:
    Type: Number
    Default: 14
    Description: 'CloudWatch log retention in days'
    AllowedValues: [1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, 3653]

Conditions:
  CreateAlarms: !Equals [!Ref EnableCloudWatchAlarms, 'true']

Resources:
  # S3 Bucket for storing replay logs and artifacts
  ReplayLogsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'eventbridge-replay-logs-${EnvironmentName}-${AWS::AccountId}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldVersions
            Status: Enabled
            NoncurrentVersionExpirationInDays: 30
      NotificationConfiguration:
        CloudWatchConfigurations:
          - Event: s3:ObjectCreated:*
            CloudWatchConfiguration:
              LogGroupName: !Ref ReplayMonitoringLogGroup
      Tags:
        - Key: Purpose
          Value: EventReplayLogs
        - Key: Environment
          Value: !Ref EnvironmentName

  # Custom EventBridge Event Bus for replay demonstration
  ReplayEventBus:
    Type: AWS::Events::EventBus
    Properties:
      Name: !Sub 'replay-demo-bus-${EnvironmentName}'
      Description: 'Custom event bus for event replay demonstration'
      Tags:
        - Key: Purpose
          Value: EventReplayDemo
        - Key: Environment
          Value: !Ref EnvironmentName

  # IAM Role for Lambda function
  EventReplayProcessorRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'EventReplayProcessorRole-${EnvironmentName}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: EventReplayProcessorPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - events:PutEvents
                  - events:ListReplays
                  - events:DescribeReplay
                Resource: '*'
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                Resource: !Sub '${ReplayLogsBucket}/*'
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*'
      Tags:
        - Key: Purpose
          Value: EventReplayProcessor
        - Key: Environment
          Value: !Ref EnvironmentName

  # Lambda function for processing events
  EventReplayProcessor:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'replay-processor-${EnvironmentName}'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt EventReplayProcessorRole.Arn
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemorySize
      Description: 'Process events and detect replay scenarios for EventBridge Archive demonstration'
      Environment:
        Variables:
          LOG_LEVEL: INFO
          ENVIRONMENT: !Ref EnvironmentName
          S3_BUCKET: !Ref ReplayLogsBucket
      Code:
        ZipFile: |
          import json
          import boto3
          import logging
          import os
          from datetime import datetime, timezone
          from typing import Dict, Any, Optional
          
          # Configure logging
          log_level = os.environ.get('LOG_LEVEL', 'INFO')
          logging.basicConfig(level=getattr(logging, log_level))
          logger = logging.getLogger(__name__)
          
          # Initialize AWS clients
          s3_client = boto3.client('s3')
          events_client = boto3.client('events')
          
          def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
              """
              Process EventBridge events and log details for replay analysis.
              
              Args:
                  event: EventBridge event data
                  context: Lambda context object
                  
              Returns:
                  Dict containing processing results
              """
              try:
                  # Log the complete event for debugging
                  logger.info(f"Received event: {json.dumps(event, indent=2, default=str)}")
                  
                  # Extract event metadata
                  event_source = event.get('source', 'unknown')
                  event_type = event.get('detail-type', 'unknown')
                  event_time = event.get('time', datetime.now(timezone.utc).isoformat())
                  event_id = event.get('id', 'unknown')
                  
                  # Check if this is a replayed event
                  replay_name = event.get('replay-name')
                  is_replay = bool(replay_name)
                  
                  if is_replay:
                      logger.info(f"Processing REPLAYED event from replay: {replay_name}")
                  else:
                      logger.info(f"Processing ORIGINAL event: {event_id}")
                  
                  # Process based on event source
                  processing_result = process_event_by_source(event, is_replay)
                  
                  # Log processing metrics
                  log_processing_metrics(event, is_replay, processing_result)
                  
                  # Store event details in S3 for analysis
                  store_event_analysis(event, is_replay, processing_result)
                  
                  return {
                      'statusCode': 200,
                      'body': json.dumps({
                          'message': 'Event processed successfully',
                          'eventId': event_id,
                          'eventSource': event_source,
                          'eventType': event_type,
                          'isReplay': is_replay,
                          'replayName': replay_name,
                          'processingResult': processing_result,
                          'timestamp': datetime.now(timezone.utc).isoformat()
                      }, default=str)
                  }
                  
              except Exception as e:
                  logger.error(f"Error processing event: {str(e)}", exc_info=True)
                  return {
                      'statusCode': 500,
                      'body': json.dumps({
                          'error': str(e),
                          'timestamp': datetime.now(timezone.utc).isoformat()
                      })
                  }
          
          def process_event_by_source(event: Dict[str, Any], is_replay: bool) -> Dict[str, Any]:
              """
              Process events based on their source with different logic for replays.
              
              Args:
                  event: Event data
                  is_replay: Whether this is a replayed event
                  
              Returns:
                  Processing result dictionary
              """
              event_source = event.get('source', 'unknown')
              detail = event.get('detail', {})
              
              result = {
                  'processed': True,
                  'action': 'processed',
                  'notes': []
              }
              
              try:
                  if event_source == 'myapp.orders':
                      result.update(process_order_event(detail, is_replay))
                  elif event_source == 'myapp.users':
                      result.update(process_user_event(detail, is_replay))
                  elif event_source == 'myapp.inventory':
                      result.update(process_inventory_event(detail, is_replay))
                  else:
                      result.update({
                          'action': 'generic_processing',
                          'notes': [f'Processed generic event from {event_source}']
                      })
                      
              except Exception as e:
                  logger.error(f"Error processing event from {event_source}: {str(e)}")
                  result.update({
                      'processed': False,
                      'error': str(e)
                  })
                  
              return result
          
          def process_order_event(detail: Dict[str, Any], is_replay: bool) -> Dict[str, Any]:
              """Process order-related events"""
              order_id = detail.get('orderId', 'unknown')
              amount = detail.get('amount', 0)
              customer_id = detail.get('customerId', 'unknown')
              
              logger.info(f"Processing order event - ID: {order_id}, Amount: {amount}, Customer: {customer_id}")
              
              result = {
                  'action': 'order_processed',
                  'order_id': order_id,
                  'amount': amount,
                  'customer_id': customer_id,
                  'notes': []
              }
              
              if is_replay:
                  # For replay events, we might want to skip certain actions
                  result['notes'].append('Replay: Skipped payment processing to avoid duplicate charges')
                  result['action'] = 'order_replay_processed'
              else:
                  result['notes'].append('Original: Full order processing completed')
                  
              return result
          
          def process_user_event(detail: Dict[str, Any], is_replay: bool) -> Dict[str, Any]:
              """Process user-related events"""
              user_id = detail.get('userId', 'unknown')
              email = detail.get('email', 'unknown')
              
              logger.info(f"Processing user event - ID: {user_id}, Email: {email}")
              
              result = {
                  'action': 'user_processed',
                  'user_id': user_id,
                  'email': email,
                  'notes': []
              }
              
              if is_replay:
                  result['notes'].append('Replay: Skipped welcome email to avoid duplicate notifications')
                  result['action'] = 'user_replay_processed'
              else:
                  result['notes'].append('Original: Welcome email sent')
                  
              return result
          
          def process_inventory_event(detail: Dict[str, Any], is_replay: bool) -> Dict[str, Any]:
              """Process inventory-related events"""
              product_id = detail.get('productId', 'unknown')
              quantity = detail.get('quantity', 0)
              
              logger.info(f"Processing inventory event - Product: {product_id}, Quantity: {quantity}")
              
              result = {
                  'action': 'inventory_processed',
                  'product_id': product_id,
                  'quantity': quantity,
                  'notes': []
              }
              
              if is_replay:
                  result['notes'].append('Replay: Inventory adjustment validated only')
                  result['action'] = 'inventory_replay_validated'
              else:
                  result['notes'].append('Original: Inventory levels updated')
                  
              return result
          
          def log_processing_metrics(event: Dict[str, Any], is_replay: bool, result: Dict[str, Any]) -> None:
              """Log processing metrics for monitoring"""
              metrics = {
                  'event_source': event.get('source', 'unknown'),
                  'event_type': event.get('detail-type', 'unknown'),
                  'is_replay': is_replay,
                  'processing_successful': result.get('processed', False),
                  'processing_time': datetime.now(timezone.utc).isoformat(),
                  'replay_name': event.get('replay-name', None)
              }
              
              logger.info(f"Processing metrics: {json.dumps(metrics, default=str)}")
          
          def store_event_analysis(event: Dict[str, Any], is_replay: bool, result: Dict[str, Any]) -> None:
              """Store event analysis results in S3"""
              try:
                  bucket_name = os.environ.get('S3_BUCKET')
                  if not bucket_name:
                      logger.warning("S3_BUCKET environment variable not set, skipping storage")
                      return
                  
                  # Create analysis object
                  analysis = {
                      'event_id': event.get('id', 'unknown'),
                      'event_source': event.get('source', 'unknown'),
                      'event_type': event.get('detail-type', 'unknown'),
                      'is_replay': is_replay,
                      'replay_name': event.get('replay-name', None),
                      'processing_result': result,
                      'timestamp': datetime.now(timezone.utc).isoformat(),
                      'original_event': event
                  }
                  
                  # Generate S3 key
                  date_prefix = datetime.now(timezone.utc).strftime('%Y/%m/%d')
                  event_type = 'replay' if is_replay else 'original'
                  key = f"event-analysis/{date_prefix}/{event_type}/{event.get('id', 'unknown')}.json"
                  
                  # Store in S3
                  s3_client.put_object(
                      Bucket=bucket_name,
                      Key=key,
                      Body=json.dumps(analysis, indent=2, default=str),
                      ContentType='application/json'
                  )
                  
                  logger.info(f"Stored event analysis in S3: {key}")
                  
              except Exception as e:
                  logger.error(f"Error storing event analysis: {str(e)}")
      Tags:
        - Key: Purpose
          Value: EventReplayProcessor
        - Key: Environment
          Value: !Ref EnvironmentName

  # CloudWatch Log Group for Lambda function
  EventReplayProcessorLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/replay-processor-${EnvironmentName}'
      RetentionInDays: !Ref LogRetentionDays
      Tags:
        - Key: Purpose
          Value: EventReplayProcessor
        - Key: Environment
          Value: !Ref EnvironmentName

  # EventBridge Rule for processing application events
  EventProcessingRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub 'replay-demo-rule-${EnvironmentName}'
      EventBusName: !Ref ReplayEventBus
      Description: 'Rule for processing application events including replayed events'
      EventPattern:
        source:
          - 'myapp.orders'
          - 'myapp.users'
          - 'myapp.inventory'
        detail-type:
          - 'Order Created'
          - 'User Registered'
          - 'Inventory Updated'
      State: ENABLED
      Targets:
        - Arn: !GetAtt EventReplayProcessor.Arn
          Id: 'EventReplayProcessorTarget'
          RetryPolicy:
            MaximumRetryAttempts: 3
            MaximumEventAge: 3600
          DeadLetterConfig:
            Arn: !GetAtt EventProcessingDeadLetterQueue.Arn
      Tags:
        - Key: Purpose
          Value: EventReplayProcessing
        - Key: Environment
          Value: !Ref EnvironmentName

  # Permission for EventBridge to invoke Lambda
  EventBridgeLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref EventReplayProcessor
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt EventProcessingRule.Arn

  # Dead Letter Queue for failed event processing
  EventProcessingDeadLetterQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: !Sub 'replay-demo-dlq-${EnvironmentName}'
      MessageRetentionPeriod: 1209600  # 14 days
      VisibilityTimeoutSeconds: 300
      KmsMasterKeyId: alias/aws/sqs
      Tags:
        - Key: Purpose
          Value: EventReplayDLQ
        - Key: Environment
          Value: !Ref EnvironmentName

  # EventBridge Archive for selective event storage
  EventReplayArchive:
    Type: AWS::Events::Archive
    Properties:
      ArchiveName: !Sub 'replay-demo-archive-${EnvironmentName}'
      SourceArn: !GetAtt ReplayEventBus.Arn
      Description: 'Archive for order and user events with selective filtering'
      EventPattern:
        source:
          - 'myapp.orders'
          - 'myapp.users'
        detail-type:
          - 'Order Created'
          - 'User Registered'
      RetentionDays: !Ref ArchiveRetentionDays

  # CloudWatch Log Group for replay monitoring
  ReplayMonitoringLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/events/replay-monitoring-${EnvironmentName}'
      RetentionInDays: !Ref LogRetentionDays
      Tags:
        - Key: Purpose
          Value: ReplayMonitoring
        - Key: Environment
          Value: !Ref EnvironmentName

  # IAM Role for EventBridge Replay operations
  EventBridgeReplayRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'EventBridgeReplayRole-${EnvironmentName}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: events.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: ReplayOperationsPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - events:PutEvents
                  - events:StartReplay
                  - events:CancelReplay
                  - events:DescribeReplay
                  - events:ListReplays
                Resource: '*'
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/events/*'
      Tags:
        - Key: Purpose
          Value: EventBridgeReplay
        - Key: Environment
          Value: !Ref EnvironmentName

  # CloudWatch Alarms for monitoring
  ReplayFailureAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: CreateAlarms
    Properties:
      AlarmName: !Sub 'EventBridge-Replay-Failures-${EnvironmentName}'
      AlarmDescription: 'Alert when EventBridge replay operations fail'
      MetricName: 'ReplayFailures'
      Namespace: 'AWS/Events'
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      TreatMissingData: notBreaching
      AlarmActions:
        - !Ref ReplayAlertsSnsTopic

  LambdaErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: CreateAlarms
    Properties:
      AlarmName: !Sub 'EventReplay-Lambda-Errors-${EnvironmentName}'
      AlarmDescription: 'Alert when Lambda function encounters errors'
      MetricName: 'Errors'
      Namespace: 'AWS/Lambda'
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      TreatMissingData: notBreaching
      Dimensions:
        - Name: FunctionName
          Value: !Ref EventReplayProcessor
      AlarmActions:
        - !Ref ReplayAlertsSnsTopic

  DeadLetterQueueAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: CreateAlarms
    Properties:
      AlarmName: !Sub 'EventReplay-DLQ-Messages-${EnvironmentName}'
      AlarmDescription: 'Alert when messages appear in dead letter queue'
      MetricName: 'ApproximateNumberOfVisibleMessages'
      Namespace: 'AWS/SQS'
      Statistic: Average
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      TreatMissingData: notBreaching
      Dimensions:
        - Name: QueueName
          Value: !GetAtt EventProcessingDeadLetterQueue.QueueName
      AlarmActions:
        - !Ref ReplayAlertsSnsTopic

  # SNS Topic for alerts
  ReplayAlertsSnsTopic:
    Type: AWS::SNS::Topic
    Condition: CreateAlarms
    Properties:
      TopicName: !Sub 'eventbridge-replay-alerts-${EnvironmentName}'
      DisplayName: !Sub 'EventBridge Replay Alerts - ${EnvironmentName}'
      KmsMasterKeyId: alias/aws/sns
      Tags:
        - Key: Purpose
          Value: EventReplayAlerts
        - Key: Environment
          Value: !Ref EnvironmentName

  # Custom Resource for generating test events (optional)
  EventGeneratorCustomResource:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ServiceToken: !GetAtt EventGeneratorFunction.Arn
      EventBusName: !Ref ReplayEventBus
      EventCount: 5
      TriggerUpdate: !Ref AWS::StackName  # Forces update on stack updates

  # Lambda function for generating test events
  EventGeneratorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'event-generator-${EnvironmentName}'
      Runtime: python3.9
      Handler: index.handler
      Role: !GetAtt EventGeneratorRole.Arn
      Timeout: 60
      Code:
        ZipFile: |
          import json
          import boto3
          import cfnresponse
          import random
          from datetime import datetime, timezone
          
          events_client = boto3.client('events')
          
          def handler(event, context):
              try:
                  request_type = event['RequestType']
                  
                  if request_type == 'Create' or request_type == 'Update':
                      event_bus_name = event['ResourceProperties']['EventBusName']
                      event_count = int(event['ResourceProperties'].get('EventCount', 5))
                      
                      # Generate sample events
                      for i in range(event_count):
                          # Generate order event
                          order_detail = {
                              'orderId': f'demo-order-{i+1}',
                              'amount': random.randint(50, 1000),
                              'customerId': f'demo-customer-{i+1}',
                              'timestamp': datetime.now(timezone.utc).isoformat()
                          }
                          
                          events_client.put_events(
                              Entries=[
                                  {
                                      'Source': 'myapp.orders',
                                      'DetailType': 'Order Created',
                                      'Detail': json.dumps(order_detail),
                                      'EventBusName': event_bus_name
                                  }
                              ]
                          )
                          
                          # Generate user event
                          user_detail = {
                              'userId': f'demo-user-{i+1}',
                              'email': f'demo-user{i+1}@example.com',
                              'timestamp': datetime.now(timezone.utc).isoformat()
                          }
                          
                          events_client.put_events(
                              Entries=[
                                  {
                                      'Source': 'myapp.users',
                                      'DetailType': 'User Registered',
                                      'Detail': json.dumps(user_detail),
                                      'EventBusName': event_bus_name
                                  }
                              ]
                          )
                      
                      response_data = {
                          'EventsGenerated': event_count * 2,
                          'Message': f'Generated {event_count * 2} test events'
                      }
                      
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)
                  else:
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      
              except Exception as e:
                  print(f'Error: {str(e)}')
                  cfnresponse.send(event, context, cfnresponse.FAILED, {})

  # IAM Role for Event Generator Lambda
  EventGeneratorRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: EventGeneratorPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - events:PutEvents
                Resource: !GetAtt ReplayEventBus.Arn

Outputs:
  EventBusName:
    Description: 'Name of the custom EventBridge event bus'
    Value: !Ref ReplayEventBus
    Export:
      Name: !Sub '${AWS::StackName}-EventBusName'

  EventBusArn:
    Description: 'ARN of the custom EventBridge event bus'
    Value: !GetAtt ReplayEventBus.Arn
    Export:
      Name: !Sub '${AWS::StackName}-EventBusArn'

  ArchiveName:
    Description: 'Name of the EventBridge archive'
    Value: !Ref EventReplayArchive
    Export:
      Name: !Sub '${AWS::StackName}-ArchiveName'

  LambdaFunctionName:
    Description: 'Name of the event processing Lambda function'
    Value: !Ref EventReplayProcessor
    Export:
      Name: !Sub '${AWS::StackName}-LambdaFunctionName'

  LambdaFunctionArn:
    Description: 'ARN of the event processing Lambda function'
    Value: !GetAtt EventReplayProcessor.Arn
    Export:
      Name: !Sub '${AWS::StackName}-LambdaFunctionArn'

  S3BucketName:
    Description: 'Name of the S3 bucket for replay logs'
    Value: !Ref ReplayLogsBucket
    Export:
      Name: !Sub '${AWS::StackName}-S3BucketName'

  DeadLetterQueueUrl:
    Description: 'URL of the dead letter queue'
    Value: !Ref EventProcessingDeadLetterQueue
    Export:
      Name: !Sub '${AWS::StackName}-DeadLetterQueueUrl'

  ReplayRoleArn:
    Description: 'ARN of the IAM role for replay operations'
    Value: !GetAtt EventBridgeReplayRole.Arn
    Export:
      Name: !Sub '${AWS::StackName}-ReplayRoleArn'

  SnsTopicArn:
    Description: 'ARN of the SNS topic for alerts'
    Value: !If [CreateAlarms, !Ref ReplayAlertsSnsTopic, 'Alarms disabled']
    Export:
      Name: !Sub '${AWS::StackName}-SnsTopicArn'

  StartReplayCommand:
    Description: 'Sample AWS CLI command to start a replay'
    Value: !Sub |
      aws events start-replay \
        --replay-name "demo-replay-$(date +%Y%m%d-%H%M%S)" \
        --event-source-arn "${ReplayEventBus.Arn}" \
        --event-start-time "$(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ)" \
        --event-end-time "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
        --destination '{"Arn": "${ReplayEventBus.Arn}", "FilterArns": ["${EventProcessingRule.Arn}"]}'

  MonitoringQueries:
    Description: 'CloudWatch Insights queries for monitoring'
    Value: !Sub |
      Lambda Logs: fields @timestamp, @message | filter @message like /Processing/ | sort @timestamp desc
      Replay Events: fields @timestamp, @message | filter @message like /REPLAYED/ | sort @timestamp desc
      Error Analysis: fields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc