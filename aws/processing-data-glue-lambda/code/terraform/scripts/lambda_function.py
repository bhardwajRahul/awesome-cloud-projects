#!/usr/bin/env python3
"""
AWS Lambda Function for ETL Pipeline Orchestration

This function orchestrates the serverless ETL pipeline by triggering
AWS Glue jobs, monitoring their status, and handling error conditions.

Author: Generated by Terraform IaC
Version: 1.0
"""

import json
import boto3
import logging
import os
from datetime import datetime
from typing import Dict, Any, Optional

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Initialize AWS clients
glue_client = boto3.client('glue')
s3_client = boto3.client('s3')

# Environment variables
GLUE_JOB_NAME = os.environ.get('GLUE_JOB_NAME')
GLUE_DATABASE = os.environ.get('GLUE_DATABASE')
S3_BUCKET = os.environ.get('S3_BUCKET')
OUTPUT_PATH = os.environ.get('OUTPUT_PATH')

def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Main Lambda handler function for ETL pipeline orchestration
    
    Args:
        event: Lambda event object (can be S3 event, scheduled event, or manual trigger)
        context: Lambda context object
        
    Returns:
        Dict containing response status and metadata
    """
    
    try:
        logger.info(f"Lambda function invoked with event: {json.dumps(event, default=str)}")
        
        # Determine event source and extract relevant information
        event_source = determine_event_source(event)
        
        if event_source == 's3':
            return handle_s3_event(event, context)
        elif event_source == 'schedule':
            return handle_scheduled_event(event, context)
        elif event_source == 'manual':
            return handle_manual_trigger(event, context)
        else:
            logger.warning(f"Unknown event source: {event_source}")
            return create_response(400, f"Unknown event source: {event_source}")
            
    except Exception as e:
        logger.error(f"Error in Lambda handler: {str(e)}", exc_info=True)
        return create_response(500, f"Lambda execution failed: {str(e)}")

def determine_event_source(event: Dict[str, Any]) -> str:
    """
    Determine the source of the Lambda invocation
    
    Args:
        event: Lambda event object
        
    Returns:
        String indicating event source ('s3', 'schedule', 'manual')
    """
    
    if 'Records' in event and event['Records']:
        # Check if it's an S3 event
        if event['Records'][0].get('eventSource') == 'aws:s3':
            return 's3'
    
    # Check if it's a CloudWatch Events scheduled trigger
    if event.get('source') == 'aws.events' and 'schedule' in event.get('detail-type', '').lower():
        return 'schedule'
    
    # Check for manual trigger patterns
    if 'job_name' in event or 'manual_trigger' in event:
        return 'manual'
    
    # Default to manual if pattern is unclear
    return 'manual'

def handle_s3_event(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Handle S3 event-triggered ETL processing
    
    Args:
        event: S3 event object
        context: Lambda context
        
    Returns:
        Dict containing response status and job information
    """
    
    try:
        logger.info("Processing S3 event trigger")
        
        # Extract S3 event information
        s3_records = event['Records']
        processed_files = []
        
        for record in s3_records:
            bucket_name = record['s3']['bucket']['name']
            object_key = record['s3']['object']['key']
            
            logger.info(f"Processing S3 object: s3://{bucket_name}/{object_key}")
            
            # Validate that the file is in the expected location and format
            if is_valid_data_file(object_key):
                processed_files.append({
                    'bucket': bucket_name,
                    'key': object_key,
                    'size': record['s3']['object']['size']
                })
            else:
                logger.info(f"Skipping file {object_key} - not a valid data file")
        
        if not processed_files:
            logger.info("No valid data files found in S3 event")
            return create_response(200, "No valid data files to process")
        
        # Trigger Glue job for new data
        job_response = start_glue_job({
            'trigger_type': 's3_event',
            'processed_files': processed_files,
            'event_time': datetime.now().isoformat()
        })
        
        return create_response(200, "ETL job triggered successfully", job_response)
        
    except Exception as e:
        logger.error(f"Error handling S3 event: {str(e)}", exc_info=True)
        return create_response(500, f"S3 event processing failed: {str(e)}")

def handle_scheduled_event(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Handle scheduled ETL processing
    
    Args:
        event: CloudWatch Events scheduled event
        context: Lambda context
        
    Returns:
        Dict containing response status and job information
    """
    
    try:
        logger.info("Processing scheduled event trigger")
        
        # Check if there's new data to process
        data_status = check_for_new_data()
        
        if not data_status['has_new_data']:
            logger.info("No new data found for scheduled processing")
            return create_response(200, "No new data to process", data_status)
        
        # Trigger Glue job for scheduled processing
        job_response = start_glue_job({
            'trigger_type': 'scheduled',
            'data_status': data_status,
            'scheduled_time': event.get('time', datetime.now().isoformat())
        })
        
        return create_response(200, "Scheduled ETL job triggered successfully", job_response)
        
    except Exception as e:
        logger.error(f"Error handling scheduled event: {str(e)}", exc_info=True)
        return create_response(500, f"Scheduled event processing failed: {str(e)}")

def handle_manual_trigger(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Handle manual ETL job trigger
    
    Args:
        event: Manual trigger event with job parameters
        context: Lambda context
        
    Returns:
        Dict containing response status and job information
    """
    
    try:
        logger.info("Processing manual trigger")
        
        # Extract job parameters from event
        job_params = extract_job_parameters(event)
        
        # Validate required parameters
        if not validate_job_parameters(job_params):
            return create_response(400, "Invalid job parameters provided")
        
        # Trigger Glue job with provided parameters
        job_response = start_glue_job({
            'trigger_type': 'manual',
            'job_params': job_params,
            'trigger_time': datetime.now().isoformat()
        })
        
        return create_response(200, "Manual ETL job triggered successfully", job_response)
        
    except Exception as e:
        logger.error(f"Error handling manual trigger: {str(e)}", exc_info=True)
        return create_response(500, f"Manual trigger processing failed: {str(e)}")

def start_glue_job(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    Start AWS Glue ETL job with appropriate parameters
    
    Args:
        metadata: Additional metadata about the job trigger
        
    Returns:
        Dict containing job run information
    """
    
    try:
        # Prepare job arguments
        job_arguments = {
            '--database_name': GLUE_DATABASE or metadata.get('database_name', 'default'),
            '--table_prefix': 'raw_data',
            '--output_path': OUTPUT_PATH or metadata.get('output_path', f's3://{S3_BUCKET}/processed-data'),
            '--trigger_metadata': json.dumps(metadata)
        }
        
        # Start the Glue job
        logger.info(f"Starting Glue job: {GLUE_JOB_NAME}")
        response = glue_client.start_job_run(
            JobName=GLUE_JOB_NAME,
            Arguments=job_arguments
        )
        
        job_run_id = response['JobRunId']
        logger.info(f"Glue job started successfully with run ID: {job_run_id}")
        
        return {
            'job_name': GLUE_JOB_NAME,
            'job_run_id': job_run_id,
            'arguments': job_arguments,
            'metadata': metadata,
            'status': 'STARTING'
        }
        
    except Exception as e:
        logger.error(f"Error starting Glue job: {str(e)}", exc_info=True)
        raise e

def check_glue_job_status(job_name: str, job_run_id: str) -> Dict[str, Any]:
    """
    Check the status of a Glue job run
    
    Args:
        job_name: Name of the Glue job
        job_run_id: ID of the job run
        
    Returns:
        Dict containing job status information
    """
    
    try:
        response = glue_client.get_job_run(
            JobName=job_name,
            RunId=job_run_id
        )
        
        job_run = response['JobRun']
        
        return {
            'job_name': job_name,
            'job_run_id': job_run_id,
            'status': job_run['JobRunState'],
            'started_on': job_run.get('StartedOn'),
            'completed_on': job_run.get('CompletedOn'),
            'execution_time': job_run.get('ExecutionTime'),
            'error_message': job_run.get('ErrorMessage'),
            'max_capacity': job_run.get('MaxCapacity'),
            'worker_type': job_run.get('WorkerType'),
            'number_of_workers': job_run.get('NumberOfWorkers')
        }
        
    except Exception as e:
        logger.error(f"Error checking job status: {str(e)}", exc_info=True)
        return {'error': str(e)}

def is_valid_data_file(object_key: str) -> bool:
    """
    Check if the S3 object is a valid data file for processing
    
    Args:
        object_key: S3 object key
        
    Returns:
        Boolean indicating if file is valid for processing
    """
    
    # Check if file is in raw-data directory and has valid extension
    valid_prefixes = ['raw-data/']
    valid_extensions = ['.csv', '.json', '.parquet', '.avro']
    
    # Skip temporary/hidden files
    if any(part.startswith('.') or part.startswith('_') for part in object_key.split('/')):
        return False
    
    # Check prefix
    if not any(object_key.startswith(prefix) for prefix in valid_prefixes):
        return False
    
    # Check extension
    if not any(object_key.lower().endswith(ext) for ext in valid_extensions):
        return False
    
    return True

def check_for_new_data() -> Dict[str, Any]:
    """
    Check if there's new data available for processing
    
    Returns:
        Dict containing information about available data
    """
    
    try:
        # List objects in raw-data directory
        response = s3_client.list_objects_v2(
            Bucket=S3_BUCKET,
            Prefix='raw-data/',
            MaxKeys=100
        )
        
        if 'Contents' not in response:
            return {'has_new_data': False, 'file_count': 0}
        
        # Filter for valid data files
        valid_files = [
            obj for obj in response['Contents']
            if is_valid_data_file(obj['Key'])
        ]
        
        return {
            'has_new_data': len(valid_files) > 0,
            'file_count': len(valid_files),
            'total_size': sum(obj['Size'] for obj in valid_files),
            'latest_modified': max(obj['LastModified'] for obj in valid_files) if valid_files else None
        }
        
    except Exception as e:
        logger.error(f"Error checking for new data: {str(e)}", exc_info=True)
        return {'has_new_data': False, 'error': str(e)}

def extract_job_parameters(event: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract job parameters from manual trigger event
    
    Args:
        event: Lambda event object
        
    Returns:
        Dict containing extracted job parameters
    """
    
    return {
        'job_name': event.get('job_name', GLUE_JOB_NAME),
        'database_name': event.get('database_name', GLUE_DATABASE),
        'output_path': event.get('output_path', OUTPUT_PATH),
        'table_prefix': event.get('table_prefix', 'raw_data'),
        'force_run': event.get('force_run', False)
    }

def validate_job_parameters(job_params: Dict[str, Any]) -> bool:
    """
    Validate job parameters for manual trigger
    
    Args:
        job_params: Job parameters to validate
        
    Returns:
        Boolean indicating if parameters are valid
    """
    
    required_params = ['job_name', 'database_name', 'output_path']
    
    for param in required_params:
        if not job_params.get(param):
            logger.error(f"Missing required parameter: {param}")
            return False
    
    return True

def create_response(status_code: int, message: str, data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Create standardized Lambda response
    
    Args:
        status_code: HTTP status code
        message: Response message
        data: Optional additional data
        
    Returns:
        Dict containing standardized response
    """
    
    response = {
        'statusCode': status_code,
        'body': json.dumps({
            'message': message,
            'timestamp': datetime.now().isoformat(),
            'data': data or {}
        })
    }
    
    return response