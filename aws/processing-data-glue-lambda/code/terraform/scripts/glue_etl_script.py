#!/usr/bin/env python3
"""
AWS Glue ETL Script for Serverless Data Processing Pipeline

This script processes sales and customer data, performing transformations
and creating analytics-ready datasets stored in Parquet format.

Author: Generated by Terraform IaC
Version: 1.0
"""

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import functions as F
from pyspark.sql.types import *
import logging

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def main():
    """Main ETL processing function"""
    
    # Parse job arguments
    args = getResolvedOptions(sys.argv, [
        'JOB_NAME',
        'database_name',
        'table_prefix',
        'output_path'
    ])
    
    # Initialize Spark and Glue contexts
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    
    logger.info(f"Starting ETL job: {args['JOB_NAME']}")
    logger.info(f"Database: {args['database_name']}")
    logger.info(f"Output path: {args['output_path']}")
    
    try:
        # Read data from Glue Data Catalog
        logger.info("Reading sales data from catalog...")
        sales_df = glueContext.create_dynamic_frame.from_catalog(
            database=args['database_name'],
            table_name=f"{args['table_prefix']}_sales_data_csv"
        ).toDF()
        
        logger.info("Reading customer data from catalog...")
        customer_df = glueContext.create_dynamic_frame.from_catalog(
            database=args['database_name'],
            table_name=f"{args['table_prefix']}_customer_data_csv"
        ).toDF()
        
        # Data quality checks
        logger.info("Performing data quality checks...")
        sales_count = sales_df.count()
        customer_count = customer_df.count()
        
        if sales_count == 0:
            raise ValueError("Sales data is empty")
        if customer_count == 0:
            raise ValueError("Customer data is empty")
            
        logger.info(f"Sales records: {sales_count}")
        logger.info(f"Customer records: {customer_count}")
        
        # Data transformations
        logger.info("Starting data transformations...")
        
        # 1. Clean and convert sales data
        sales_df = clean_sales_data(sales_df)
        
        # 2. Clean customer data
        customer_df = clean_customer_data(customer_df)
        
        # 3. Join sales with customer data
        logger.info("Joining sales and customer data...")
        enriched_df = sales_df.join(customer_df, "customer_id", "inner")
        
        # 4. Calculate aggregated metrics
        logger.info("Calculating daily sales metrics...")
        daily_sales = calculate_daily_sales(enriched_df)
        
        logger.info("Calculating customer metrics...")
        customer_metrics = calculate_customer_metrics(enriched_df)
        
        # 5. Data validation
        validate_transformed_data(daily_sales, customer_metrics)
        
        # Write processed data to S3
        logger.info("Writing processed data to S3...")
        write_data_to_s3(glueContext, daily_sales, customer_metrics, args['output_path'])
        
        # Log processing statistics
        logger.info("ETL processing completed successfully!")
        logger.info(f"Processed {sales_count} sales records")
        logger.info(f"Processed {customer_count} customer records")
        logger.info(f"Generated {daily_sales.count()} daily sales records")
        logger.info(f"Generated {customer_metrics.count()} customer metric records")
        
    except Exception as e:
        logger.error(f"ETL job failed: {str(e)}")
        raise e
    finally:
        job.commit()

def clean_sales_data(sales_df):
    """Clean and transform sales data"""
    
    # Convert price to numeric and calculate total
    sales_df = sales_df.withColumn("price", F.col("price").cast("double"))
    sales_df = sales_df.withColumn("quantity", F.col("quantity").cast("integer"))
    sales_df = sales_df.withColumn("total_amount", 
        F.col("quantity") * F.col("price"))
    
    # Convert order_date to proper date format
    sales_df = sales_df.withColumn("order_date", 
        F.to_date(F.col("order_date"), "yyyy-MM-dd"))
    
    # Add derived columns
    sales_df = sales_df.withColumn("order_year", F.year("order_date"))
    sales_df = sales_df.withColumn("order_month", F.month("order_date"))
    sales_df = sales_df.withColumn("order_day_of_week", F.dayofweek("order_date"))
    
    # Filter out invalid records
    sales_df = sales_df.filter(
        (F.col("price") > 0) & 
        (F.col("quantity") > 0) & 
        (F.col("order_date").isNotNull())
    )
    
    return sales_df

def clean_customer_data(customer_df):
    """Clean and transform customer data"""
    
    # Convert registration_date to proper date format
    customer_df = customer_df.withColumn("registration_date", 
        F.to_date(F.col("registration_date"), "yyyy-MM-dd"))
    
    # Standardize status values
    customer_df = customer_df.withColumn("status", 
        F.lower(F.trim(F.col("status"))))
    
    # Add customer tenure calculation
    customer_df = customer_df.withColumn("customer_tenure_days",
        F.datediff(F.current_date(), F.col("registration_date")))
    
    # Filter out invalid records
    customer_df = customer_df.filter(
        (F.col("customer_id").isNotNull()) &
        (F.col("name").isNotNull()) &
        (F.col("registration_date").isNotNull())
    )
    
    return customer_df

def calculate_daily_sales(enriched_df):
    """Calculate daily sales aggregations"""
    
    daily_sales = enriched_df.groupBy("order_date", "region").agg(
        F.sum("total_amount").alias("daily_revenue"),
        F.count("order_id").alias("daily_orders"),
        F.countDistinct("customer_id").alias("unique_customers"),
        F.avg("total_amount").alias("avg_order_value"),
        F.min("total_amount").alias("min_order_value"),
        F.max("total_amount").alias("max_order_value"),
        F.sum("quantity").alias("total_items_sold")
    )
    
    # Add calculated metrics
    daily_sales = daily_sales.withColumn("revenue_per_customer",
        F.col("daily_revenue") / F.col("unique_customers"))
    
    return daily_sales

def calculate_customer_metrics(enriched_df):
    """Calculate customer-level metrics"""
    
    customer_metrics = enriched_df.groupBy("customer_id", "name", "status", "registration_date").agg(
        F.sum("total_amount").alias("total_spent"),
        F.count("order_id").alias("total_orders"),
        F.avg("total_amount").alias("avg_order_value"),
        F.sum("quantity").alias("total_items_purchased"),
        F.min("order_date").alias("first_order_date"),
        F.max("order_date").alias("last_order_date"),
        F.countDistinct("product_id").alias("unique_products_purchased"),
        F.countDistinct("region").alias("regions_shopped")
    )
    
    # Add calculated metrics
    customer_metrics = customer_metrics.withColumn("days_between_first_last_order",
        F.datediff(F.col("last_order_date"), F.col("first_order_date")))
    
    customer_metrics = customer_metrics.withColumn("order_frequency",
        F.when(F.col("days_between_first_last_order") > 0,
               F.col("total_orders") / F.col("days_between_first_last_order") * 30)
        .otherwise(0))
    
    # Customer segmentation
    customer_metrics = customer_metrics.withColumn("customer_segment",
        F.when(F.col("total_spent") >= 200, "High Value")
        .when(F.col("total_spent") >= 100, "Medium Value")
        .otherwise("Low Value"))
    
    return customer_metrics

def validate_transformed_data(daily_sales, customer_metrics):
    """Validate transformed data quality"""
    
    # Check for null values in key columns
    daily_sales_nulls = daily_sales.filter(
        F.col("daily_revenue").isNull() | 
        F.col("daily_orders").isNull()
    ).count()
    
    customer_metrics_nulls = customer_metrics.filter(
        F.col("total_spent").isNull() | 
        F.col("total_orders").isNull()
    ).count()
    
    if daily_sales_nulls > 0:
        logger.warning(f"Found {daily_sales_nulls} null values in daily sales data")
    
    if customer_metrics_nulls > 0:
        logger.warning(f"Found {customer_metrics_nulls} null values in customer metrics data")
    
    # Check for negative values
    negative_revenue = daily_sales.filter(F.col("daily_revenue") < 0).count()
    negative_spending = customer_metrics.filter(F.col("total_spent") < 0).count()
    
    if negative_revenue > 0:
        raise ValueError(f"Found {negative_revenue} negative revenue values")
    
    if negative_spending > 0:
        raise ValueError(f"Found {negative_spending} negative spending values")
    
    logger.info("Data validation completed successfully")

def write_data_to_s3(glue_context, daily_sales, customer_metrics, output_path):
    """Write processed data to S3 in Parquet format"""
    
    # Convert back to DynamicFrame for writing
    daily_sales_df = DynamicFrame.fromDF(daily_sales, glue_context, "daily_sales")
    customer_metrics_df = DynamicFrame.fromDF(customer_metrics, glue_context, "customer_metrics")
    
    # Write daily sales data with partitioning
    logger.info("Writing daily sales data...")
    glue_context.write_dynamic_frame.from_options(
        frame=daily_sales_df,
        connection_type="s3",
        connection_options={
            "path": f"{output_path}/daily_sales/",
            "partitionKeys": ["order_date"]
        },
        format="parquet",
        format_options={
            "compression": "snappy"
        }
    )
    
    # Write customer metrics data
    logger.info("Writing customer metrics data...")
    glue_context.write_dynamic_frame.from_options(
        frame=customer_metrics_df,
        connection_type="s3",
        connection_options={
            "path": f"{output_path}/customer_metrics/"
        },
        format="parquet",
        format_options={
            "compression": "snappy"
        }
    )
    
    logger.info("Data writing completed successfully")

if __name__ == "__main__":
    main()